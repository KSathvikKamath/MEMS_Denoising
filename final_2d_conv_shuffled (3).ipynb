{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF5enzZIrvay"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yXWJYUqsg4_",
        "outputId": "f820aa4e-5bf1-4e65-c5f3-00d18f6f104d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             X    Y\n",
            "0    -0.385173 -0.4\n",
            "1     0.270294  0.3\n",
            "2    -0.514897 -0.4\n",
            "3    -0.496396 -0.4\n",
            "4     0.019509  0.0\n",
            "...        ...  ...\n",
            "4995  0.529139  0.6\n",
            "4996  0.046812  0.0\n",
            "4997  0.043144  0.0\n",
            "4998  0.101030  0.0\n",
            "4999  0.029457  0.0\n",
            "\n",
            "[5000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_excel('Data (1).xlsx')\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDOysPBRtXSQ",
        "outputId": "239c8875-d544-4efb-bb48-729b66b08273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3852,  0.2703, -0.5149,  ...,  0.3542,  0.4892, -0.2395],\n",
            "        [ 0.4783, -0.5012, -0.2637,  ..., -0.5038, -0.4614,  0.0993],\n",
            "        [ 0.0138,  0.2581, -0.2445,  ...,  0.5223, -0.4196, -0.0514],\n",
            "        ...,\n",
            "        [ 0.2618, -0.2438, -0.4598,  ..., -0.0342,  0.2702, -0.4503],\n",
            "        [ 0.1504,  0.0278,  0.2638,  ..., -0.2341,  0.2722, -0.4199],\n",
            "        [-0.5918,  0.6202,  0.4938,  ...,  0.0431,  0.1010,  0.0295]])\n"
          ]
        }
      ],
      "source": [
        "ip_data = df['X'].to_numpy()\n",
        "op_data = df['Y'].to_numpy()\n",
        "\n",
        "inputVal= np.array(op_data).astype(np.float32)\n",
        "trainVal= np.array(ip_data).astype(np.float32)\n",
        "a= torch.tensor(inputVal)\n",
        "b= torch.tensor(trainVal)\n",
        "x_op_train= a.resize_(50,100)\n",
        "x_ip_train= b.resize_(50,100)\n",
        "print(b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iFLJsGHtnUH",
        "outputId": "9ad85682-caec-4612-bd50-7e1b87a562e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10, 10])\n",
            "tensor([[[[ 0.4218,  0.4769, -0.1782, -0.2163, -0.3836,  0.5298,  0.4136,\n",
            "            0.5148,  0.5016,  0.0572],\n",
            "          [ 0.0773, -0.4411,  0.2369,  0.2160,  0.7819, -0.1769,  0.4637,\n",
            "            0.5456, -0.2534,  0.1230],\n",
            "          [-0.2436,  0.0720,  0.4428,  0.0019,  0.0833,  0.1065, -0.4476,\n",
            "           -0.4936,  0.0053,  0.1444],\n",
            "          [ 0.4887, -0.1327,  0.5191,  0.0485, -0.4773,  0.0909, -0.4065,\n",
            "           -0.4809,  0.0392,  0.0637],\n",
            "          [-0.1028,  0.5360,  0.0722,  0.5383, -0.3605,  0.5003,  0.0070,\n",
            "            0.6250,  0.0448, -0.1832],\n",
            "          [ 0.0604,  0.0864, -0.1429,  0.2553,  0.0351, -0.2889,  0.5189,\n",
            "            0.0453,  0.0798, -0.3987],\n",
            "          [ 0.1039,  0.6062, -0.1590,  0.5447,  0.5610,  0.0336,  0.4596,\n",
            "            0.4551,  0.1231, -0.3643],\n",
            "          [ 0.4037,  0.0612,  0.1317,  0.5358,  0.0396, -0.4935, -0.4829,\n",
            "           -0.5029,  0.5117,  0.1158],\n",
            "          [ 0.5343, -0.2757, -0.4683, -0.0650,  0.0400,  0.0708,  0.3636,\n",
            "            0.0665,  0.3485, -0.3805],\n",
            "          [ 0.2572,  0.0241,  0.0022, -0.0030, -0.3804,  0.0845,  0.2431,\n",
            "            0.1552,  0.0919,  0.5711]]]])\n",
            "tensor([[[[ 0.5000,  0.5000, -0.2000, -0.2000, -0.4000,  0.6000,  0.5000,\n",
            "            0.6000,  0.5000,  0.2000],\n",
            "          [ 0.0000, -0.4000,  0.2000,  0.3000,  0.5000, -0.2000,  0.6000,\n",
            "            0.5000, -0.2000,  0.2000],\n",
            "          [-0.2000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000, -0.4000,\n",
            "           -0.4000,  0.0000,  0.2000],\n",
            "          [ 0.5000, -0.2000,  0.5000,  0.0000, -0.4000,  0.0000,  0.0000,\n",
            "           -0.4000,  0.0000,  0.0000],\n",
            "          [-0.2000,  0.6000,  0.0000,  0.0000, -0.4000,  0.6000,  0.0000,\n",
            "            0.6000,  0.0000, -0.2000],\n",
            "          [ 0.0000,  0.0000, -0.2000,  0.3000,  0.0000, -0.2000,  0.6000,\n",
            "            0.0000,  0.0000, -0.4000],\n",
            "          [ 0.2000,  0.6000, -0.2000,  0.5000,  0.5000,  0.0000,  0.5000,\n",
            "            0.6000,  0.0000, -0.4000],\n",
            "          [ 0.5000,  0.0000,  0.2000,  0.6000,  0.0000, -0.4000, -0.4000,\n",
            "           -0.4000,  0.6000,  0.0000],\n",
            "          [ 0.6000, -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.3000,\n",
            "            0.0000,  0.3000, -0.4000],\n",
            "          [ 0.3000,  0.0000,  0.0000,  0.0000, -0.4000,  0.0000,  0.2000,\n",
            "            0.2000,  0.0000,  0.6000]]]])\n",
            "torch.Size([1, 1, 10, 10])\n",
            "tensor([[[[ 0.1059,  0.0190, -0.4720, -0.0012, -0.0172,  0.2841, -0.4873,\n",
            "           -0.4750, -0.4099, -0.0279],\n",
            "          [ 0.4126, -0.4753,  0.3125,  0.0585,  0.0322, -0.4855, -0.0232,\n",
            "            0.0359,  0.3941, -0.1989],\n",
            "          [-0.4377, -0.4430,  0.0093,  0.5370, -0.2246,  0.0432, -0.1796,\n",
            "            0.0602,  0.2837,  0.4458],\n",
            "          [ 0.1424,  0.1370, -0.0070,  0.2724, -0.0934,  0.3456,  0.6233,\n",
            "           -0.4458, -0.0141, -0.3899],\n",
            "          [ 0.4581, -0.1773, -0.3175,  0.4989,  0.0425,  0.0343,  0.0170,\n",
            "           -0.1713,  0.4537,  0.2542],\n",
            "          [-0.2912,  0.5484,  0.0045, -0.5487,  0.0871, -0.5207,  0.5463,\n",
            "           -0.4722,  0.5612,  0.5307],\n",
            "          [ 0.0517,  0.0067, -0.2042,  0.0075, -0.0523, -0.1850,  0.5532,\n",
            "           -0.4042,  0.0036,  0.2202],\n",
            "          [ 0.4416,  0.4994,  0.0579,  0.5305,  0.0878,  0.1498, -0.2274,\n",
            "            0.5484,  0.0845,  0.0767],\n",
            "          [-0.0019,  0.1445,  0.0629,  0.3397, -0.4955, -0.4349, -0.0043,\n",
            "           -0.3100, -0.0858,  0.0554],\n",
            "          [ 0.0187,  0.0981, -0.3990,  0.0452,  0.4897,  0.0380, -0.0121,\n",
            "           -0.4865, -0.0057, -0.2591]]]])\n",
            "tensor([[[[ 0.2000,  0.0000, -0.4000,  0.0000,  0.0000,  0.3000, -0.4000,\n",
            "           -0.4000, -0.4000,  0.0000],\n",
            "          [ 0.6000, -0.4000,  0.3000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
            "            0.0000,  0.5000, -0.2000],\n",
            "          [-0.4000, -0.4000,  0.0000,  0.5000, -0.2000,  0.2000, -0.2000,\n",
            "            0.0000,  0.3000,  0.6000],\n",
            "          [ 0.0000,  0.2000,  0.0000,  0.3000,  0.0000,  0.3000,  0.6000,\n",
            "           -0.4000,  0.0000, -0.4000],\n",
            "          [ 0.5000, -0.2000, -0.4000,  0.5000,  0.0000,  0.0000,  0.0000,\n",
            "           -0.2000,  0.5000,  0.3000],\n",
            "          [-0.2000,  0.5000,  0.0000, -0.4000,  0.2000, -0.4000,  0.6000,\n",
            "           -0.4000,  0.5000,  0.6000],\n",
            "          [ 0.2000,  0.0000, -0.2000,  0.0000,  0.0000, -0.2000,  0.5000,\n",
            "           -0.4000,  0.0000,  0.2000],\n",
            "          [ 0.6000,  0.6000,  0.0000,  0.6000,  0.2000,  0.0000, -0.2000,\n",
            "            0.5000,  0.0000,  0.0000],\n",
            "          [ 0.0000,  0.2000,  0.0000,  0.3000, -0.4000,  0.0000,  0.0000,\n",
            "           -0.2000, -0.2000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.4000,  0.0000,  0.6000,  0.0000,  0.0000,\n",
            "           -0.4000,  0.0000, -0.2000]]]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:775: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x_ip_train,x_op_train, test_size=0.2, random_state=42,shuffle=True)\n",
        "X_val, X_test1, y_val, y_test1 = train_test_split(X_test, y_test, test_size=0.5, random_state=42,shuffle=True)\n",
        "\n",
        "X_train = X_train.reshape(40,1,10,10)\n",
        "y_train = y_train.reshape(40,1,10,10)\n",
        "\n",
        "\n",
        "X_val = X_val.resize(5,1,10,10)\n",
        "y_val = y_val.resize(5,1,10,10)\n",
        "\n",
        "X_test1 = X_test1.resize(5,1,10,10)\n",
        "y_test1 = y_test1.resize(5,1,10,10)\n",
        "\n",
        "loader = DataLoader(list(zip(X_train,y_train)), shuffle=True,batch_size=1)\n",
        "test_loader=DataLoader(list(zip(X_test1,y_test1)), shuffle=True,batch_size=1)\n",
        "val_loader=DataLoader(list(zip(X_val,y_val)), shuffle=True,batch_size=1)\n",
        "\n",
        "for X_batch,Y_batch in loader:\n",
        "    print(X_batch.size())\n",
        "    print(X_batch)\n",
        "    print(Y_batch)\n",
        "    break\n",
        "for X_batch,Y_batch in val_loader:\n",
        "    print(X_batch.size())\n",
        "    print(X_batch)\n",
        "    print(Y_batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNlb1WS8t1iM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "     \n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "        self.dropout = nn.Dropout(0.2) \n",
        "        # Decoder\n",
        "        self.t_conv1 = nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=3, stride=1,padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.t_conv2 = nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=3, stride=1,padding=1)\n",
        "        self.t_conv3 = nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=3, stride=1,padding=1)\n",
        "        self.t_conv4 = nn.ConvTranspose1d(in_channels=16, out_channels=1, kernel_size=3, stride=1,padding=1)\n",
        "        self.unpool = nn.Upsample(scale_factor=2,mode='nearest')\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        #print('conv1',x.size())\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        #print('conv1pool',x.size())\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.relu(x)\n",
        "        #print('conv2',x.size())\n",
        "        #x = self.relu(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        #print('conv3',x.size())\n",
        "        x = self.relu(x)\n",
        "        x = nn.functional.relu(self.conv4(x))\n",
        "        #print('conv4',x.size())\n",
        "        x= self.dropout(x)\n",
        "        #print('dropout',x.size())\n",
        "        \n",
        "        # Decoder\n",
        "        \n",
        "        \n",
        "        x = nn.functional.relu(self.t_conv1(x))\n",
        "        #print('conv1t',x.size())\n",
        "        x = nn.functional.relu(self.t_conv2(x))\n",
        "        #print('conv2t',x.size())\n",
        "        #x = self.pool(x)\n",
        "        #print('pool',x.size())\n",
        "        #x = x.view(1,50,50)\n",
        "        #print('x_out',x.size())\n",
        "       \n",
        "        x = self.t_conv3(x)\n",
        "        x = self.unpool(x)\n",
        "        #print('conv2t',x.size())\n",
        "        #print('xunpool',x.size())\n",
        "        x = self.t_conv4(x)\n",
        "        #print('conv4t',x.size())\n",
        "        #print('conv3t',x.size())\n",
        "        #x = x[:, :, :400]\n",
        "  \n",
        "        return x\n",
        "model = Autoencoder()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "     \n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0,return_indices=True)\n",
        "        self.dropout = nn.Dropout(0.2) \n",
        "        # Decoder\n",
        "        self.t_conv1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=1,padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.t_conv2 = nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=3, stride=1,padding=1)\n",
        "        self.t_conv3 = nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=3, stride=1,padding=1)\n",
        "        self.t_conv4 = nn.ConvTranspose2d(in_channels=16, out_channels=1, kernel_size=3, stride=1,padding=1)\n",
        "        self.unpool = nn.MaxUnpool2d(kernel_size =2, stride=2, padding=0)\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        #print('x',x.size())\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        #print('conv1',x.size())\n",
        "        x = self.relu(x)\n",
        "        x, indices = self.pool(x)\n",
        "        #print('conv1pool',x.size())\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.relu(x)\n",
        "        #print('conv2',x.size())\n",
        "        #x = self.relu(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        #print('conv3',x.size())\n",
        "        x = self.relu(x)\n",
        "        x = nn.functional.relu(self.conv4(x))\n",
        "        #print('conv4',x.size())\n",
        "        x= self.dropout(x)\n",
        "        #print('dropout',x.size())\n",
        "        \n",
        "        # Decoder\n",
        "        \n",
        "        \n",
        "        x = nn.functional.relu(self.t_conv1(x))\n",
        "        #print('conv1t',x.size())\n",
        "        x = nn.functional.relu(self.t_conv2(x))\n",
        "        #print('conv2t',x.size())\n",
        "        #x = self.pool(x)\n",
        "        #print('pool',x.size())\n",
        "        #x = x.view(1,50,50)\n",
        "        #print('conv_2t',x.size())\n",
        "       \n",
        "        x = self.t_conv3(x)\n",
        "        #print('conv_3t',x.size())\n",
        "        x = self.unpool(x,indices)\n",
        "        #print('unpool',x.size())\n",
        "        #print('xunpool',x.size())\n",
        "        x = self.t_conv4(x)\n",
        "        #print('conv4t',x.size())\n",
        "        #print('conv3t',x.size())\n",
        "        #x = x[:, :, :400]\n",
        "  \n",
        "        return x\n",
        "model = Autoencoder()\n"
      ],
      "metadata": {
        "id": "kH8DVtBvRfSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 8, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encode\n",
        "        x = self.encoder(x)\n",
        "        # Decode\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "model = Autoencoder()"
      ],
      "metadata": {
        "id": "4H2Y28d-D3eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy4w124It6eP"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader,loss_fn, optimizer):\n",
        "    num_correct=0\n",
        "    size = len(dataloader.dataset)\n",
        "    print(size)\n",
        "    for batch, (X, y) in enumerate(dataloader):       \n",
        "        \n",
        "        pred = model(X)       \n",
        "        #pred = pred.reshape(1,400)  \n",
        "        #print('pred',pred)     \n",
        "        loss = loss_fn(pred.float(), y.float())\n",
        "        #num_correct += (pred.argmax(1) == y).type(torch.float).sum().item()       \n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer.step() \n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    #num_correct /=size\n",
        "    #accuracy = 100 * num_correct\n",
        "    #print(f\"Accuracy: {accuracy:.2f}% \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk2cI_QMuA2b"
      },
      "outputs": [],
      "source": [
        "def val_loop(dataloader, loss_fn):\n",
        "    size = len(dataloader.dataset)    \n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    test_loss, correct = 0, 0\n",
        "    i=1\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:           \n",
        "            #print('y',y)\n",
        "            pred = model(X) \n",
        "            #pred = pred.reshape(1,50)  \n",
        "            #print('pred',pred)         \n",
        "            test_loss += loss_fn(pred.float(), y.float()).item()            \n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            #print(test_loss)\n",
        "            \n",
        "    test_loss /= num_batches \n",
        "    #print('tstLoss',test_loss)   \n",
        "    #correct /= size\n",
        "    print(f\"Test Error:  Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGxki6KeuMiG"
      },
      "outputs": [],
      "source": [
        "x =[]\n",
        "Y = []\n",
        "preds =[]\n",
        "\n",
        "def test_loop(dataloader, loss_fn):\n",
        "    size = len(dataloader.dataset)    \n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    test_loss, correct = 0, 0\n",
        "    i=1\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "             \n",
        "            print('x',X)          \n",
        "            print('y',y)\n",
        "            pred = model(X) \n",
        "            #pred = pred.reshape(1,100)  \n",
        "            print('pred',pred)  \n",
        "            x.append(X)\n",
        "            Y.append(y)\n",
        "            preds.append(pred)\n",
        "            test_loss += loss_fn(pred.float(), y.float()).item()            \n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            #print(test_loss)\n",
        "            \n",
        "    test_loss /= num_batches \n",
        "    #print('tstLoss',test_loss)   \n",
        "    correct /= size\n",
        "    print(f\"Test Error:  Avg loss: {test_loss:>8f} \\n\")\n",
        "    print(x)\n",
        "    print(Y)\n",
        "    print(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHlMKACuuQAD",
        "outputId": "5867d256-54cb-401b-aa94-8e1eb0d6f252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.098260  [    1/   40]\n",
            "loss: 0.088022  [   21/   40]\n",
            "Test Error:  Avg loss: 0.081935 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.082801  [    1/   40]\n",
            "loss: 0.070835  [   21/   40]\n",
            "Test Error:  Avg loss: 0.077816 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.100652  [    1/   40]\n",
            "loss: 0.079630  [   21/   40]\n",
            "Test Error:  Avg loss: 0.075483 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.088406  [    1/   40]\n",
            "loss: 0.098702  [   21/   40]\n",
            "Test Error:  Avg loss: 0.074315 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.064748  [    1/   40]\n",
            "loss: 0.081173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.073223 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.078052  [    1/   40]\n",
            "loss: 0.070188  [   21/   40]\n",
            "Test Error:  Avg loss: 0.072416 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.114752  [    1/   40]\n",
            "loss: 0.096981  [   21/   40]\n",
            "Test Error:  Avg loss: 0.071257 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.071289  [    1/   40]\n",
            "loss: 0.084864  [   21/   40]\n",
            "Test Error:  Avg loss: 0.070870 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.081188  [    1/   40]\n",
            "loss: 0.107915  [   21/   40]\n",
            "Test Error:  Avg loss: 0.070113 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.082543  [    1/   40]\n",
            "loss: 0.078374  [   21/   40]\n",
            "Test Error:  Avg loss: 0.069258 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.081060  [    1/   40]\n",
            "loss: 0.075956  [   21/   40]\n",
            "Test Error:  Avg loss: 0.068732 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.078162  [    1/   40]\n",
            "loss: 0.076019  [   21/   40]\n",
            "Test Error:  Avg loss: 0.067348 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.076064  [    1/   40]\n",
            "loss: 0.067763  [   21/   40]\n",
            "Test Error:  Avg loss: 0.066866 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.060573  [    1/   40]\n",
            "loss: 0.088238  [   21/   40]\n",
            "Test Error:  Avg loss: 0.066752 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.086049  [    1/   40]\n",
            "loss: 0.071302  [   21/   40]\n",
            "Test Error:  Avg loss: 0.065845 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.067837  [    1/   40]\n",
            "loss: 0.071274  [   21/   40]\n",
            "Test Error:  Avg loss: 0.065075 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.075135  [    1/   40]\n",
            "loss: 0.085826  [   21/   40]\n",
            "Test Error:  Avg loss: 0.064922 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.059366  [    1/   40]\n",
            "loss: 0.078156  [   21/   40]\n",
            "Test Error:  Avg loss: 0.064435 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.059944  [    1/   40]\n",
            "loss: 0.064413  [   21/   40]\n",
            "Test Error:  Avg loss: 0.065059 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.062263  [    1/   40]\n",
            "loss: 0.065897  [   21/   40]\n",
            "Test Error:  Avg loss: 0.063312 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.064734  [    1/   40]\n",
            "loss: 0.053884  [   21/   40]\n",
            "Test Error:  Avg loss: 0.062648 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.065058  [    1/   40]\n",
            "loss: 0.048996  [   21/   40]\n",
            "Test Error:  Avg loss: 0.061000 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.070362  [    1/   40]\n",
            "loss: 0.060417  [   21/   40]\n",
            "Test Error:  Avg loss: 0.060715 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.058520  [    1/   40]\n",
            "loss: 0.071973  [   21/   40]\n",
            "Test Error:  Avg loss: 0.058624 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.055718  [    1/   40]\n",
            "loss: 0.063150  [   21/   40]\n",
            "Test Error:  Avg loss: 0.056859 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.054771  [    1/   40]\n",
            "loss: 0.044473  [   21/   40]\n",
            "Test Error:  Avg loss: 0.057485 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.057350  [    1/   40]\n",
            "loss: 0.066833  [   21/   40]\n",
            "Test Error:  Avg loss: 0.056081 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.054315  [    1/   40]\n",
            "loss: 0.057355  [   21/   40]\n",
            "Test Error:  Avg loss: 0.054750 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.052682  [    1/   40]\n",
            "loss: 0.056628  [   21/   40]\n",
            "Test Error:  Avg loss: 0.055247 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.053006  [    1/   40]\n",
            "loss: 0.053040  [   21/   40]\n",
            "Test Error:  Avg loss: 0.053373 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.051947  [    1/   40]\n",
            "loss: 0.053243  [   21/   40]\n",
            "Test Error:  Avg loss: 0.053056 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.057946  [    1/   40]\n",
            "loss: 0.042644  [   21/   40]\n",
            "Test Error:  Avg loss: 0.052415 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.052221  [    1/   40]\n",
            "loss: 0.047909  [   21/   40]\n",
            "Test Error:  Avg loss: 0.051325 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.051177  [    1/   40]\n",
            "loss: 0.051244  [   21/   40]\n",
            "Test Error:  Avg loss: 0.050664 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.049837  [    1/   40]\n",
            "loss: 0.051663  [   21/   40]\n",
            "Test Error:  Avg loss: 0.051035 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.046496  [    1/   40]\n",
            "loss: 0.047598  [   21/   40]\n",
            "Test Error:  Avg loss: 0.049557 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.046773  [    1/   40]\n",
            "loss: 0.046328  [   21/   40]\n",
            "Test Error:  Avg loss: 0.049740 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.040975  [    1/   40]\n",
            "loss: 0.045509  [   21/   40]\n",
            "Test Error:  Avg loss: 0.048919 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.043875  [    1/   40]\n",
            "loss: 0.055367  [   21/   40]\n",
            "Test Error:  Avg loss: 0.047622 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.052795  [    1/   40]\n",
            "loss: 0.045992  [   21/   40]\n",
            "Test Error:  Avg loss: 0.047811 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.046507  [    1/   40]\n",
            "loss: 0.032539  [   21/   40]\n",
            "Test Error:  Avg loss: 0.047581 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.035294  [    1/   40]\n",
            "loss: 0.042999  [   21/   40]\n",
            "Test Error:  Avg loss: 0.047604 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.036924  [    1/   40]\n",
            "loss: 0.039104  [   21/   40]\n",
            "Test Error:  Avg loss: 0.046658 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.036084  [    1/   40]\n",
            "loss: 0.048988  [   21/   40]\n",
            "Test Error:  Avg loss: 0.046646 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.047197  [    1/   40]\n",
            "loss: 0.041559  [   21/   40]\n",
            "Test Error:  Avg loss: 0.045590 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.040101  [    1/   40]\n",
            "loss: 0.040875  [   21/   40]\n",
            "Test Error:  Avg loss: 0.045324 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.040005  [    1/   40]\n",
            "loss: 0.034885  [   21/   40]\n",
            "Test Error:  Avg loss: 0.045398 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.039262  [    1/   40]\n",
            "loss: 0.047293  [   21/   40]\n",
            "Test Error:  Avg loss: 0.044714 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.043241  [    1/   40]\n",
            "loss: 0.038921  [   21/   40]\n",
            "Test Error:  Avg loss: 0.043194 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.046761  [    1/   40]\n",
            "loss: 0.034182  [   21/   40]\n",
            "Test Error:  Avg loss: 0.043590 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.035094  [    1/   40]\n",
            "loss: 0.033732  [   21/   40]\n",
            "Test Error:  Avg loss: 0.044337 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.036909  [    1/   40]\n",
            "loss: 0.032992  [   21/   40]\n",
            "Test Error:  Avg loss: 0.043850 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.039569  [    1/   40]\n",
            "loss: 0.048727  [   21/   40]\n",
            "Test Error:  Avg loss: 0.043263 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.034697  [    1/   40]\n",
            "loss: 0.045312  [   21/   40]\n",
            "Test Error:  Avg loss: 0.043446 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.030464  [    1/   40]\n",
            "loss: 0.036595  [   21/   40]\n",
            "Test Error:  Avg loss: 0.042619 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.030614  [    1/   40]\n",
            "loss: 0.035565  [   21/   40]\n",
            "Test Error:  Avg loss: 0.042671 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.031033  [    1/   40]\n",
            "loss: 0.034044  [   21/   40]\n",
            "Test Error:  Avg loss: 0.043443 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.039606  [    1/   40]\n",
            "loss: 0.036273  [   21/   40]\n",
            "Test Error:  Avg loss: 0.041154 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.033621  [    1/   40]\n",
            "loss: 0.036010  [   21/   40]\n",
            "Test Error:  Avg loss: 0.041263 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.036650  [    1/   40]\n",
            "loss: 0.030532  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040526 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.033689  [    1/   40]\n",
            "loss: 0.025108  [   21/   40]\n",
            "Test Error:  Avg loss: 0.041754 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.028500  [    1/   40]\n",
            "loss: 0.029204  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040919 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.029088  [    1/   40]\n",
            "loss: 0.035735  [   21/   40]\n",
            "Test Error:  Avg loss: 0.041515 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.032830  [    1/   40]\n",
            "loss: 0.031605  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040046 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.032014  [    1/   40]\n",
            "loss: 0.026704  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039823 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.030253  [    1/   40]\n",
            "loss: 0.041628  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040357 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.033571  [    1/   40]\n",
            "loss: 0.034247  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040595 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.029374  [    1/   40]\n",
            "loss: 0.036036  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040810 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.024863  [    1/   40]\n",
            "loss: 0.029271  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040113 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.031077  [    1/   40]\n",
            "loss: 0.030547  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040185 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.030575  [    1/   40]\n",
            "loss: 0.028531  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040283 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.034816  [    1/   40]\n",
            "loss: 0.023837  [   21/   40]\n",
            "Test Error:  Avg loss: 0.040335 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.034220  [    1/   40]\n",
            "loss: 0.023027  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039698 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023395  [    1/   40]\n",
            "loss: 0.027999  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039098 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.028432  [    1/   40]\n",
            "loss: 0.033506  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039121 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023873  [    1/   40]\n",
            "loss: 0.029104  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039254 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027156  [    1/   40]\n",
            "loss: 0.018447  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039050 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027850  [    1/   40]\n",
            "loss: 0.035504  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039065 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019628  [    1/   40]\n",
            "loss: 0.019121  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039547 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.034468  [    1/   40]\n",
            "loss: 0.033169  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039757 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.024946  [    1/   40]\n",
            "loss: 0.026268  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039193 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027891  [    1/   40]\n",
            "loss: 0.021416  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038770 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.026379  [    1/   40]\n",
            "loss: 0.034149  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038017 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.025323  [    1/   40]\n",
            "loss: 0.026458  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039093 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016804  [    1/   40]\n",
            "loss: 0.032160  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038841 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.025110  [    1/   40]\n",
            "loss: 0.024521  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039486 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.026933  [    1/   40]\n",
            "loss: 0.022252  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038726 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.024092  [    1/   40]\n",
            "loss: 0.021991  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038797 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021190  [    1/   40]\n",
            "loss: 0.016188  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038432 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.031781  [    1/   40]\n",
            "loss: 0.024733  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038120 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020467  [    1/   40]\n",
            "loss: 0.023655  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039022 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.022660  [    1/   40]\n",
            "loss: 0.023515  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036635 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016205  [    1/   40]\n",
            "loss: 0.026473  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038079 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021523  [    1/   40]\n",
            "loss: 0.023585  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037413 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019558  [    1/   40]\n",
            "loss: 0.025422  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038420 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023000  [    1/   40]\n",
            "loss: 0.025223  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038157 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.022954  [    1/   40]\n",
            "loss: 0.015071  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037501 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023568  [    1/   40]\n",
            "loss: 0.021779  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037897 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.022190  [    1/   40]\n",
            "loss: 0.024538  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037110 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020691  [    1/   40]\n",
            "loss: 0.021990  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037430 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018737  [    1/   40]\n",
            "loss: 0.020162  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036347 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027370  [    1/   40]\n",
            "loss: 0.019289  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036203 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018170  [    1/   40]\n",
            "loss: 0.021081  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037688 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023615  [    1/   40]\n",
            "loss: 0.025735  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037102 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018950  [    1/   40]\n",
            "loss: 0.017522  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037572 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018988  [    1/   40]\n",
            "loss: 0.020075  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037229 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012459  [    1/   40]\n",
            "loss: 0.012810  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037004 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021902  [    1/   40]\n",
            "loss: 0.026239  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037794 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019073  [    1/   40]\n",
            "loss: 0.019191  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038263 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021653  [    1/   40]\n",
            "loss: 0.012869  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037586 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012359  [    1/   40]\n",
            "loss: 0.016169  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037576 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014597  [    1/   40]\n",
            "loss: 0.017191  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038041 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018969  [    1/   40]\n",
            "loss: 0.018177  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037135 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011813  [    1/   40]\n",
            "loss: 0.013900  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037299 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016024  [    1/   40]\n",
            "loss: 0.017323  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037571 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015803  [    1/   40]\n",
            "loss: 0.012554  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037784 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019660  [    1/   40]\n",
            "loss: 0.019067  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036888 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014815  [    1/   40]\n",
            "loss: 0.015796  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037504 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018901  [    1/   40]\n",
            "loss: 0.018117  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036798 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019090  [    1/   40]\n",
            "loss: 0.016629  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037042 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016155  [    1/   40]\n",
            "loss: 0.018325  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038344 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012683  [    1/   40]\n",
            "loss: 0.017837  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037054 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.022674  [    1/   40]\n",
            "loss: 0.015723  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036957 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015351  [    1/   40]\n",
            "loss: 0.014075  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036874 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015464  [    1/   40]\n",
            "loss: 0.011643  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036703 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020029  [    1/   40]\n",
            "loss: 0.017140  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036433 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016635  [    1/   40]\n",
            "loss: 0.016255  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036788 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014805  [    1/   40]\n",
            "loss: 0.017943  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037580 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014788  [    1/   40]\n",
            "loss: 0.018435  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037704 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014662  [    1/   40]\n",
            "loss: 0.016690  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037632 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018662  [    1/   40]\n",
            "loss: 0.017423  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037193 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016781  [    1/   40]\n",
            "loss: 0.015780  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037049 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014713  [    1/   40]\n",
            "loss: 0.018019  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037311 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012929  [    1/   40]\n",
            "loss: 0.011419  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036330 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013722  [    1/   40]\n",
            "loss: 0.014275  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037954 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014175  [    1/   40]\n",
            "loss: 0.016563  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036301 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015610  [    1/   40]\n",
            "loss: 0.011491  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036200 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011597  [    1/   40]\n",
            "loss: 0.014161  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037474 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013493  [    1/   40]\n",
            "loss: 0.014098  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038158 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018762  [    1/   40]\n",
            "loss: 0.017553  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037393 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011287  [    1/   40]\n",
            "loss: 0.015294  [   21/   40]\n",
            "Test Error:  Avg loss: 0.035853 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011864  [    1/   40]\n",
            "loss: 0.008457  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036204 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007879  [    1/   40]\n",
            "loss: 0.017275  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036345 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015890  [    1/   40]\n",
            "loss: 0.013498  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037694 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013003  [    1/   40]\n",
            "loss: 0.012532  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037217 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017103  [    1/   40]\n",
            "loss: 0.013367  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036297 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013855  [    1/   40]\n",
            "loss: 0.010159  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036385 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015302  [    1/   40]\n",
            "loss: 0.010920  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037853 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012579  [    1/   40]\n",
            "loss: 0.013968  [   21/   40]\n",
            "Test Error:  Avg loss: 0.035879 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008548  [    1/   40]\n",
            "loss: 0.016950  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036402 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009187  [    1/   40]\n",
            "loss: 0.016146  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036323 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020093  [    1/   40]\n",
            "loss: 0.013412  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037388 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014083  [    1/   40]\n",
            "loss: 0.013869  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037223 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010630  [    1/   40]\n",
            "loss: 0.009238  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037992 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011524  [    1/   40]\n",
            "loss: 0.009129  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036100 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014404  [    1/   40]\n",
            "loss: 0.012891  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038067 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011502  [    1/   40]\n",
            "loss: 0.014241  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036618 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010745  [    1/   40]\n",
            "loss: 0.008242  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037432 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011038  [    1/   40]\n",
            "loss: 0.014586  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036642 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013596  [    1/   40]\n",
            "loss: 0.014141  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037342 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009673  [    1/   40]\n",
            "loss: 0.011389  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037596 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013976  [    1/   40]\n",
            "loss: 0.012293  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038224 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013855  [    1/   40]\n",
            "loss: 0.009929  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037623 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012093  [    1/   40]\n",
            "loss: 0.010983  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036354 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010879  [    1/   40]\n",
            "loss: 0.012679  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036964 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013291  [    1/   40]\n",
            "loss: 0.006775  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037743 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009868  [    1/   40]\n",
            "loss: 0.010507  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037848 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011037  [    1/   40]\n",
            "loss: 0.012477  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038365 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011081  [    1/   40]\n",
            "loss: 0.007047  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036637 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011449  [    1/   40]\n",
            "loss: 0.010438  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037962 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009699  [    1/   40]\n",
            "loss: 0.011085  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037815 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010888  [    1/   40]\n",
            "loss: 0.009940  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037760 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009600  [    1/   40]\n",
            "loss: 0.010645  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038726 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008539  [    1/   40]\n",
            "loss: 0.009924  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036976 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009299  [    1/   40]\n",
            "loss: 0.010903  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037554 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007961  [    1/   40]\n",
            "loss: 0.009803  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038190 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010367  [    1/   40]\n",
            "loss: 0.011857  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038456 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008022  [    1/   40]\n",
            "loss: 0.009140  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036782 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008047  [    1/   40]\n",
            "loss: 0.008376  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038601 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008399  [    1/   40]\n",
            "loss: 0.009519  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037315 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009545  [    1/   40]\n",
            "loss: 0.012000  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036171 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006852  [    1/   40]\n",
            "loss: 0.012556  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036874 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009158  [    1/   40]\n",
            "loss: 0.010448  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036596 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008808  [    1/   40]\n",
            "loss: 0.009656  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038112 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008140  [    1/   40]\n",
            "loss: 0.010611  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038109 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006475  [    1/   40]\n",
            "loss: 0.008634  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037610 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005584  [    1/   40]\n",
            "loss: 0.008821  [   21/   40]\n",
            "Test Error:  Avg loss: 0.035904 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008503  [    1/   40]\n",
            "loss: 0.010852  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037254 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007229  [    1/   40]\n",
            "loss: 0.010824  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037287 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012759  [    1/   40]\n",
            "loss: 0.007629  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037405 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007650  [    1/   40]\n",
            "loss: 0.009294  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036539 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007652  [    1/   40]\n",
            "loss: 0.007419  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038063 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008001  [    1/   40]\n",
            "loss: 0.012768  [   21/   40]\n",
            "Test Error:  Avg loss: 0.038424 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009891  [    1/   40]\n",
            "loss: 0.008553  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037219 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011684  [    1/   40]\n",
            "loss: 0.006998  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037233 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008676  [    1/   40]\n",
            "loss: 0.010957  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037565 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010228  [    1/   40]\n",
            "loss: 0.009614  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036741 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005956  [    1/   40]\n",
            "loss: 0.011783  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037229 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.004850  [    1/   40]\n",
            "loss: 0.010406  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036463 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006401  [    1/   40]\n",
            "loss: 0.009085  [   21/   40]\n",
            "Test Error:  Avg loss: 0.036866 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001,momentum=0.9)\n",
        "\n",
        "epochs = 200\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(loader, loss_fn, optimizer)\n",
        "    val_loop(val_loader, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh4tsrGLuTEy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "torch.save(model.state_dict(), 'saved_data_model.pt')\n",
        "torch.save(optimizer.state_dict(), 'saved_data_optimizer.pt')\n",
        "\n",
        "\n",
        "model = Autoencoder()\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "#optimizer = torch.optim.Adam(params_to_optimize, lr=0.0001, weight_decay=1e-05)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
        "\n",
        "model.load_state_dict(torch.load('saved_data_model.pt'))\n",
        "optimizer.load_state_dict(torch.load('saved_data_optimizer.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfKln-1tup6v",
        "outputId": "1638df12-aff5-47ae-f8a7-28248947f62d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "x tensor([[[[ 0.0328, -0.4573, -0.4725, -0.0247,  0.0540,  0.0859,  0.4515,\n",
            "            0.1295, -0.2223, -0.0262],\n",
            "          [ 0.0335, -0.4611,  0.4399, -0.0082, -0.4945, -0.3633, -0.2324,\n",
            "            0.5773,  0.6022,  0.2374],\n",
            "          [ 0.0691, -0.0194,  0.4261,  0.1651, -0.2748,  0.5246,  0.0692,\n",
            "            0.4750,  0.1088,  0.0235],\n",
            "          [ 0.1454,  0.3780,  0.3894,  0.4578, -0.4706,  0.5107,  0.1996,\n",
            "            0.4277,  0.6494,  0.1303],\n",
            "          [ 0.5510,  0.4877,  0.4463,  0.4929, -0.0060,  0.0577, -0.4770,\n",
            "            0.0755,  0.0523, -0.5525],\n",
            "          [ 0.0088,  0.4627,  0.0546,  0.5075,  0.6435,  0.1435,  0.4656,\n",
            "           -0.3355,  0.5981,  0.1251],\n",
            "          [ 0.1323,  0.1832, -0.4654, -0.2253,  0.5101, -0.0036,  0.3210,\n",
            "            0.0282,  0.1331,  0.5667],\n",
            "          [ 0.2723, -0.1965, -0.2652,  0.4898,  0.0672,  0.4451,  0.0650,\n",
            "           -0.0210,  0.0807,  0.0530],\n",
            "          [-0.5118, -0.4409,  0.1643, -0.2564, -0.4262,  0.2940, -0.5093,\n",
            "            0.3116, -0.3308, -0.5235],\n",
            "          [ 0.4505,  0.2786,  0.5227,  0.5679, -0.1540, -0.2014,  0.0721,\n",
            "           -0.2696,  0.4109,  0.0056]]]])\n",
            "y tensor([[[[ 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "            0.0000, -0.2000,  0.0000],\n",
            "          [ 0.0000, -0.4000,  0.5000,  0.0000, -0.4000, -0.4000, -0.2000,\n",
            "            0.6000,  0.6000,  0.3000],\n",
            "          [ 0.0000,  0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,\n",
            "            0.5000,  0.0000,  0.0000],\n",
            "          [ 0.2000,  0.5000,  0.3000,  0.5000, -0.4000,  0.5000,  0.2000,\n",
            "            0.5000,  0.6000,  0.2000],\n",
            "          [ 0.5000,  0.5000,  0.5000,  0.5000,  0.0000,  0.0000, -0.4000,\n",
            "            0.0000,  0.0000, -0.4000],\n",
            "          [ 0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,  0.6000,\n",
            "           -0.4000,  0.6000,  0.2000],\n",
            "          [ 0.0000,  0.2000, -0.4000, -0.2000,  0.6000,  0.0000,  0.3000,\n",
            "            0.0000,  0.2000,  0.6000],\n",
            "          [ 0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
            "            0.0000,  0.0000,  0.0000],\n",
            "          [-0.4000, -0.4000,  0.2000, -0.2000, -0.4000,  0.3000, -0.4000,\n",
            "            0.3000, -0.2000, -0.4000],\n",
            "          [ 0.5000,  0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000,\n",
            "           -0.2000,  0.5000,  0.0000]]]])\n",
            "pred tensor([[[[ 0.1245, -0.4191, -0.4042, -0.0309, -0.0127,  0.0894,  0.3684,\n",
            "            0.0634, -0.1731, -0.1370],\n",
            "          [ 0.0398, -0.4175,  0.3953, -0.0818, -0.4071, -0.3251, -0.1423,\n",
            "            0.5217,  0.6768,  0.2523],\n",
            "          [ 0.1053, -0.0477,  0.4147,  0.2054, -0.3016,  0.5511,  0.0260,\n",
            "            0.3450,  0.2064, -0.0643],\n",
            "          [ 0.2353,  0.3881,  0.3941,  0.5507, -0.3601,  0.3502,  0.3635,\n",
            "            0.3598,  0.6160,  0.1088],\n",
            "          [ 0.6676,  0.4080,  0.2772,  0.5105,  0.0704,  0.0563, -0.3605,\n",
            "            0.0728, -0.0465, -0.4352],\n",
            "          [ 0.1010,  0.5673,  0.0103,  0.4505,  0.5784,  0.2057,  0.4969,\n",
            "           -0.2954,  0.5521,  0.2517],\n",
            "          [ 0.1120,  0.1227, -0.4469, -0.2185,  0.5233,  0.0469,  0.4626,\n",
            "            0.1437,  0.2577,  0.5091],\n",
            "          [ 0.2159, -0.1136, -0.2118,  0.5507,  0.0661,  0.4159,  0.1155,\n",
            "           -0.0646,  0.0714, -0.0338],\n",
            "          [-0.3502, -0.3897,  0.1530, -0.1984, -0.2786,  0.2881, -0.4132,\n",
            "            0.3232, -0.3257, -0.5354],\n",
            "          [ 0.3498,  0.3488,  0.4834,  0.4229, -0.2028, -0.1964,  0.0126,\n",
            "           -0.2553,  0.3892,  0.0238]]]])\n",
            "x tensor([[[[ 1.5040e-01,  2.7753e-02,  2.6380e-01, -4.1143e-01,  5.8431e-01,\n",
            "            3.5105e-01,  3.9801e-02,  6.2555e-02,  4.4214e-01, -4.6540e-01],\n",
            "          [ 1.9462e-01,  5.3590e-02, -4.6464e-01, -3.0317e-01,  1.7399e-02,\n",
            "            1.5962e-03, -2.5415e-01,  3.5795e-01, -3.8097e-01,  4.9359e-01],\n",
            "          [ 4.8813e-01,  4.1093e-01, -2.1811e-01,  8.8324e-02,  2.7562e-01,\n",
            "           -7.1405e-02,  4.5740e-01,  3.1607e-01,  2.5520e-01,  3.7994e-02],\n",
            "          [ 5.7717e-01,  1.2074e-02,  6.7664e-02, -6.1348e-03, -4.8868e-02,\n",
            "           -6.5389e-02, -5.4172e-01,  5.6659e-01,  3.2106e-02,  1.2346e-01],\n",
            "          [ 7.8257e-02,  4.5159e-01,  4.8174e-01,  3.4322e-02, -6.3859e-02,\n",
            "            8.9762e-02, -3.6590e-01,  2.7255e-01,  5.2111e-02, -2.3775e-01],\n",
            "          [ 2.0834e-01, -4.0831e-01,  2.9185e-01, -6.6780e-06,  7.1918e-02,\n",
            "           -3.3397e-02,  4.5936e-01, -2.7687e-01,  2.5574e-01, -5.4087e-03],\n",
            "          [ 1.0142e-01, -4.8906e-01,  2.7102e-02,  6.3380e-02, -2.2070e-01,\n",
            "           -2.2836e-02, -5.0412e-02,  4.2620e-01, -4.2748e-01,  4.2188e-01],\n",
            "          [-5.3945e-01, -3.4004e-01, -4.1858e-01, -3.2877e-01, -2.2333e-01,\n",
            "            5.4785e-02,  3.2787e-01,  2.5013e-01,  8.4432e-02,  4.8181e-01],\n",
            "          [ 1.7438e-02, -5.4653e-01,  4.1048e-02, -4.2961e-01,  3.5564e-01,\n",
            "            3.8394e-03,  1.0678e-01,  5.4504e-02, -4.8490e-01,  3.6136e-02],\n",
            "          [ 4.7570e-01,  1.6878e-01,  4.7856e-01,  2.5320e-02,  6.2321e-02,\n",
            "            2.8568e-02, -5.0877e-01, -2.3411e-01,  2.7215e-01, -4.1985e-01]]]])\n",
            "y tensor([[[[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
            "            0.0000,  0.5000, -0.4000],\n",
            "          [ 0.2000,  0.0000, -0.4000, -0.2000,  0.0000,  0.0000, -0.2000,\n",
            "            0.5000, -0.4000,  0.5000],\n",
            "          [ 0.5000,  0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,\n",
            "            0.3000,  0.3000,  0.0000],\n",
            "          [ 0.6000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "            0.6000,  0.0000,  0.2000],\n",
            "          [ 0.2000,  0.5000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "            0.3000,  0.0000, -0.2000],\n",
            "          [ 0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "           -0.2000,  0.2000,  0.0000],\n",
            "          [ 0.2000, -0.4000,  0.0000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
            "            0.6000, -0.4000,  0.5000],\n",
            "          [-0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
            "            0.3000,  0.2000,  0.5000],\n",
            "          [ 0.0000, -0.4000,  0.0000, -0.4000,  0.3000,  0.0000,  0.0000,\n",
            "            0.0000, -0.4000,  0.0000],\n",
            "          [ 0.5000,  0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "           -0.2000,  0.3000, -0.4000]]]])\n",
            "pred tensor([[[[-0.0008,  0.0507,  0.2621, -0.4382,  0.4404,  0.3615,  0.0831,\n",
            "           -0.0107,  0.3775, -0.3198],\n",
            "          [ 0.1656, -0.0037, -0.4014, -0.2951, -0.0223,  0.0514, -0.0676,\n",
            "            0.3716, -0.2980,  0.4366],\n",
            "          [ 0.6147,  0.4943, -0.2243, -0.0156,  0.2958, -0.0489,  0.4820,\n",
            "            0.1849,  0.2832,  0.0641],\n",
            "          [ 0.5896,  0.0512,  0.0163,  0.0299, -0.0111, -0.0522, -0.4083,\n",
            "            0.6522,  0.0914,  0.1229],\n",
            "          [ 0.0665,  0.3549,  0.6872,  0.0966, -0.1152,  0.0983, -0.3330,\n",
            "            0.2783, -0.0236, -0.2592],\n",
            "          [ 0.1426, -0.3869,  0.3097,  0.0212,  0.0915,  0.0180,  0.4120,\n",
            "           -0.3299,  0.2980,  0.0721],\n",
            "          [ 0.1013, -0.3693, -0.0614,  0.0546, -0.3093, -0.0048, -0.1455,\n",
            "            0.4906, -0.3684,  0.2382],\n",
            "          [-0.4091, -0.2860, -0.3765, -0.3276, -0.3263,  0.0961,  0.3766,\n",
            "            0.1952,  0.0975,  0.3634],\n",
            "          [ 0.0602, -0.3755,  0.0515, -0.3295,  0.3461, -0.0403,  0.1355,\n",
            "            0.0007, -0.4528,  0.0649],\n",
            "          [ 0.3373,  0.2691,  0.4083,  0.0369,  0.0405,  0.0047, -0.4675,\n",
            "           -0.1889,  0.2920, -0.3130]]]])\n",
            "x tensor([[[[ 1.5207e-01, -4.5816e-01, -3.2321e-01,  1.2050e-02,  1.2225e-01,\n",
            "           -4.5844e-01,  8.9613e-02, -2.7273e-01,  1.1309e-01, -2.3914e-02],\n",
            "          [ 5.0364e-01,  1.5636e-01,  4.9550e-02, -2.3475e-01, -2.3575e-01,\n",
            "            1.8438e-01,  3.8710e-01, -4.4908e-01,  2.7194e-01,  3.4552e-01],\n",
            "          [ 1.2505e-01, -3.8047e-02,  1.4039e-02,  4.5893e-01,  6.1139e-02,\n",
            "           -3.2275e-01, -5.0564e-01,  2.7278e-01, -1.5879e-02,  3.7046e-01],\n",
            "          [-4.4813e-01,  1.6258e-01,  4.9993e-01, -4.5045e-01,  2.6548e-02,\n",
            "            5.0095e-01, -7.1420e-03,  8.5321e-02, -4.7502e-01, -5.1078e-01],\n",
            "          [-7.2781e-02, -4.2473e-01,  1.7987e-01,  1.8869e-03,  3.7284e-02,\n",
            "            9.7761e-02, -2.4007e-01, -1.9300e-02, -2.1564e-01, -4.7269e-01],\n",
            "          [-4.6971e-01, -3.4520e-01,  4.3727e-01,  5.0005e-01, -2.6470e-01,\n",
            "            8.2406e-02,  7.9866e-02,  5.3849e-02, -2.4046e-01,  1.3397e-01],\n",
            "          [-2.0271e-01,  5.6790e-01,  2.6949e-01,  7.5627e-02,  4.4006e-01,\n",
            "            5.0170e-01, -4.4758e-01, -4.6685e-01,  1.3127e-01,  4.5948e-02],\n",
            "          [-2.7190e-01,  7.3369e-02,  6.3947e-02,  1.2952e-01,  4.9203e-01,\n",
            "            7.1726e-02, -2.7601e-02, -4.5895e-01, -4.7017e-01,  4.4787e-02],\n",
            "          [-2.5383e-02,  4.4729e-01,  4.4143e-01, -2.3552e-01, -4.8626e-01,\n",
            "            3.9564e-01, -1.8680e-01, -8.8002e-03,  1.1407e-01, -2.9984e-01],\n",
            "          [-3.4489e-02, -5.1843e-01,  2.4105e-02,  6.2846e-01,  8.5015e-02,\n",
            "            3.6069e-02, -2.1637e-04, -4.3929e-02, -5.0672e-01, -3.9851e-01]]]])\n",
            "y tensor([[[[ 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
            "           -0.2000,  0.2000,  0.0000],\n",
            "          [ 0.5000,  0.2000,  0.0000, -0.2000, -0.2000,  0.2000,  0.5000,\n",
            "           -0.4000,  0.3000,  0.3000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,\n",
            "            0.3000,  0.0000,  0.3000],\n",
            "          [-0.4000,  0.2000,  0.5000, -0.4000,  0.0000,  0.5000,  0.0000,\n",
            "            0.2000, -0.4000, -0.4000],\n",
            "          [ 0.0000, -0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.2000,\n",
            "            0.0000, -0.2000, -0.4000],\n",
            "          [-0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,  0.0000,\n",
            "            0.0000, -0.2000,  0.2000],\n",
            "          [-0.2000,  0.6000,  0.3000,  0.0000,  0.5000,  0.6000, -0.4000,\n",
            "           -0.4000,  0.2000,  0.0000],\n",
            "          [-0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
            "           -0.4000, -0.4000,  0.0000],\n",
            "          [ 0.0000,  0.5000,  0.5000, -0.2000, -0.4000,  0.3000, -0.2000,\n",
            "            0.0000,  0.0000, -0.2000],\n",
            "          [ 0.0000, -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,\n",
            "            0.0000, -0.4000, -0.4000]]]])\n",
            "pred tensor([[[[ 0.1657, -0.3835, -0.2296,  0.0311,  0.1557, -0.3483,  0.0622,\n",
            "           -0.3317, -0.0186,  0.1113],\n",
            "          [ 0.6204,  0.1243,  0.0926, -0.2536, -0.2294,  0.1751,  0.2932,\n",
            "           -0.3457,  0.3515,  0.2999],\n",
            "          [-0.0295,  0.0561,  0.1426,  0.5678,  0.0526, -0.2463, -0.3848,\n",
            "            0.3139,  0.0152,  0.3710],\n",
            "          [-0.3719,  0.0472,  0.4632, -0.4147,  0.0554,  0.6607,  0.0453,\n",
            "            0.1624, -0.3120, -0.4602],\n",
            "          [-0.0711, -0.3837,  0.1580, -0.0061, -0.0020,  0.0578, -0.2047,\n",
            "           -0.0763, -0.2547, -0.3261],\n",
            "          [-0.3581, -0.3583,  0.4595,  0.5443, -0.2240,  0.1090,  0.0849,\n",
            "            0.1119, -0.2416,  0.0944],\n",
            "          [-0.1947,  0.6267,  0.1833,  0.0402,  0.3114,  0.5041, -0.3228,\n",
            "           -0.3504,  0.1593,  0.1671],\n",
            "          [-0.3594,  0.1678,  0.2038,  0.1250,  0.4123,  0.1423, -0.0276,\n",
            "           -0.4840, -0.3914, -0.0112],\n",
            "          [ 0.0271,  0.4049,  0.3513, -0.3407, -0.4511,  0.3357, -0.1676,\n",
            "           -0.0215,  0.0990, -0.2799],\n",
            "          [ 0.0384, -0.3930,  0.1465,  0.5238,  0.0917,  0.0986,  0.0244,\n",
            "           -0.0288, -0.4389, -0.2361]]]])\n",
            "x tensor([[[[-2.6127e-01, -4.6763e-01,  6.2371e-02,  5.2001e-01, -7.0590e-03,\n",
            "           -5.2496e-01,  1.4165e-01, -5.2515e-01,  2.6908e-02,  3.4106e-03],\n",
            "          [-5.3161e-01,  5.9588e-01,  6.6905e-02,  3.2951e-01,  4.4232e-01,\n",
            "           -2.1317e-02, -1.8266e-01,  4.8799e-01,  2.1765e-02, -2.6534e-02],\n",
            "          [ 1.9717e-01, -4.2183e-01, -1.4312e-02,  4.1642e-01,  2.6945e-01,\n",
            "            6.4946e-01,  2.3988e-01,  4.8178e-02, -2.6797e-02, -3.7925e-01],\n",
            "          [-5.7487e-01, -2.4094e-01, -2.5117e-01,  7.0525e-01,  1.3802e-01,\n",
            "            3.7031e-01, -5.4238e-01,  5.7693e-01,  5.1506e-01,  4.7791e-01],\n",
            "          [ 2.0435e-02,  3.5112e-02,  7.6267e-02,  6.1888e-01, -2.5576e-02,\n",
            "           -4.8947e-01, -4.7947e-01, -3.6862e-01, -4.5220e-01,  2.7477e-01],\n",
            "          [-2.1003e-01,  7.2061e-02, -2.0308e-02, -3.9764e-01,  4.9790e-01,\n",
            "            5.3874e-02, -1.4719e-02,  5.0923e-01,  1.0956e-01, -4.2560e-01],\n",
            "          [-4.1023e-01,  1.6442e-01,  1.7020e-01,  3.6917e-02, -2.5307e-01,\n",
            "            4.4099e-01,  7.4603e-05, -3.9181e-01, -1.8042e-01,  1.0617e-01],\n",
            "          [ 5.3088e-01, -2.0616e-01,  5.6170e-01,  1.7280e-01,  4.5557e-01,\n",
            "           -4.1374e-02, -5.7587e-01,  6.8499e-02, -4.1197e-01, -1.6708e-01],\n",
            "          [-1.9162e-01,  4.8799e-01,  3.8270e-01, -4.5628e-01,  1.4194e-01,\n",
            "            7.3972e-02, -1.5844e-01, -2.7551e-01, -4.5341e-01,  3.8624e-01],\n",
            "          [ 1.7708e-01,  6.0247e-02, -3.0290e-02,  6.3880e-02,  1.2618e-02,\n",
            "            4.4718e-01, -2.0038e-01,  4.0293e-01,  5.5030e-01,  4.1853e-01]]]])\n",
            "y tensor([[[[-0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
            "           -0.4000,  0.2000,  0.0000],\n",
            "          [-0.4000,  0.5000,  0.0000,  0.3000,  0.5000,  0.0000, -0.2000,\n",
            "            0.6000,  0.0000,  0.0000],\n",
            "          [ 0.2000, -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,\n",
            "            0.0000,  0.0000, -0.4000],\n",
            "          [-0.4000, -0.2000, -0.2000,  0.6000,  0.2000,  0.5000, -0.4000,\n",
            "            0.5000,  0.6000,  0.5000],\n",
            "          [ 0.0000,  0.0000,  0.2000,  0.6000,  0.0000, -0.4000, -0.4000,\n",
            "           -0.2000, -0.4000,  0.5000],\n",
            "          [-0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,  0.0000,\n",
            "            0.6000,  0.0000, -0.4000],\n",
            "          [-0.4000,  0.3000,  0.2000,  0.0000, -0.2000,  0.5000,  0.0000,\n",
            "           -0.4000, -0.2000,  0.2000],\n",
            "          [ 0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
            "            0.0000, -0.4000, -0.2000],\n",
            "          [-0.2000,  0.6000,  0.3000, -0.4000,  0.2000,  0.0000, -0.2000,\n",
            "           -0.2000, -0.4000,  0.5000],\n",
            "          [ 0.2000,  0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,\n",
            "            0.5000,  0.5000,  0.6000]]]])\n",
            "pred tensor([[[[-0.1877, -0.3030,  0.1996,  0.6096, -0.0181, -0.3243,  0.0009,\n",
            "           -0.4484,  0.0785,  0.0751],\n",
            "          [-0.4679,  0.4463,  0.0901,  0.2089,  0.5209, -0.0191, -0.2329,\n",
            "            0.4963, -0.0546, -0.0223],\n",
            "          [ 0.2146, -0.3441, -0.0100,  0.4103,  0.1737,  0.6178,  0.3528,\n",
            "            0.0749, -0.0027, -0.3158],\n",
            "          [-0.4049, -0.1828, -0.1228,  0.6777,  0.0545,  0.4376, -0.4438,\n",
            "            0.6061,  0.5786,  0.3762],\n",
            "          [ 0.0342,  0.1331,  0.0706,  0.5413, -0.0962, -0.3753, -0.4120,\n",
            "           -0.3940, -0.3769,  0.2872],\n",
            "          [-0.1953,  0.0993, -0.0077, -0.4021,  0.4976,  0.0134, -0.1036,\n",
            "            0.6081,  0.1765, -0.3565],\n",
            "          [-0.3650,  0.1350,  0.1360,  0.0717, -0.2977,  0.4775, -0.0349,\n",
            "           -0.3124, -0.1804,  0.1039],\n",
            "          [ 0.5616, -0.2728,  0.5626,  0.1826,  0.4196,  0.0138, -0.4560,\n",
            "            0.0486, -0.3379, -0.1309],\n",
            "          [-0.1296,  0.4592,  0.3626, -0.3928,  0.2075,  0.1004, -0.1168,\n",
            "           -0.2675, -0.3814,  0.2511],\n",
            "          [ 0.0974,  0.2542, -0.0372,  0.0292,  0.0793,  0.4050, -0.1619,\n",
            "            0.2179,  0.5123,  0.0878]]]])\n",
            "x tensor([[[[ 0.0618,  0.4910,  0.0659,  0.0617,  0.1002,  0.0078,  0.3823,\n",
            "            0.6141,  0.2087,  0.2565],\n",
            "          [ 0.0369,  0.0386,  0.0204,  0.0185,  0.3893, -0.4769,  0.5229,\n",
            "            0.4450, -0.4036,  0.0044],\n",
            "          [-0.0042,  0.0559,  0.0243,  0.4507, -0.1911,  0.3623, -0.4669,\n",
            "            0.2460,  0.4909, -0.0077],\n",
            "          [-0.0043,  0.0563, -0.1680,  0.0477, -0.1491,  0.1000,  0.0429,\n",
            "           -0.0586, -0.1763,  0.4750],\n",
            "          [-0.5252, -0.0102,  0.0044,  0.3419, -0.4832,  0.5341,  0.0817,\n",
            "           -0.0328,  0.2554,  0.0550],\n",
            "          [ 0.1156, -0.4956, -0.4484, -0.5622,  0.3319, -0.3887, -0.4813,\n",
            "            0.1118,  0.1446,  0.0871],\n",
            "          [ 0.3897,  0.0433, -0.4223, -0.0112, -0.0103,  0.0583,  0.0187,\n",
            "            0.4850,  0.0213,  0.5210],\n",
            "          [ 0.1450,  0.0028, -0.2643,  0.1555, -0.4445, -0.5146, -0.4895,\n",
            "           -0.0116,  0.2272,  0.6472],\n",
            "          [ 0.1607, -0.5420,  0.0293, -0.4902, -0.5888, -0.0096,  0.5049,\n",
            "           -0.0019,  0.0395,  0.5038],\n",
            "          [-0.0426, -0.1720, -0.4981,  0.0987,  0.0627,  0.0377,  0.1711,\n",
            "            0.6532, -0.0529,  0.4699]]]])\n",
            "y tensor([[[[ 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
            "            0.6000,  0.3000,  0.3000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5000, -0.4000,  0.6000,\n",
            "            0.6000, -0.4000,  0.0000],\n",
            "          [ 0.0000,  0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,\n",
            "            0.3000,  0.6000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.2000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
            "            0.0000, -0.2000,  0.6000],\n",
            "          [-0.4000,  0.6000,  0.0000,  0.3000, -0.4000,  0.6000,  0.0000,\n",
            "            0.0000,  0.3000,  0.0000],\n",
            "          [ 0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000, -0.4000,\n",
            "            0.0000,  0.2000,  0.0000],\n",
            "          [ 0.3000,  0.0000, -0.4000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "            0.5000,  0.0000,  0.5000],\n",
            "          [ 0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
            "            0.0000,  0.2000,  0.6000],\n",
            "          [ 0.2000, -0.4000,  0.0000, -0.4000, -0.4000,  0.0000,  0.5000,\n",
            "            0.0000,  0.0000,  0.5000],\n",
            "          [ 0.0000, -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,\n",
            "            0.6000,  0.0000,  0.5000]]]])\n",
            "pred tensor([[[[ 0.0467,  0.5641,  0.0149,  0.1195,  0.0957,  0.0616,  0.4116,\n",
            "            0.7231,  0.3079,  0.2399],\n",
            "          [ 0.0674,  0.0093,  0.0781, -0.0038,  0.4295, -0.3175,  0.4236,\n",
            "            0.4706, -0.2477, -0.0228],\n",
            "          [ 0.0455, -0.0152, -0.0317,  0.5799, -0.1068,  0.4310, -0.4244,\n",
            "            0.2750,  0.5649, -0.0559],\n",
            "          [ 0.0007,  0.0502, -0.2131,  0.0630, -0.1231, -0.0045,  0.1468,\n",
            "           -0.1212, -0.1783,  0.3116],\n",
            "          [-0.5023,  0.0235,  0.0804,  0.2913, -0.4157,  0.4883,  0.0462,\n",
            "           -0.1132,  0.2619,  0.1244],\n",
            "          [ 0.0989, -0.4613, -0.3102, -0.3651,  0.3905, -0.2439, -0.3691,\n",
            "            0.1926,  0.1993,  0.0012],\n",
            "          [ 0.5132,  0.0441, -0.3292,  0.0257, -0.0321, -0.0909, -0.0373,\n",
            "            0.4354,  0.0193,  0.3925],\n",
            "          [ 0.1143,  0.0640, -0.2294,  0.0428, -0.3938, -0.4856, -0.3962,\n",
            "           -0.0161,  0.3511,  0.5807],\n",
            "          [ 0.0510, -0.4643, -0.0225, -0.3825, -0.5458, -0.0927,  0.5463,\n",
            "           -0.0746,  0.2194,  0.6130],\n",
            "          [-0.0872, -0.1184, -0.5119,  0.1157,  0.0809, -0.0030,  0.2424,\n",
            "            0.5041, -0.1262,  0.3168]]]])\n",
            "Test Error:  Avg loss: 0.010068 \n",
            "\n",
            "[tensor([[[[ 0.0328, -0.4573, -0.4725, -0.0247,  0.0540,  0.0859,  0.4515,\n",
            "            0.1295, -0.2223, -0.0262],\n",
            "          [ 0.0335, -0.4611,  0.4399, -0.0082, -0.4945, -0.3633, -0.2324,\n",
            "            0.5773,  0.6022,  0.2374],\n",
            "          [ 0.0691, -0.0194,  0.4261,  0.1651, -0.2748,  0.5246,  0.0692,\n",
            "            0.4750,  0.1088,  0.0235],\n",
            "          [ 0.1454,  0.3780,  0.3894,  0.4578, -0.4706,  0.5107,  0.1996,\n",
            "            0.4277,  0.6494,  0.1303],\n",
            "          [ 0.5510,  0.4877,  0.4463,  0.4929, -0.0060,  0.0577, -0.4770,\n",
            "            0.0755,  0.0523, -0.5525],\n",
            "          [ 0.0088,  0.4627,  0.0546,  0.5075,  0.6435,  0.1435,  0.4656,\n",
            "           -0.3355,  0.5981,  0.1251],\n",
            "          [ 0.1323,  0.1832, -0.4654, -0.2253,  0.5101, -0.0036,  0.3210,\n",
            "            0.0282,  0.1331,  0.5667],\n",
            "          [ 0.2723, -0.1965, -0.2652,  0.4898,  0.0672,  0.4451,  0.0650,\n",
            "           -0.0210,  0.0807,  0.0530],\n",
            "          [-0.5118, -0.4409,  0.1643, -0.2564, -0.4262,  0.2940, -0.5093,\n",
            "            0.3116, -0.3308, -0.5235],\n",
            "          [ 0.4505,  0.2786,  0.5227,  0.5679, -0.1540, -0.2014,  0.0721,\n",
            "           -0.2696,  0.4109,  0.0056]]]]), tensor([[[[ 1.5040e-01,  2.7753e-02,  2.6380e-01, -4.1143e-01,  5.8431e-01,\n",
            "            3.5105e-01,  3.9801e-02,  6.2555e-02,  4.4214e-01, -4.6540e-01],\n",
            "          [ 1.9462e-01,  5.3590e-02, -4.6464e-01, -3.0317e-01,  1.7399e-02,\n",
            "            1.5962e-03, -2.5415e-01,  3.5795e-01, -3.8097e-01,  4.9359e-01],\n",
            "          [ 4.8813e-01,  4.1093e-01, -2.1811e-01,  8.8324e-02,  2.7562e-01,\n",
            "           -7.1405e-02,  4.5740e-01,  3.1607e-01,  2.5520e-01,  3.7994e-02],\n",
            "          [ 5.7717e-01,  1.2074e-02,  6.7664e-02, -6.1348e-03, -4.8868e-02,\n",
            "           -6.5389e-02, -5.4172e-01,  5.6659e-01,  3.2106e-02,  1.2346e-01],\n",
            "          [ 7.8257e-02,  4.5159e-01,  4.8174e-01,  3.4322e-02, -6.3859e-02,\n",
            "            8.9762e-02, -3.6590e-01,  2.7255e-01,  5.2111e-02, -2.3775e-01],\n",
            "          [ 2.0834e-01, -4.0831e-01,  2.9185e-01, -6.6780e-06,  7.1918e-02,\n",
            "           -3.3397e-02,  4.5936e-01, -2.7687e-01,  2.5574e-01, -5.4087e-03],\n",
            "          [ 1.0142e-01, -4.8906e-01,  2.7102e-02,  6.3380e-02, -2.2070e-01,\n",
            "           -2.2836e-02, -5.0412e-02,  4.2620e-01, -4.2748e-01,  4.2188e-01],\n",
            "          [-5.3945e-01, -3.4004e-01, -4.1858e-01, -3.2877e-01, -2.2333e-01,\n",
            "            5.4785e-02,  3.2787e-01,  2.5013e-01,  8.4432e-02,  4.8181e-01],\n",
            "          [ 1.7438e-02, -5.4653e-01,  4.1048e-02, -4.2961e-01,  3.5564e-01,\n",
            "            3.8394e-03,  1.0678e-01,  5.4504e-02, -4.8490e-01,  3.6136e-02],\n",
            "          [ 4.7570e-01,  1.6878e-01,  4.7856e-01,  2.5320e-02,  6.2321e-02,\n",
            "            2.8568e-02, -5.0877e-01, -2.3411e-01,  2.7215e-01, -4.1985e-01]]]]), tensor([[[[ 1.5207e-01, -4.5816e-01, -3.2321e-01,  1.2050e-02,  1.2225e-01,\n",
            "           -4.5844e-01,  8.9613e-02, -2.7273e-01,  1.1309e-01, -2.3914e-02],\n",
            "          [ 5.0364e-01,  1.5636e-01,  4.9550e-02, -2.3475e-01, -2.3575e-01,\n",
            "            1.8438e-01,  3.8710e-01, -4.4908e-01,  2.7194e-01,  3.4552e-01],\n",
            "          [ 1.2505e-01, -3.8047e-02,  1.4039e-02,  4.5893e-01,  6.1139e-02,\n",
            "           -3.2275e-01, -5.0564e-01,  2.7278e-01, -1.5879e-02,  3.7046e-01],\n",
            "          [-4.4813e-01,  1.6258e-01,  4.9993e-01, -4.5045e-01,  2.6548e-02,\n",
            "            5.0095e-01, -7.1420e-03,  8.5321e-02, -4.7502e-01, -5.1078e-01],\n",
            "          [-7.2781e-02, -4.2473e-01,  1.7987e-01,  1.8869e-03,  3.7284e-02,\n",
            "            9.7761e-02, -2.4007e-01, -1.9300e-02, -2.1564e-01, -4.7269e-01],\n",
            "          [-4.6971e-01, -3.4520e-01,  4.3727e-01,  5.0005e-01, -2.6470e-01,\n",
            "            8.2406e-02,  7.9866e-02,  5.3849e-02, -2.4046e-01,  1.3397e-01],\n",
            "          [-2.0271e-01,  5.6790e-01,  2.6949e-01,  7.5627e-02,  4.4006e-01,\n",
            "            5.0170e-01, -4.4758e-01, -4.6685e-01,  1.3127e-01,  4.5948e-02],\n",
            "          [-2.7190e-01,  7.3369e-02,  6.3947e-02,  1.2952e-01,  4.9203e-01,\n",
            "            7.1726e-02, -2.7601e-02, -4.5895e-01, -4.7017e-01,  4.4787e-02],\n",
            "          [-2.5383e-02,  4.4729e-01,  4.4143e-01, -2.3552e-01, -4.8626e-01,\n",
            "            3.9564e-01, -1.8680e-01, -8.8002e-03,  1.1407e-01, -2.9984e-01],\n",
            "          [-3.4489e-02, -5.1843e-01,  2.4105e-02,  6.2846e-01,  8.5015e-02,\n",
            "            3.6069e-02, -2.1637e-04, -4.3929e-02, -5.0672e-01, -3.9851e-01]]]]), tensor([[[[-2.6127e-01, -4.6763e-01,  6.2371e-02,  5.2001e-01, -7.0590e-03,\n",
            "           -5.2496e-01,  1.4165e-01, -5.2515e-01,  2.6908e-02,  3.4106e-03],\n",
            "          [-5.3161e-01,  5.9588e-01,  6.6905e-02,  3.2951e-01,  4.4232e-01,\n",
            "           -2.1317e-02, -1.8266e-01,  4.8799e-01,  2.1765e-02, -2.6534e-02],\n",
            "          [ 1.9717e-01, -4.2183e-01, -1.4312e-02,  4.1642e-01,  2.6945e-01,\n",
            "            6.4946e-01,  2.3988e-01,  4.8178e-02, -2.6797e-02, -3.7925e-01],\n",
            "          [-5.7487e-01, -2.4094e-01, -2.5117e-01,  7.0525e-01,  1.3802e-01,\n",
            "            3.7031e-01, -5.4238e-01,  5.7693e-01,  5.1506e-01,  4.7791e-01],\n",
            "          [ 2.0435e-02,  3.5112e-02,  7.6267e-02,  6.1888e-01, -2.5576e-02,\n",
            "           -4.8947e-01, -4.7947e-01, -3.6862e-01, -4.5220e-01,  2.7477e-01],\n",
            "          [-2.1003e-01,  7.2061e-02, -2.0308e-02, -3.9764e-01,  4.9790e-01,\n",
            "            5.3874e-02, -1.4719e-02,  5.0923e-01,  1.0956e-01, -4.2560e-01],\n",
            "          [-4.1023e-01,  1.6442e-01,  1.7020e-01,  3.6917e-02, -2.5307e-01,\n",
            "            4.4099e-01,  7.4603e-05, -3.9181e-01, -1.8042e-01,  1.0617e-01],\n",
            "          [ 5.3088e-01, -2.0616e-01,  5.6170e-01,  1.7280e-01,  4.5557e-01,\n",
            "           -4.1374e-02, -5.7587e-01,  6.8499e-02, -4.1197e-01, -1.6708e-01],\n",
            "          [-1.9162e-01,  4.8799e-01,  3.8270e-01, -4.5628e-01,  1.4194e-01,\n",
            "            7.3972e-02, -1.5844e-01, -2.7551e-01, -4.5341e-01,  3.8624e-01],\n",
            "          [ 1.7708e-01,  6.0247e-02, -3.0290e-02,  6.3880e-02,  1.2618e-02,\n",
            "            4.4718e-01, -2.0038e-01,  4.0293e-01,  5.5030e-01,  4.1853e-01]]]]), tensor([[[[ 0.0618,  0.4910,  0.0659,  0.0617,  0.1002,  0.0078,  0.3823,\n",
            "            0.6141,  0.2087,  0.2565],\n",
            "          [ 0.0369,  0.0386,  0.0204,  0.0185,  0.3893, -0.4769,  0.5229,\n",
            "            0.4450, -0.4036,  0.0044],\n",
            "          [-0.0042,  0.0559,  0.0243,  0.4507, -0.1911,  0.3623, -0.4669,\n",
            "            0.2460,  0.4909, -0.0077],\n",
            "          [-0.0043,  0.0563, -0.1680,  0.0477, -0.1491,  0.1000,  0.0429,\n",
            "           -0.0586, -0.1763,  0.4750],\n",
            "          [-0.5252, -0.0102,  0.0044,  0.3419, -0.4832,  0.5341,  0.0817,\n",
            "           -0.0328,  0.2554,  0.0550],\n",
            "          [ 0.1156, -0.4956, -0.4484, -0.5622,  0.3319, -0.3887, -0.4813,\n",
            "            0.1118,  0.1446,  0.0871],\n",
            "          [ 0.3897,  0.0433, -0.4223, -0.0112, -0.0103,  0.0583,  0.0187,\n",
            "            0.4850,  0.0213,  0.5210],\n",
            "          [ 0.1450,  0.0028, -0.2643,  0.1555, -0.4445, -0.5146, -0.4895,\n",
            "           -0.0116,  0.2272,  0.6472],\n",
            "          [ 0.1607, -0.5420,  0.0293, -0.4902, -0.5888, -0.0096,  0.5049,\n",
            "           -0.0019,  0.0395,  0.5038],\n",
            "          [-0.0426, -0.1720, -0.4981,  0.0987,  0.0627,  0.0377,  0.1711,\n",
            "            0.6532, -0.0529,  0.4699]]]])]\n",
            "[tensor([[[[ 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "            0.0000, -0.2000,  0.0000],\n",
            "          [ 0.0000, -0.4000,  0.5000,  0.0000, -0.4000, -0.4000, -0.2000,\n",
            "            0.6000,  0.6000,  0.3000],\n",
            "          [ 0.0000,  0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,\n",
            "            0.5000,  0.0000,  0.0000],\n",
            "          [ 0.2000,  0.5000,  0.3000,  0.5000, -0.4000,  0.5000,  0.2000,\n",
            "            0.5000,  0.6000,  0.2000],\n",
            "          [ 0.5000,  0.5000,  0.5000,  0.5000,  0.0000,  0.0000, -0.4000,\n",
            "            0.0000,  0.0000, -0.4000],\n",
            "          [ 0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,  0.6000,\n",
            "           -0.4000,  0.6000,  0.2000],\n",
            "          [ 0.0000,  0.2000, -0.4000, -0.2000,  0.6000,  0.0000,  0.3000,\n",
            "            0.0000,  0.2000,  0.6000],\n",
            "          [ 0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
            "            0.0000,  0.0000,  0.0000],\n",
            "          [-0.4000, -0.4000,  0.2000, -0.2000, -0.4000,  0.3000, -0.4000,\n",
            "            0.3000, -0.2000, -0.4000],\n",
            "          [ 0.5000,  0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000,\n",
            "           -0.2000,  0.5000,  0.0000]]]]), tensor([[[[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
            "            0.0000,  0.5000, -0.4000],\n",
            "          [ 0.2000,  0.0000, -0.4000, -0.2000,  0.0000,  0.0000, -0.2000,\n",
            "            0.5000, -0.4000,  0.5000],\n",
            "          [ 0.5000,  0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,\n",
            "            0.3000,  0.3000,  0.0000],\n",
            "          [ 0.6000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "            0.6000,  0.0000,  0.2000],\n",
            "          [ 0.2000,  0.5000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "            0.3000,  0.0000, -0.2000],\n",
            "          [ 0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "           -0.2000,  0.2000,  0.0000],\n",
            "          [ 0.2000, -0.4000,  0.0000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
            "            0.6000, -0.4000,  0.5000],\n",
            "          [-0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
            "            0.3000,  0.2000,  0.5000],\n",
            "          [ 0.0000, -0.4000,  0.0000, -0.4000,  0.3000,  0.0000,  0.0000,\n",
            "            0.0000, -0.4000,  0.0000],\n",
            "          [ 0.5000,  0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "           -0.2000,  0.3000, -0.4000]]]]), tensor([[[[ 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
            "           -0.2000,  0.2000,  0.0000],\n",
            "          [ 0.5000,  0.2000,  0.0000, -0.2000, -0.2000,  0.2000,  0.5000,\n",
            "           -0.4000,  0.3000,  0.3000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,\n",
            "            0.3000,  0.0000,  0.3000],\n",
            "          [-0.4000,  0.2000,  0.5000, -0.4000,  0.0000,  0.5000,  0.0000,\n",
            "            0.2000, -0.4000, -0.4000],\n",
            "          [ 0.0000, -0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.2000,\n",
            "            0.0000, -0.2000, -0.4000],\n",
            "          [-0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,  0.0000,\n",
            "            0.0000, -0.2000,  0.2000],\n",
            "          [-0.2000,  0.6000,  0.3000,  0.0000,  0.5000,  0.6000, -0.4000,\n",
            "           -0.4000,  0.2000,  0.0000],\n",
            "          [-0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
            "           -0.4000, -0.4000,  0.0000],\n",
            "          [ 0.0000,  0.5000,  0.5000, -0.2000, -0.4000,  0.3000, -0.2000,\n",
            "            0.0000,  0.0000, -0.2000],\n",
            "          [ 0.0000, -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,\n",
            "            0.0000, -0.4000, -0.4000]]]]), tensor([[[[-0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
            "           -0.4000,  0.2000,  0.0000],\n",
            "          [-0.4000,  0.5000,  0.0000,  0.3000,  0.5000,  0.0000, -0.2000,\n",
            "            0.6000,  0.0000,  0.0000],\n",
            "          [ 0.2000, -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,\n",
            "            0.0000,  0.0000, -0.4000],\n",
            "          [-0.4000, -0.2000, -0.2000,  0.6000,  0.2000,  0.5000, -0.4000,\n",
            "            0.5000,  0.6000,  0.5000],\n",
            "          [ 0.0000,  0.0000,  0.2000,  0.6000,  0.0000, -0.4000, -0.4000,\n",
            "           -0.2000, -0.4000,  0.5000],\n",
            "          [-0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,  0.0000,\n",
            "            0.6000,  0.0000, -0.4000],\n",
            "          [-0.4000,  0.3000,  0.2000,  0.0000, -0.2000,  0.5000,  0.0000,\n",
            "           -0.4000, -0.2000,  0.2000],\n",
            "          [ 0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
            "            0.0000, -0.4000, -0.2000],\n",
            "          [-0.2000,  0.6000,  0.3000, -0.4000,  0.2000,  0.0000, -0.2000,\n",
            "           -0.2000, -0.4000,  0.5000],\n",
            "          [ 0.2000,  0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,\n",
            "            0.5000,  0.5000,  0.6000]]]]), tensor([[[[ 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
            "            0.6000,  0.3000,  0.3000],\n",
            "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.5000, -0.4000,  0.6000,\n",
            "            0.6000, -0.4000,  0.0000],\n",
            "          [ 0.0000,  0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,\n",
            "            0.3000,  0.6000,  0.0000],\n",
            "          [ 0.0000,  0.0000, -0.2000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
            "            0.0000, -0.2000,  0.6000],\n",
            "          [-0.4000,  0.6000,  0.0000,  0.3000, -0.4000,  0.6000,  0.0000,\n",
            "            0.0000,  0.3000,  0.0000],\n",
            "          [ 0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000, -0.4000,\n",
            "            0.0000,  0.2000,  0.0000],\n",
            "          [ 0.3000,  0.0000, -0.4000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "            0.5000,  0.0000,  0.5000],\n",
            "          [ 0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
            "            0.0000,  0.2000,  0.6000],\n",
            "          [ 0.2000, -0.4000,  0.0000, -0.4000, -0.4000,  0.0000,  0.5000,\n",
            "            0.0000,  0.0000,  0.5000],\n",
            "          [ 0.0000, -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,\n",
            "            0.6000,  0.0000,  0.5000]]]])]\n",
            "[tensor([[[[ 0.1245, -0.4191, -0.4042, -0.0309, -0.0127,  0.0894,  0.3684,\n",
            "            0.0634, -0.1731, -0.1370],\n",
            "          [ 0.0398, -0.4175,  0.3953, -0.0818, -0.4071, -0.3251, -0.1423,\n",
            "            0.5217,  0.6768,  0.2523],\n",
            "          [ 0.1053, -0.0477,  0.4147,  0.2054, -0.3016,  0.5511,  0.0260,\n",
            "            0.3450,  0.2064, -0.0643],\n",
            "          [ 0.2353,  0.3881,  0.3941,  0.5507, -0.3601,  0.3502,  0.3635,\n",
            "            0.3598,  0.6160,  0.1088],\n",
            "          [ 0.6676,  0.4080,  0.2772,  0.5105,  0.0704,  0.0563, -0.3605,\n",
            "            0.0728, -0.0465, -0.4352],\n",
            "          [ 0.1010,  0.5673,  0.0103,  0.4505,  0.5784,  0.2057,  0.4969,\n",
            "           -0.2954,  0.5521,  0.2517],\n",
            "          [ 0.1120,  0.1227, -0.4469, -0.2185,  0.5233,  0.0469,  0.4626,\n",
            "            0.1437,  0.2577,  0.5091],\n",
            "          [ 0.2159, -0.1136, -0.2118,  0.5507,  0.0661,  0.4159,  0.1155,\n",
            "           -0.0646,  0.0714, -0.0338],\n",
            "          [-0.3502, -0.3897,  0.1530, -0.1984, -0.2786,  0.2881, -0.4132,\n",
            "            0.3232, -0.3257, -0.5354],\n",
            "          [ 0.3498,  0.3488,  0.4834,  0.4229, -0.2028, -0.1964,  0.0126,\n",
            "           -0.2553,  0.3892,  0.0238]]]]), tensor([[[[-0.0008,  0.0507,  0.2621, -0.4382,  0.4404,  0.3615,  0.0831,\n",
            "           -0.0107,  0.3775, -0.3198],\n",
            "          [ 0.1656, -0.0037, -0.4014, -0.2951, -0.0223,  0.0514, -0.0676,\n",
            "            0.3716, -0.2980,  0.4366],\n",
            "          [ 0.6147,  0.4943, -0.2243, -0.0156,  0.2958, -0.0489,  0.4820,\n",
            "            0.1849,  0.2832,  0.0641],\n",
            "          [ 0.5896,  0.0512,  0.0163,  0.0299, -0.0111, -0.0522, -0.4083,\n",
            "            0.6522,  0.0914,  0.1229],\n",
            "          [ 0.0665,  0.3549,  0.6872,  0.0966, -0.1152,  0.0983, -0.3330,\n",
            "            0.2783, -0.0236, -0.2592],\n",
            "          [ 0.1426, -0.3869,  0.3097,  0.0212,  0.0915,  0.0180,  0.4120,\n",
            "           -0.3299,  0.2980,  0.0721],\n",
            "          [ 0.1013, -0.3693, -0.0614,  0.0546, -0.3093, -0.0048, -0.1455,\n",
            "            0.4906, -0.3684,  0.2382],\n",
            "          [-0.4091, -0.2860, -0.3765, -0.3276, -0.3263,  0.0961,  0.3766,\n",
            "            0.1952,  0.0975,  0.3634],\n",
            "          [ 0.0602, -0.3755,  0.0515, -0.3295,  0.3461, -0.0403,  0.1355,\n",
            "            0.0007, -0.4528,  0.0649],\n",
            "          [ 0.3373,  0.2691,  0.4083,  0.0369,  0.0405,  0.0047, -0.4675,\n",
            "           -0.1889,  0.2920, -0.3130]]]]), tensor([[[[ 0.1657, -0.3835, -0.2296,  0.0311,  0.1557, -0.3483,  0.0622,\n",
            "           -0.3317, -0.0186,  0.1113],\n",
            "          [ 0.6204,  0.1243,  0.0926, -0.2536, -0.2294,  0.1751,  0.2932,\n",
            "           -0.3457,  0.3515,  0.2999],\n",
            "          [-0.0295,  0.0561,  0.1426,  0.5678,  0.0526, -0.2463, -0.3848,\n",
            "            0.3139,  0.0152,  0.3710],\n",
            "          [-0.3719,  0.0472,  0.4632, -0.4147,  0.0554,  0.6607,  0.0453,\n",
            "            0.1624, -0.3120, -0.4602],\n",
            "          [-0.0711, -0.3837,  0.1580, -0.0061, -0.0020,  0.0578, -0.2047,\n",
            "           -0.0763, -0.2547, -0.3261],\n",
            "          [-0.3581, -0.3583,  0.4595,  0.5443, -0.2240,  0.1090,  0.0849,\n",
            "            0.1119, -0.2416,  0.0944],\n",
            "          [-0.1947,  0.6267,  0.1833,  0.0402,  0.3114,  0.5041, -0.3228,\n",
            "           -0.3504,  0.1593,  0.1671],\n",
            "          [-0.3594,  0.1678,  0.2038,  0.1250,  0.4123,  0.1423, -0.0276,\n",
            "           -0.4840, -0.3914, -0.0112],\n",
            "          [ 0.0271,  0.4049,  0.3513, -0.3407, -0.4511,  0.3357, -0.1676,\n",
            "           -0.0215,  0.0990, -0.2799],\n",
            "          [ 0.0384, -0.3930,  0.1465,  0.5238,  0.0917,  0.0986,  0.0244,\n",
            "           -0.0288, -0.4389, -0.2361]]]]), tensor([[[[-0.1877, -0.3030,  0.1996,  0.6096, -0.0181, -0.3243,  0.0009,\n",
            "           -0.4484,  0.0785,  0.0751],\n",
            "          [-0.4679,  0.4463,  0.0901,  0.2089,  0.5209, -0.0191, -0.2329,\n",
            "            0.4963, -0.0546, -0.0223],\n",
            "          [ 0.2146, -0.3441, -0.0100,  0.4103,  0.1737,  0.6178,  0.3528,\n",
            "            0.0749, -0.0027, -0.3158],\n",
            "          [-0.4049, -0.1828, -0.1228,  0.6777,  0.0545,  0.4376, -0.4438,\n",
            "            0.6061,  0.5786,  0.3762],\n",
            "          [ 0.0342,  0.1331,  0.0706,  0.5413, -0.0962, -0.3753, -0.4120,\n",
            "           -0.3940, -0.3769,  0.2872],\n",
            "          [-0.1953,  0.0993, -0.0077, -0.4021,  0.4976,  0.0134, -0.1036,\n",
            "            0.6081,  0.1765, -0.3565],\n",
            "          [-0.3650,  0.1350,  0.1360,  0.0717, -0.2977,  0.4775, -0.0349,\n",
            "           -0.3124, -0.1804,  0.1039],\n",
            "          [ 0.5616, -0.2728,  0.5626,  0.1826,  0.4196,  0.0138, -0.4560,\n",
            "            0.0486, -0.3379, -0.1309],\n",
            "          [-0.1296,  0.4592,  0.3626, -0.3928,  0.2075,  0.1004, -0.1168,\n",
            "           -0.2675, -0.3814,  0.2511],\n",
            "          [ 0.0974,  0.2542, -0.0372,  0.0292,  0.0793,  0.4050, -0.1619,\n",
            "            0.2179,  0.5123,  0.0878]]]]), tensor([[[[ 0.0467,  0.5641,  0.0149,  0.1195,  0.0957,  0.0616,  0.4116,\n",
            "            0.7231,  0.3079,  0.2399],\n",
            "          [ 0.0674,  0.0093,  0.0781, -0.0038,  0.4295, -0.3175,  0.4236,\n",
            "            0.4706, -0.2477, -0.0228],\n",
            "          [ 0.0455, -0.0152, -0.0317,  0.5799, -0.1068,  0.4310, -0.4244,\n",
            "            0.2750,  0.5649, -0.0559],\n",
            "          [ 0.0007,  0.0502, -0.2131,  0.0630, -0.1231, -0.0045,  0.1468,\n",
            "           -0.1212, -0.1783,  0.3116],\n",
            "          [-0.5023,  0.0235,  0.0804,  0.2913, -0.4157,  0.4883,  0.0462,\n",
            "           -0.1132,  0.2619,  0.1244],\n",
            "          [ 0.0989, -0.4613, -0.3102, -0.3651,  0.3905, -0.2439, -0.3691,\n",
            "            0.1926,  0.1993,  0.0012],\n",
            "          [ 0.5132,  0.0441, -0.3292,  0.0257, -0.0321, -0.0909, -0.0373,\n",
            "            0.4354,  0.0193,  0.3925],\n",
            "          [ 0.1143,  0.0640, -0.2294,  0.0428, -0.3938, -0.4856, -0.3962,\n",
            "           -0.0161,  0.3511,  0.5807],\n",
            "          [ 0.0510, -0.4643, -0.0225, -0.3825, -0.5458, -0.0927,  0.5463,\n",
            "           -0.0746,  0.2194,  0.6130],\n",
            "          [-0.0872, -0.1184, -0.5119,  0.1157,  0.0809, -0.0030,  0.2424,\n",
            "            0.5041, -0.1262,  0.3168]]]])]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    #train_loop(loader, loss_fn, optimizer)\n",
        "    test_loop(test_loader, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4UZ27YGus_w",
        "outputId": "6241a529-7fec-4ce9-b413-79175090d8a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uTZtxT7yF0I",
        "outputId": "a951052d-ec40-41a4-bf1a-fef9937023ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(19.9708)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from torchmetrics import PeakSignalNoiseRatio\n",
        "psnr = PeakSignalNoiseRatio()\n",
        "\n",
        "\n",
        "preds = torch.tensor([[ 0.1245, -0.4191, -0.4042, -0.0309, -0.0127,  0.0894,  0.3684,\n",
        "            0.0634, -0.1731, -0.1370,\n",
        "           0.0398, -0.4175,  0.3953, -0.0818, -0.4071, -0.3251, -0.1423,\n",
        "            0.5217,  0.6768,  0.2523,\n",
        "           0.1053, -0.0477,  0.4147,  0.2054, -0.3016,  0.5511,  0.0260,\n",
        "            0.3450,  0.2064, -0.0643,\n",
        "           0.2353,  0.3881,  0.3941,  0.5507, -0.3601,  0.3502,  0.3635,\n",
        "            0.3598,  0.6160,  0.1088,\n",
        "           0.6676,  0.4080,  0.2772,  0.5105,  0.0704,  0.0563, -0.3605,\n",
        "            0.0728, -0.0465, -0.4352,\n",
        "           0.1010,  0.5673,  0.0103,  0.4505,  0.5784,  0.2057,  0.4969,\n",
        "           -0.2954,  0.5521,  0.2517,\n",
        "           0.1120,  0.1227, -0.4469, -0.2185,  0.5233,  0.0469,  0.4626,\n",
        "            0.1437,  0.2577,  0.5091,\n",
        "           0.2159, -0.1136, -0.2118,  0.5507,  0.0661,  0.4159,  0.1155,\n",
        "           -0.0646,  0.0714, -0.0338,\n",
        "        -0.3502, -0.3897,  0.1530, -0.1984, -0.2786,  0.2881, -0.4132,\n",
        "            0.3232, -0.3257, -0.5354,\n",
        "          0.3498,  0.3488,  0.4834,  0.4229, -0.2028, -0.1964,  0.0126,\n",
        "           -0.2553,  0.3892,  0.0238, -0.0008,  0.0507,  0.2621, -0.4382,  0.4404,  0.3615,  0.0831,\n",
        "           -0.0107,  0.3775, -0.3198,\n",
        "           0.1656, -0.0037, -0.4014, -0.2951, -0.0223,  0.0514, -0.0676,\n",
        "            0.3716, -0.2980,  0.4366,\n",
        "           0.6147,  0.4943, -0.2243, -0.0156,  0.2958, -0.0489,  0.4820,\n",
        "            0.1849,  0.2832,  0.0641,\n",
        "           0.5896,  0.0512,  0.0163,  0.0299, -0.0111, -0.0522, -0.4083,\n",
        "            0.6522,  0.0914,  0.1229,\n",
        "           0.0665,  0.3549,  0.6872,  0.0966, -0.1152,  0.0983, -0.3330,\n",
        "            0.2783, -0.0236, -0.2592,\n",
        "           0.1426, -0.3869,  0.3097,  0.0212,  0.0915,  0.0180,  0.4120,\n",
        "           -0.3299,  0.2980,  0.0721,\n",
        "           0.1013, -0.3693, -0.0614,  0.0546, -0.3093, -0.0048, -0.1455,\n",
        "            0.4906, -0.3684,  0.2382,\n",
        "          -0.4091, -0.2860, -0.3765, -0.3276, -0.3263,  0.0961,  0.3766,\n",
        "            0.1952,  0.0975,  0.3634,\n",
        "         0.0602, -0.3755,  0.0515, -0.3295,  0.3461, -0.0403,  0.1355,\n",
        "            0.0007, -0.4528,  0.0649,\n",
        "           0.3373,  0.2691,  0.4083,  0.0369,  0.0405,  0.0047, -0.4675,\n",
        "           -0.1889,  0.2920, -0.3130,  0.1657, -0.3835, -0.2296,  0.0311,  0.1557, -0.3483,  0.0622,\n",
        "           -0.3317, -0.0186,  0.1113,\n",
        "           0.6204,  0.1243,  0.0926, -0.2536, -0.2294,  0.1751,  0.2932,\n",
        "           -0.3457,  0.3515,  0.2999,\n",
        "          -0.0295,  0.0561,  0.1426,  0.5678,  0.0526, -0.2463, -0.3848,\n",
        "            0.3139,  0.0152,  0.3710,\n",
        "        -0.3719,  0.0472,  0.4632, -0.4147,  0.0554,  0.6607,  0.0453,\n",
        "            0.1624, -0.3120, -0.4602,\n",
        "          -0.0711, -0.3837,  0.1580, -0.0061, -0.0020,  0.0578, -0.2047,\n",
        "           -0.0763, -0.2547, -0.3261,\n",
        "          -0.3581, -0.3583,  0.4595,  0.5443, -0.2240,  0.1090,  0.0849,\n",
        "            0.1119, -0.2416,  0.0944,\n",
        "          -0.1947,  0.6267,  0.1833,  0.0402,  0.3114,  0.5041, -0.3228,\n",
        "           -0.3504,  0.1593,  0.1671,\n",
        "          -0.3594,  0.1678,  0.2038,  0.1250,  0.4123,  0.1423, -0.0276,\n",
        "           -0.4840, -0.3914, -0.0112,\n",
        "           0.0271,  0.4049,  0.3513, -0.3407, -0.4511,  0.3357, -0.1676,\n",
        "           -0.0215,  0.0990, -0.2799,\n",
        "          0.0384, -0.3930,  0.1465,  0.5238,  0.0917,  0.0986,  0.0244,\n",
        "           -0.0288, -0.4389, -0.2361, -0.1877, -0.3030,  0.1996,  0.6096, -0.0181, -0.3243,  0.0009,\n",
        "           -0.4484,  0.0785,  0.0751,\n",
        "        -0.4679,  0.4463,  0.0901,  0.2089,  0.5209, -0.0191, -0.2329,\n",
        "            0.4963, -0.0546, -0.0223,\n",
        "           0.2146, -0.3441, -0.0100,  0.4103,  0.1737,  0.6178,  0.3528,\n",
        "            0.0749, -0.0027, -0.3158,\n",
        "          -0.4049, -0.1828, -0.1228,  0.6777,  0.0545,  0.4376, -0.4438,\n",
        "            0.6061,  0.5786,  0.3762,\n",
        "           0.0342,  0.1331,  0.0706,  0.5413, -0.0962, -0.3753, -0.4120,\n",
        "           -0.3940, -0.3769,  0.2872,\n",
        "          -0.1953,  0.0993, -0.0077, -0.4021,  0.4976,  0.0134, -0.1036,\n",
        "            0.6081,  0.1765, -0.3565,\n",
        "          -0.3650,  0.1350,  0.1360,  0.0717, -0.2977,  0.4775, -0.0349,\n",
        "           -0.3124, -0.1804,  0.1039,\n",
        "           0.5616, -0.2728,  0.5626,  0.1826,  0.4196,  0.0138, -0.4560,\n",
        "            0.0486, -0.3379, -0.1309,\n",
        "          -0.1296,  0.4592,  0.3626, -0.3928,  0.2075,  0.1004, -0.1168,\n",
        "           -0.2675, -0.3814,  0.2511,\n",
        "           0.0974,  0.2542, -0.0372,  0.0292,  0.0793,  0.4050, -0.1619,\n",
        "            0.2179,  0.5123,  0.0878,  0.0467,  0.5641,  0.0149,  0.1195,  0.0957,  0.0616,  0.4116,\n",
        "            0.7231,  0.3079,  0.2399,\n",
        "           0.0674,  0.0093,  0.0781, -0.0038,  0.4295, -0.3175,  0.4236,\n",
        "            0.4706, -0.2477, -0.0228,\n",
        "           0.0455, -0.0152, -0.0317,  0.5799, -0.1068,  0.4310, -0.4244,\n",
        "            0.2750,  0.5649, -0.0559,\n",
        "           0.0007,  0.0502, -0.2131,  0.0630, -0.1231, -0.0045,  0.1468,\n",
        "           -0.1212, -0.1783,  0.3116,\n",
        "          -0.5023,  0.0235,  0.0804,  0.2913, -0.4157,  0.4883,  0.0462,\n",
        "           -0.1132,  0.2619,  0.1244,\n",
        "           0.0989, -0.4613, -0.3102, -0.3651,  0.3905, -0.2439, -0.3691,\n",
        "            0.1926,  0.1993,  0.0012,\n",
        "           0.5132,  0.0441, -0.3292,  0.0257, -0.0321, -0.0909, -0.0373,\n",
        "            0.4354,  0.0193,  0.3925,\n",
        "           0.1143,  0.0640, -0.2294,  0.0428, -0.3938, -0.4856, -0.3962,\n",
        "           -0.0161,  0.3511,  0.5807,\n",
        "           0.0510, -0.4643, -0.0225, -0.3825, -0.5458, -0.0927,  0.5463,\n",
        "           -0.0746,  0.2194,  0.6130,\n",
        "          -0.0872, -0.1184, -0.5119,  0.1157,  0.0809, -0.0030,  0.2424,\n",
        "            0.5041, -0.1262,  0.3168]])\n",
        "\n",
        "Y = torch.tensor([[ 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
        "            0.0000, -0.2000,  0.0000,\n",
        "           0.0000, -0.4000,  0.5000,  0.0000, -0.4000, -0.4000, -0.2000,\n",
        "            0.6000,  0.6000,  0.3000,\n",
        "           0.0000,  0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,\n",
        "            0.5000,  0.0000,  0.0000,\n",
        "           0.2000,  0.5000,  0.3000,  0.5000, -0.4000,  0.5000,  0.2000,\n",
        "            0.5000,  0.6000,  0.2000,\n",
        "           0.5000,  0.5000,  0.5000,  0.5000,  0.0000,  0.0000, -0.4000,\n",
        "            0.0000,  0.0000, -0.4000,\n",
        "          0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,  0.6000,\n",
        "           -0.4000,  0.6000,  0.2000,\n",
        "           0.0000,  0.2000, -0.4000, -0.2000,  0.6000,  0.0000,  0.3000,\n",
        "            0.0000,  0.2000,  0.6000,\n",
        "           0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
        "            0.0000,  0.0000,  0.0000,\n",
        "          -0.4000, -0.4000,  0.2000, -0.2000, -0.4000,  0.3000, -0.4000,\n",
        "            0.3000, -0.2000, -0.4000,\n",
        "           0.5000,  0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000,\n",
        "           -0.2000,  0.5000,  0.0000,  0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
        "            0.0000,  0.5000, -0.4000,\n",
        "           0.2000,  0.0000, -0.4000, -0.2000,  0.0000,  0.0000, -0.2000,\n",
        "            0.5000, -0.4000,  0.5000,\n",
        "           0.5000,  0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,\n",
        "            0.3000,  0.3000,  0.0000,\n",
        "           0.6000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
        "            0.6000,  0.0000,  0.2000,\n",
        "           0.2000,  0.5000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
        "            0.3000,  0.0000, -0.2000,\n",
        "           0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
        "           -0.2000,  0.2000,  0.0000,\n",
        "           0.2000, -0.4000,  0.0000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
        "            0.6000, -0.4000,  0.5000,\n",
        "        -0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
        "            0.3000,  0.2000,  0.5000,\n",
        "           0.0000, -0.4000,  0.0000, -0.4000,  0.3000,  0.0000,  0.0000,\n",
        "            0.0000, -0.4000,  0.0000,\n",
        "           0.5000,  0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
        "           -0.2000,  0.3000, -0.4000,  0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
        "           -0.2000,  0.2000,  0.0000,\n",
        "           0.5000,  0.2000,  0.0000, -0.2000, -0.2000,  0.2000,  0.5000,\n",
        "           -0.4000,  0.3000,  0.3000,\n",
        "         0.0000,  0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,\n",
        "            0.3000,  0.0000,  0.3000,\n",
        "        -0.4000,  0.2000,  0.5000, -0.4000,  0.0000,  0.5000,  0.0000,\n",
        "            0.2000, -0.4000, -0.4000,\n",
        "          0.0000, -0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.2000,\n",
        "            0.0000, -0.2000, -0.4000,\n",
        "          -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,  0.0000,\n",
        "            0.0000, -0.2000,  0.2000,\n",
        "          -0.2000,  0.6000,  0.3000,  0.0000,  0.5000,  0.6000, -0.4000,\n",
        "           -0.4000,  0.2000,  0.0000,\n",
        "          -0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
        "           -0.4000, -0.4000,  0.0000,\n",
        "           0.0000,  0.5000,  0.5000, -0.2000, -0.4000,  0.3000, -0.2000,\n",
        "            0.0000,  0.0000, -0.2000,\n",
        "           0.0000, -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,\n",
        "            0.0000, -0.4000, -0.4000, -0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
        "           -0.4000,  0.2000,  0.0000,\n",
        "          -0.4000,  0.5000,  0.0000,  0.3000,  0.5000,  0.0000, -0.2000,\n",
        "            0.6000,  0.0000,  0.0000,\n",
        "           0.2000, -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,\n",
        "            0.0000,  0.0000, -0.4000,\n",
        "          -0.4000, -0.2000, -0.2000,  0.6000,  0.2000,  0.5000, -0.4000,\n",
        "            0.5000,  0.6000,  0.5000,\n",
        "           0.0000,  0.0000,  0.2000,  0.6000,  0.0000, -0.4000, -0.4000,\n",
        "           -0.2000, -0.4000,  0.5000,\n",
        "          -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,  0.0000,\n",
        "            0.6000,  0.0000, -0.4000,\n",
        "          -0.4000,  0.3000,  0.2000,  0.0000, -0.2000,  0.5000,  0.0000,\n",
        "           -0.4000, -0.2000,  0.2000,\n",
        "           0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
        "            0.0000, -0.4000, -0.2000,\n",
        "          -0.2000,  0.6000,  0.3000, -0.4000,  0.2000,  0.0000, -0.2000,\n",
        "           -0.2000, -0.4000,  0.5000,\n",
        "           0.2000,  0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,\n",
        "            0.5000,  0.5000,  0.6000, 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
        "            0.6000,  0.3000,  0.3000,\n",
        "           0.0000,  0.0000,  0.0000,  0.0000,  0.5000, -0.4000,  0.6000,\n",
        "            0.6000, -0.4000,  0.0000,\n",
        "           0.0000,  0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,\n",
        "            0.3000,  0.6000,  0.0000,\n",
        "           0.0000,  0.0000, -0.2000,  0.0000, -0.2000,  0.0000,  0.0000,\n",
        "            0.0000, -0.2000,  0.6000,\n",
        "          -0.4000,  0.6000,  0.0000,  0.3000, -0.4000,  0.6000,  0.0000,\n",
        "            0.0000,  0.3000,  0.0000,\n",
        "           0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000, -0.4000,\n",
        "            0.0000,  0.2000,  0.0000,\n",
        "           0.3000,  0.0000, -0.4000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
        "            0.5000,  0.0000,  0.5000,\n",
        "           0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
        "            0.0000,  0.2000,  0.6000,\n",
        "         0.2000, -0.4000,  0.0000, -0.4000, -0.4000,  0.0000,  0.5000,\n",
        "            0.0000,  0.0000,  0.5000,\n",
        "           0.0000, -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,\n",
        "            0.6000,  0.0000,  0.5000]])\n",
        "\n",
        "psnr(preds,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF2xP-R4yJDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d3794e-6021-453e-b648-3c34e86b0115"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18.9591)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "#4layers psnr\n",
        "predic = torch.tensor([[-0.0207,  0.1810,  0.3633, -0.2479,  0.6622,  0.4718, -0.0114,\n",
        "           0.1296,  0.4204, -0.3056,  0.0962,  0.0166, -0.4646, -0.2522,\n",
        "          -0.0138, -0.0147, -0.1760,  0.3858, -0.3291,  0.5315,  0.5110,\n",
        "           0.4770, -0.2574,  0.0888,  0.3968, -0.0182,  0.4732,  0.3894,\n",
        "           0.2196,  0.0390,  0.5770, -0.0043,  0.0816,  0.0445, -0.0727,\n",
        "          -0.0800, -0.4367,  0.5932,  0.0186,  0.1516,  0.0878,  0.5425,\n",
        "           0.5929,  0.1295, -0.0356,  0.0297, -0.2448,  0.3284, -0.0316,\n",
        "          -0.3143,  0.2354, -0.3224,  0.2995,  0.0900, -0.0099, -0.0766,\n",
        "           0.4498, -0.2367,  0.3565, -0.0758,  0.2246, -0.4318,  0.0179,\n",
        "           0.1720, -0.2885, -0.1050, -0.0537,  0.4113, -0.3954,  0.2994,\n",
        "          -0.5761, -0.2612, -0.5032, -0.3268, -0.2032,  0.0750,  0.3225,\n",
        "           0.3517,  0.0530,  0.5653, -0.0162, -0.5088,  0.0291, -0.2516,\n",
        "           0.3652, -0.0334,  0.1240,  0.1145, -0.4712, -0.0175,  0.5621,\n",
        "           0.1308,  0.4747,  0.0168,  0.0007,  0.1498, -0.5557, -0.1464,\n",
        "           0.2414, -0.1873 , 0.1102,  0.4069,  0.1036, -0.0311,  0.0446,  0.0744,  0.3796,\n",
        "           0.6837,  0.2498,  0.3552,  0.0390,  0.0345,  0.0701, -0.0130,\n",
        "           0.5248, -0.3041,  0.7109,  0.3595, -0.2475, -0.1196,  0.0384,\n",
        "          -0.0890,  0.0619,  0.3337, -0.0950,  0.3572, -0.3991,  0.1397,\n",
        "           0.6271, -0.1743,  0.1438,  0.0222,  0.0409, -0.0935, -0.0986,\n",
        "          -0.0539,  0.1511, -0.0688, -0.1225,  0.4050, -0.3919,  0.0525,\n",
        "          -0.1003,  0.0971, -0.4445,  0.5713,  0.2559, -0.1163,  0.2976,\n",
        "           0.1133,  0.1291, -0.4882, -0.3475, -0.4599,  0.3940, -0.4969,\n",
        "          -0.4184,  0.1498,  0.2406,  0.0115,  0.4180,  0.0170, -0.3239,\n",
        "          -0.0789,  0.1281,  0.0150,  0.0780,  0.3359,  0.0743,  0.4017,\n",
        "           0.1866, -0.1793, -0.1378,  0.1015, -0.3377, -0.4651, -0.4105,\n",
        "          -0.0906,  0.3038,  0.6977,  0.2837, -0.4966,  0.1104, -0.2817,\n",
        "          -0.4323, -0.0471,  0.6040, -0.0249,  0.0404,  0.3806,  0.0343,\n",
        "          -0.2414, -0.4061,  0.0454,  0.1147, -0.0130,  0.2453,  0.5205,\n",
        "           0.1641,  0.5520, 1.9512e-01, -4.7098e-01, -2.3206e-01,  4.1773e-02, -8.4268e-03,\n",
        "          -2.3763e-01,  4.0556e-02, -1.5227e-01, -1.6652e-01, -7.0769e-04,\n",
        "           4.5431e-01,  1.4204e-01, -1.5527e-02, -1.0078e-01, -2.8119e-01,\n",
        "           1.9540e-01,  3.9129e-01, -2.8585e-01,  3.3151e-01,  4.5519e-01,\n",
        "           7.8885e-02, -1.5782e-01, -3.4423e-02,  4.3259e-01,  6.4715e-02,\n",
        "          -2.3858e-01, -4.2813e-01,  2.8054e-01, -1.1322e-02,  4.2298e-01,\n",
        "          -3.0033e-01,  1.3123e-01,  5.7171e-01, -4.3111e-01,  8.6197e-02,\n",
        "           5.0196e-01,  2.1162e-01,  9.1524e-02, -4.1428e-01, -4.6830e-01,\n",
        "           1.1778e-01, -3.4076e-01,  3.2136e-01, -4.7159e-02,  5.6393e-03,\n",
        "           1.2103e-01, -2.6506e-01, -6.4008e-02, -1.8437e-01, -3.7246e-01,\n",
        "          -3.3579e-01, -3.3959e-01,  5.1774e-01,  5.9716e-01, -1.9229e-01,\n",
        "           4.6184e-02,  9.6242e-02, -4.0020e-02, -2.8932e-01,  7.3592e-02,\n",
        "          -1.8815e-01,  6.4022e-01,  3.4313e-01,  2.0856e-02,  4.1103e-01,\n",
        "           3.8102e-01, -3.7059e-01, -4.4603e-01,  1.3391e-01, -1.6248e-01,\n",
        "          -2.9770e-01,  5.6614e-02,  1.1165e-01,  1.8280e-01,  6.8752e-01,\n",
        "           3.5388e-02,  4.3538e-02, -4.9104e-01, -3.9032e-01,  6.1200e-02,\n",
        "          -2.7420e-02,  5.7896e-01,  5.2084e-01, -1.9168e-01, -4.1365e-01,\n",
        "           2.4115e-01, -1.1974e-01, -4.6052e-02,  2.3215e-01, -1.7548e-01,\n",
        "           1.3191e-04, -3.8948e-01, -5.3571e-02,  5.7901e-01,  1.4563e-01,\n",
        "           7.4949e-02,  4.2233e-02, -5.3525e-02, -3.2480e-01, -1.7591e-01,-9.9854e-02, -1.3788e-01, -3.8776e-01,  8.0480e-02,  3.0448e-02,\n",
        "           1.6202e-01,  5.4454e-01,  2.4381e-01, -2.2894e-01, -7.9946e-02,\n",
        "           7.6652e-05, -3.1508e-01,  4.0713e-01, -4.8687e-02, -4.4852e-01,\n",
        "          -1.9063e-01, -1.7528e-01,  5.0893e-01,  6.1102e-01,  2.9981e-01,\n",
        "           6.0131e-02, -2.0119e-01,  5.2424e-01,  1.6484e-01, -1.6629e-01,\n",
        "           4.4177e-01,  2.7642e-01,  3.9361e-01,  1.9986e-01, -6.1014e-02,\n",
        "           2.4779e-01,  2.4557e-01,  3.5013e-01,  6.3128e-01, -3.5783e-01,\n",
        "           3.8251e-01,  3.7870e-01,  3.7362e-01,  6.2361e-01,  1.2774e-02,\n",
        "           6.7880e-01,  5.4181e-01,  5.0542e-01,  3.6556e-01, -4.5032e-02,\n",
        "           2.1086e-01, -3.6942e-01,  1.1498e-01, -4.8986e-02, -3.0461e-01,\n",
        "          -6.6247e-03,  4.7824e-01,  1.1167e-01,  4.8855e-01,  7.1798e-01,\n",
        "           1.6879e-01,  4.8181e-01, -1.8942e-01,  7.0944e-01,  1.2955e-01,\n",
        "           3.2217e-01,  1.5427e-01, -3.8143e-01, -1.9836e-01,  4.0671e-01,\n",
        "           8.5481e-02,  3.2295e-01,  1.0239e-01, -9.0008e-03,  6.4038e-01,\n",
        "           2.8305e-01, -2.3493e-01, -3.2789e-01,  4.3777e-01,  1.7797e-01,\n",
        "           4.4773e-01,  1.4170e-01, -1.9895e-01,  1.9787e-01, -7.3833e-03,\n",
        "          -4.1867e-01, -3.8572e-01,  1.9006e-01, -2.8322e-01, -2.9506e-01,\n",
        "           2.4363e-01, -2.7715e-01,  2.1411e-01, -2.0644e-01, -4.0481e-01,\n",
        "           3.8130e-01,  4.1470e-01,  3.9903e-01,  5.7948e-01, -1.2489e-01,\n",
        "          -1.5513e-01,  1.5290e-01, -2.0956e-01,  3.6139e-01, -1.0739e-01, 0.0040, -0.5271,  0.1332,  0.4900,  0.0356, -0.5799,  0.1349,\n",
        "          -0.3593, -0.0319,  0.0324, -0.4557,  0.7101,  0.0469,  0.3834,\n",
        "           0.5740,  0.1243, -0.2093,  0.3873,  0.0895, -0.0631,  0.1749,\n",
        "          -0.4688,  0.2214,  0.4076,  0.3450,  0.5634,  0.4353, -0.0031,\n",
        "           0.0941, -0.4475, -0.3160, -0.0950, -0.0538,  0.6370,  0.3079,\n",
        "           0.2594, -0.3219,  0.5661,  0.5566,  0.3528,  0.0859, -0.0767,\n",
        "           0.2061,  0.4714,  0.0083, -0.4456, -0.2961, -0.3941, -0.2900,\n",
        "           0.0819, -0.0337, -0.0102,  0.0859, -0.4251,  0.6070, -0.0316,\n",
        "           0.1063,  0.5070,  0.2150, -0.2391, -0.4070,  0.2172,  0.0495,\n",
        "           0.0835, -0.2452,  0.5351, -0.0327, -0.2884, -0.3648, -0.0094,\n",
        "           0.5583, -0.2700,  0.5532,  0.0142,  0.7182, -0.0661, -0.5174,\n",
        "           0.0961, -0.3635, -0.1779, -0.1880,  0.5331,  0.3277, -0.3084,\n",
        "           0.1672,  0.0055, -0.2268, -0.3967, -0.4033,  0.2496,  0.3316,\n",
        "          -0.1275,  0.0867, -0.0274, -0.0131,  0.3742, -0.1140,  0.4145,\n",
        "           0.5734,  0.4997]])\n",
        "y = torch.tensor([[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
        "           0.0000,  0.5000, -0.4000,  0.2000,  0.0000, -0.4000, -0.2000,\n",
        "           0.0000,  0.0000, -0.2000,  0.5000, -0.4000,  0.5000,  0.5000,\n",
        "           0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,  0.3000,\n",
        "           0.3000,  0.0000,  0.6000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
        "           0.0000, -0.4000,  0.6000,  0.0000,  0.2000,  0.2000,  0.5000,\n",
        "           0.6000,  0.0000,  0.0000,  0.0000, -0.4000,  0.3000,  0.0000,\n",
        "          -0.2000,  0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
        "           0.5000, -0.2000,  0.2000,  0.0000,  0.2000, -0.4000,  0.0000,\n",
        "           0.0000, -0.2000,  0.0000,  0.0000,  0.6000, -0.4000,  0.5000,\n",
        "          -0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
        "           0.3000,  0.2000,  0.5000,  0.0000, -0.4000,  0.0000, -0.4000,\n",
        "           0.3000,  0.0000,  0.0000,  0.0000, -0.4000,  0.0000,  0.5000,\n",
        "           0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000, -0.2000,\n",
        "           0.3000, -0.4000 , 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
        "           0.6000,  0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
        "           0.5000, -0.4000,  0.6000,  0.6000, -0.4000,  0.0000,  0.0000,\n",
        "           0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,  0.3000,\n",
        "           0.6000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
        "           0.0000,  0.0000,  0.0000, -0.2000,  0.6000, -0.4000,  0.6000,\n",
        "           0.0000,  0.3000, -0.4000,  0.6000,  0.0000,  0.0000,  0.3000,\n",
        "           0.0000,  0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000,\n",
        "          -0.4000,  0.0000,  0.2000,  0.0000,  0.3000,  0.0000, -0.4000,\n",
        "           0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.5000,\n",
        "           0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
        "           0.0000,  0.2000,  0.6000,  0.2000, -0.4000,  0.0000, -0.4000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.0000,  0.0000,  0.5000,  0.0000,\n",
        "          -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,  0.6000,\n",
        "           0.0000,  0.5000, 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
        "          -0.2000,  0.2000,  0.0000,  0.5000,  0.2000,  0.0000, -0.2000,\n",
        "          -0.2000,  0.2000,  0.5000, -0.4000,  0.3000,  0.3000,  0.0000,\n",
        "           0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,  0.3000,\n",
        "           0.0000,  0.3000, -0.4000,  0.2000,  0.5000, -0.4000,  0.0000,\n",
        "           0.5000,  0.0000,  0.2000, -0.4000, -0.4000,  0.0000, -0.4000,\n",
        "           0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
        "          -0.4000, -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,\n",
        "           0.0000,  0.0000, -0.2000,  0.2000, -0.2000,  0.6000,  0.3000,\n",
        "           0.0000,  0.5000,  0.6000, -0.4000, -0.4000,  0.2000,  0.0000,\n",
        "          -0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
        "          -0.4000, -0.4000,  0.0000,  0.0000,  0.5000,  0.5000, -0.2000,\n",
        "          -0.4000,  0.3000, -0.2000,  0.0000,  0.0000, -0.2000,  0.0000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,  0.0000,\n",
        "          -0.4000, -0.4000, 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
        "           0.0000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
        "          -0.4000, -0.4000, -0.2000,  0.6000,  0.6000,  0.3000,  0.0000,\n",
        "           0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,  0.5000,\n",
        "           0.0000,  0.0000,  0.2000,  0.5000,  0.3000,  0.5000, -0.4000,\n",
        "           0.5000,  0.2000,  0.5000,  0.6000,  0.2000,  0.5000,  0.5000,\n",
        "           0.5000,  0.5000,  0.0000,  0.0000, -0.4000,  0.0000,  0.0000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,\n",
        "           0.6000, -0.4000,  0.6000,  0.2000,  0.0000,  0.2000, -0.4000,\n",
        "          -0.2000,  0.6000,  0.0000,  0.3000,  0.0000,  0.2000,  0.6000,\n",
        "           0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
        "           0.0000,  0.0000,  0.0000, -0.4000, -0.4000,  0.2000, -0.2000,\n",
        "          -0.4000,  0.3000, -0.4000,  0.3000, -0.2000, -0.4000,  0.5000,\n",
        "           0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000, -0.2000,\n",
        "           0.5000,  0.0000, -0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
        "          -0.4000,  0.2000,  0.0000, -0.4000,  0.5000,  0.0000,  0.3000,\n",
        "           0.5000,  0.0000, -0.2000,  0.6000,  0.0000,  0.0000,  0.2000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,  0.0000,\n",
        "           0.0000, -0.4000, -0.4000, -0.2000, -0.2000,  0.6000,  0.2000,\n",
        "           0.5000, -0.4000,  0.5000,  0.6000,  0.5000,  0.0000,  0.0000,\n",
        "           0.2000,  0.6000,  0.0000, -0.4000, -0.4000, -0.2000, -0.4000,\n",
        "           0.5000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
        "           0.0000,  0.6000,  0.0000, -0.4000, -0.4000,  0.3000,  0.2000,\n",
        "           0.0000, -0.2000,  0.5000,  0.0000, -0.4000, -0.2000,  0.2000,\n",
        "           0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
        "           0.0000, -0.4000, -0.2000, -0.2000,  0.6000,  0.3000, -0.4000,\n",
        "           0.2000,  0.0000, -0.2000, -0.2000, -0.4000,  0.5000,  0.2000,\n",
        "           0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,  0.5000,\n",
        "           0.5000,  0.6000]])\n",
        "psnr(predic, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3z9K58_fKs1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}