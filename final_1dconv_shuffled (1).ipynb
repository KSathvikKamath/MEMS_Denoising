{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF5enzZIrvay"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yXWJYUqsg4_",
        "outputId": "68eb4f1d-7951-4520-9ee7-8d71618ea9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             X    Y\n",
            "0    -0.385173 -0.4\n",
            "1     0.270294  0.3\n",
            "2    -0.514897 -0.4\n",
            "3    -0.496396 -0.4\n",
            "4     0.019509  0.0\n",
            "...        ...  ...\n",
            "4995  0.529139  0.6\n",
            "4996  0.046812  0.0\n",
            "4997  0.043144  0.0\n",
            "4998  0.101030  0.0\n",
            "4999  0.029457  0.0\n",
            "\n",
            "[5000 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_excel('Data (1).xlsx')\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDOysPBRtXSQ",
        "outputId": "ae3b99c0-8f48-46ce-ac93-e327db116c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3852,  0.2703, -0.5149,  ...,  0.3542,  0.4892, -0.2395],\n",
            "        [ 0.4783, -0.5012, -0.2637,  ..., -0.5038, -0.4614,  0.0993],\n",
            "        [ 0.0138,  0.2581, -0.2445,  ...,  0.5223, -0.4196, -0.0514],\n",
            "        ...,\n",
            "        [ 0.2618, -0.2438, -0.4598,  ..., -0.0342,  0.2702, -0.4503],\n",
            "        [ 0.1504,  0.0278,  0.2638,  ..., -0.2341,  0.2722, -0.4199],\n",
            "        [-0.5918,  0.6202,  0.4938,  ...,  0.0431,  0.1010,  0.0295]])\n"
          ]
        }
      ],
      "source": [
        "ip_data = df['X'].to_numpy()\n",
        "op_data = df['Y'].to_numpy()\n",
        "\n",
        "inputVal= np.array(op_data).astype(np.float32)\n",
        "trainVal= np.array(ip_data).astype(np.float32)\n",
        "a= torch.tensor(inputVal)\n",
        "b= torch.tensor(trainVal)\n",
        "x_op_train= a.resize_(50,100)\n",
        "x_ip_train= b.resize_(50,100)\n",
        "print(b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iFLJsGHtnUH",
        "outputId": "2e5837e9-b8d7-48aa-b5b6-e13eb8a9a313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 100])\n",
            "tensor([[[ 0.6106, -0.4206,  0.5336,  0.1708,  0.4902, -0.3182, -0.4627,\n",
            "           0.0990, -0.4953,  0.1400,  0.1668,  0.5718, -0.2472, -0.1554,\n",
            "           0.1055,  0.1034,  0.4733,  0.3353, -0.4877,  0.4978, -0.5606,\n",
            "           0.1206,  0.4430,  0.3148,  0.4996, -0.2337, -0.3936,  0.2560,\n",
            "           0.0135,  0.0348,  0.0838, -0.2311, -0.5195,  0.4363,  0.3053,\n",
            "          -0.2698,  0.0776,  0.0124,  0.0878,  0.5329,  0.5137,  0.1034,\n",
            "           0.4547, -0.4211, -0.4350, -0.4431, -0.4481,  0.4435,  0.5099,\n",
            "          -0.3413,  0.1738, -0.4349,  0.0569,  0.0399,  0.4245,  0.2505,\n",
            "           0.1821, -0.4673,  0.3470,  0.4805,  0.4187,  0.0063, -0.2307,\n",
            "          -0.5121,  0.0570, -0.0118, -0.2803,  0.3027,  0.0210, -0.4761,\n",
            "           0.5770,  0.3297,  0.4496,  0.1235, -0.0189,  0.3199,  0.0471,\n",
            "           0.5768,  0.5011,  0.1865, -0.4733, -0.5001,  0.0215,  0.0918,\n",
            "           0.5522, -0.2396,  0.5945, -0.4900,  0.2333,  0.1718, -0.4591,\n",
            "          -0.5464, -0.5069,  0.0935, -0.1911,  0.0931,  0.4547, -0.4906,\n",
            "           0.0333,  0.2540]]])\n",
            "tensor([[[ 0.6000, -0.4000,  0.6000,  0.2000,  0.6000, -0.2000, -0.4000,\n",
            "           0.2000, -0.4000,  0.2000,  0.2000,  0.6000, -0.2000, -0.2000,\n",
            "           0.0000,  0.2000,  0.5000,  0.5000, -0.4000,  0.6000, -0.4000,\n",
            "           0.0000,  0.5000,  0.3000,  0.5000, -0.2000, -0.4000,  0.3000,\n",
            "           0.0000,  0.0000,  0.0000, -0.2000, -0.4000,  0.6000,  0.3000,\n",
            "          -0.2000,  0.0000,  0.0000,  0.0000,  0.6000,  0.5000,  0.2000,\n",
            "           0.6000, -0.4000, -0.4000, -0.4000, -0.4000,  0.6000,  0.5000,\n",
            "          -0.4000,  0.2000, -0.4000,  0.0000,  0.0000,  0.6000,  0.3000,\n",
            "           0.5000, -0.4000,  0.3000,  0.5000,  0.6000,  0.0000, -0.2000,\n",
            "          -0.4000,  0.0000,  0.0000, -0.2000,  0.6000,  0.2000, -0.4000,\n",
            "           0.6000,  0.3000,  0.5000,  0.0000,  0.0000,  0.3000,  0.0000,\n",
            "           0.5000,  0.6000,  0.2000, -0.4000, -0.4000,  0.0000,  0.2000,\n",
            "           0.5000, -0.2000,  0.6000, -0.4000,  0.3000,  0.3000, -0.4000,\n",
            "          -0.4000, -0.4000,  0.2000, -0.2000,  0.0000,  0.5000, -0.4000,\n",
            "           0.0000,  0.3000]]])\n",
            "torch.Size([1, 1, 100])\n",
            "tensor([[[-0.4780,  0.0830,  0.5464,  0.0777,  0.1189, -0.3965, -0.3577,\n",
            "           0.2520,  0.3249,  0.0260,  0.1397, -0.3480,  0.4165,  0.5134,\n",
            "           0.2007,  0.4741, -0.0323, -0.5274,  0.3487, -0.4432, -0.4553,\n",
            "           0.4288,  0.0625,  0.0077, -0.4615,  0.0352, -0.5076,  0.0437,\n",
            "          -0.0385,  0.0384, -0.0157,  0.4230,  0.0803,  0.1737, -0.4040,\n",
            "           0.0975,  0.0468,  0.1058, -0.2763,  0.4955,  0.0076,  0.4524,\n",
            "           0.5238,  0.0703, -0.0642,  0.0433,  0.4765,  0.1652,  0.1415,\n",
            "          -0.0467, -0.4775,  0.2901,  0.0204, -0.4930,  0.0191,  0.1638,\n",
            "           0.4089, -0.4216, -0.1863, -0.4708,  0.0932,  0.3320,  0.0629,\n",
            "           0.3193,  0.6149, -0.5032,  0.0167,  0.0585,  0.1293, -0.4233,\n",
            "           0.4751,  0.2688,  0.5746,  0.1116,  0.0699,  0.2650,  0.0551,\n",
            "           0.3977, -0.4014, -0.4968,  0.0625, -0.5230,  0.2973,  0.0856,\n",
            "           0.1312,  0.0504, -0.0287,  0.0415, -0.0234,  0.0254,  0.1098,\n",
            "          -0.0533,  0.5552,  0.4642, -0.1169, -0.0805, -0.4318, -0.6009,\n",
            "           0.0565,  0.3534]]])\n",
            "tensor([[[-0.4000,  0.0000,  0.6000,  0.0000,  0.2000, -0.4000, -0.4000,\n",
            "           0.3000,  0.3000,  0.0000,  0.2000, -0.4000,  0.5000,  0.5000,\n",
            "           0.2000,  0.5000,  0.0000, -0.4000,  0.3000, -0.4000, -0.4000,\n",
            "           0.5000,  0.0000,  0.0000, -0.4000,  0.0000, -0.4000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000,  0.6000,  0.0000,  0.3000, -0.4000,\n",
            "           0.2000,  0.0000,  0.0000, -0.2000,  0.5000,  0.0000,  0.6000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000,  0.6000,  0.2000,  0.0000,\n",
            "           0.0000, -0.4000,  0.3000,  0.0000, -0.4000,  0.0000,  0.2000,\n",
            "          -0.4000, -0.4000, -0.2000, -0.4000,  0.0000,  0.3000,  0.0000,\n",
            "           0.3000,  0.6000, -0.4000,  0.0000,  0.0000,  0.0000, -0.4000,\n",
            "           0.5000,  0.2000,  0.6000,  0.2000,  0.0000,  0.3000,  0.0000,\n",
            "           0.5000, -0.4000, -0.4000,  0.0000, -0.4000,  0.3000,  0.0000,\n",
            "           0.2000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000,  0.6000,  0.6000, -0.2000, -0.2000, -0.4000, -0.4000,\n",
            "           0.0000,  0.6000]]])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py:775: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x_ip_train,x_op_train, test_size=0.2, random_state=42,shuffle=True)\n",
        "X_val, X_test1, y_val, y_test1 = train_test_split(X_test, y_test, test_size=0.5, random_state=42,shuffle=True)\n",
        "\n",
        "X_train = X_train.reshape(40,1,100)\n",
        "y_train = y_train.reshape(40,1,100)\n",
        "\n",
        "\n",
        "X_val = X_val.resize(5,1,100)\n",
        "y_val = y_val.resize(5,1,100)\n",
        "\n",
        "X_test1 = X_test1.resize(5,1,100)\n",
        "y_test1 = y_test1.resize(5,1,100)\n",
        "\n",
        "loader = DataLoader(list(zip(X_train,y_train)), shuffle=True,batch_size=1)\n",
        "test_loader=DataLoader(list(zip(X_test1,y_test1)), shuffle=True,batch_size=1)\n",
        "val_loader=DataLoader(list(zip(X_val,y_val)), shuffle=True,batch_size=1)\n",
        "\n",
        "for X_batch,Y_batch in loader:\n",
        "    print(X_batch.size())\n",
        "    print(X_batch)\n",
        "    print(Y_batch)\n",
        "    break\n",
        "for X_batch,Y_batch in val_loader:\n",
        "    print(X_batch.size())\n",
        "    print(X_batch)\n",
        "    print(Y_batch)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNlb1WS8t1iM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Encoder\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "     \n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "        self.dropout = nn.Dropout(0.2) \n",
        "        # Decoder\n",
        "        self.t_conv1 = nn.ConvTranspose1d(in_channels=128, out_channels=64, kernel_size=3, stride=1,padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.t_conv2 = nn.ConvTranspose1d(in_channels=64, out_channels=32, kernel_size=3, stride=1,padding=1)\n",
        "        self.t_conv3 = nn.ConvTranspose1d(in_channels=32, out_channels=16, kernel_size=3, stride=1,padding=1)\n",
        "        self.t_conv4 = nn.ConvTranspose1d(in_channels=16, out_channels=1, kernel_size=3, stride=1,padding=1)\n",
        "        self.unpool = nn.Upsample(scale_factor=2,mode='nearest')\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        #print('conv1',x.size())\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        #print('conv1pool',x.size())\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = self.relu(x)\n",
        "        #print('conv2',x.size())\n",
        "        #x = self.relu(x)\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        #print('conv3',x.size())\n",
        "        x = self.relu(x)\n",
        "        x = nn.functional.relu(self.conv4(x))\n",
        "        #print('conv4',x.size())\n",
        "        x= self.dropout(x)\n",
        "        #print('dropout',x.size())\n",
        "        \n",
        "        # Decoder\n",
        "        \n",
        "        \n",
        "        x = nn.functional.relu(self.t_conv1(x))\n",
        "        #print('conv1t',x.size())\n",
        "        x = nn.functional.relu(self.t_conv2(x))\n",
        "        #print('conv2t',x.size())\n",
        "        #x = self.pool(x)\n",
        "        #print('pool',x.size())\n",
        "        #x = x.view(1,50,50)\n",
        "        #print('x_out',x.size())\n",
        "       \n",
        "        x = self.t_conv3(x)\n",
        "        x = self.unpool(x)\n",
        "        #print('conv2t',x.size())\n",
        "        #print('xunpool',x.size())\n",
        "        x = self.t_conv4(x)\n",
        "        #print('conv4t',x.size())\n",
        "        #print('conv3t',x.size())\n",
        "        #x = x[:, :, :400]\n",
        "  \n",
        "        return x\n",
        "model = Autoencoder()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1,32,3,padding=1,stride=2)\n",
        "    self.act = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(32,16,3,padding=1,stride=2)\n",
        "    self.conv3 = nn.Conv2d(16,8,3,padding=1,stride=2)\n",
        "    #self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    print(x.size())\n",
        "    out = self.conv1(x)\n",
        "    out = self.act(out)\n",
        "    print(x.size())\n",
        "    #out = self.pool(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.act(out)\n",
        "    print(x.size())\n",
        "    out = self.pool(out)\n",
        "    out = self.conv3(out)\n",
        "    print(x.size())\n",
        "    out = self.act(out)\n",
        "    #out = self.pool(out)\n",
        "    return out\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.conv1 = nn.ConvTranspose2d(8,16,3,padding=1,stride=2)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.conv2 = nn.ConvTranspose2d(16,32,3,padding=1,stride=2)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.conv3 = nn.ConvTranspose2d(32,1,3,padding=1,stride=2)\n",
        "    self.act3 = nn.ReLU()\n",
        "\n",
        "   \n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv1(x)\n",
        "    out = self.act1(out)\n",
        "    print(x.size())\n",
        "    out = self.conv2(out)\n",
        "    out = self.act2(out)\n",
        "    print(x.size())\n",
        "    out = self.conv3(out)\n",
        "    out = self.act3(out)\n",
        "    print(x.size())\n",
        "    \n",
        "    return out\n",
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "class Autoencoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Autoencoder,self).__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    enc_out = self.encoder(x)\n",
        "    return self.decoder(enc_out)\n",
        "model = Autoencoder()"
      ],
      "metadata": {
        "id": "4H2Y28d-D3eY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "8c95bb25-f8a6-441e-8979-634e0190e55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-6bf142c55c0b>\"\u001b[0;36m, line \u001b[0;32m60\u001b[0m\n\u001b[0;31m    model = Autoencoder()\u001b[0m\n\u001b[0m                         \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy4w124It6eP"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader,loss_fn, optimizer):\n",
        "    num_correct=0\n",
        "    size = len(dataloader.dataset)\n",
        "    print(size)\n",
        "    for batch, (X, y) in enumerate(dataloader):       \n",
        "        \n",
        "        pred = model(X)       \n",
        "        #pred = pred.reshape(1,400)  \n",
        "        #print('pred',pred)     \n",
        "        loss = loss_fn(pred.float(), y.float())\n",
        "        #num_correct += (pred.argmax(1) == y).type(torch.float).sum().item()       \n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()  \n",
        "        optimizer.step() \n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "    #num_correct /=size\n",
        "    #accuracy = 100 * num_correct\n",
        "    #print(f\"Accuracy: {accuracy:.2f}% \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk2cI_QMuA2b"
      },
      "outputs": [],
      "source": [
        "def val_loop(dataloader, loss_fn):\n",
        "    size = len(dataloader.dataset)    \n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    test_loss, correct = 0, 0\n",
        "    i=1\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:           \n",
        "            #print('y',y)\n",
        "            pred = model(X) \n",
        "            #pred = pred.reshape(1,50)  \n",
        "            #print('pred',pred)         \n",
        "            test_loss += loss_fn(pred.float(), y.float()).item()            \n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            #print(test_loss)\n",
        "            \n",
        "    test_loss /= num_batches \n",
        "    #print('tstLoss',test_loss)   \n",
        "    #correct /= size\n",
        "    print(f\"Test Error:  Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGxki6KeuMiG"
      },
      "outputs": [],
      "source": [
        "x =[]\n",
        "Y = []\n",
        "preds =[]\n",
        "\n",
        "def test_loop(dataloader, loss_fn):\n",
        "    size = len(dataloader.dataset)    \n",
        "    num_batches = len(dataloader)\n",
        "    \n",
        "    test_loss, correct = 0, 0\n",
        "    i=1\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "             \n",
        "            print('x',X)          \n",
        "            print('y',y)\n",
        "            pred = model(X) \n",
        "            #pred = pred.reshape(1,100)  \n",
        "            print('pred',pred)  \n",
        "            x.append(X)\n",
        "            Y.append(y)\n",
        "            preds.append(pred)\n",
        "            test_loss += loss_fn(pred.float(), y.float()).item()            \n",
        "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            #print(test_loss)\n",
        "            \n",
        "    test_loss /= num_batches \n",
        "    #print('tstLoss',test_loss)   \n",
        "    correct /= size\n",
        "    print(f\"Test Error:  Avg loss: {test_loss:>8f} \\n\")\n",
        "    print(x)\n",
        "    print(Y)\n",
        "    print(preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHlMKACuuQAD",
        "outputId": "002e2570-03b8-49d0-fa46-081af3b5408b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.121196  [    1/   40]\n",
            "loss: 0.111865  [   21/   40]\n",
            "Test Error:  Avg loss: 0.095696 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.100543  [    1/   40]\n",
            "loss: 0.101702  [   21/   40]\n",
            "Test Error:  Avg loss: 0.090566 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.117101  [    1/   40]\n",
            "loss: 0.105501  [   21/   40]\n",
            "Test Error:  Avg loss: 0.081646 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.113867  [    1/   40]\n",
            "loss: 0.090439  [   21/   40]\n",
            "Test Error:  Avg loss: 0.071933 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.073983  [    1/   40]\n",
            "loss: 0.062901  [   21/   40]\n",
            "Test Error:  Avg loss: 0.067635 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.074178  [    1/   40]\n",
            "loss: 0.103254  [   21/   40]\n",
            "Test Error:  Avg loss: 0.065153 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.091351  [    1/   40]\n",
            "loss: 0.067108  [   21/   40]\n",
            "Test Error:  Avg loss: 0.061599 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.066661  [    1/   40]\n",
            "loss: 0.060365  [   21/   40]\n",
            "Test Error:  Avg loss: 0.058331 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.042711  [    1/   40]\n",
            "loss: 0.058535  [   21/   40]\n",
            "Test Error:  Avg loss: 0.056669 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.060689  [    1/   40]\n",
            "loss: 0.050488  [   21/   40]\n",
            "Test Error:  Avg loss: 0.055594 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.044431  [    1/   40]\n",
            "loss: 0.053148  [   21/   40]\n",
            "Test Error:  Avg loss: 0.053850 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.063070  [    1/   40]\n",
            "loss: 0.062058  [   21/   40]\n",
            "Test Error:  Avg loss: 0.051166 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.055806  [    1/   40]\n",
            "loss: 0.049632  [   21/   40]\n",
            "Test Error:  Avg loss: 0.047610 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.046145  [    1/   40]\n",
            "loss: 0.043562  [   21/   40]\n",
            "Test Error:  Avg loss: 0.045197 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.045749  [    1/   40]\n",
            "loss: 0.044071  [   21/   40]\n",
            "Test Error:  Avg loss: 0.041689 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.032175  [    1/   40]\n",
            "loss: 0.036314  [   21/   40]\n",
            "Test Error:  Avg loss: 0.039212 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.051757  [    1/   40]\n",
            "loss: 0.047558  [   21/   40]\n",
            "Test Error:  Avg loss: 0.037817 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.041939  [    1/   40]\n",
            "loss: 0.056143  [   21/   40]\n",
            "Test Error:  Avg loss: 0.035702 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.037580  [    1/   40]\n",
            "loss: 0.025311  [   21/   40]\n",
            "Test Error:  Avg loss: 0.034929 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.038548  [    1/   40]\n",
            "loss: 0.046223  [   21/   40]\n",
            "Test Error:  Avg loss: 0.033807 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.029699  [    1/   40]\n",
            "loss: 0.044173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.032608 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.033210  [    1/   40]\n",
            "loss: 0.049906  [   21/   40]\n",
            "Test Error:  Avg loss: 0.031220 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.025968  [    1/   40]\n",
            "loss: 0.028656  [   21/   40]\n",
            "Test Error:  Avg loss: 0.030269 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.055279  [    1/   40]\n",
            "loss: 0.036144  [   21/   40]\n",
            "Test Error:  Avg loss: 0.029075 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020101  [    1/   40]\n",
            "loss: 0.036948  [   21/   40]\n",
            "Test Error:  Avg loss: 0.028809 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.029277  [    1/   40]\n",
            "loss: 0.034590  [   21/   40]\n",
            "Test Error:  Avg loss: 0.027746 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.022622  [    1/   40]\n",
            "loss: 0.040910  [   21/   40]\n",
            "Test Error:  Avg loss: 0.027620 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.034243  [    1/   40]\n",
            "loss: 0.032412  [   21/   40]\n",
            "Test Error:  Avg loss: 0.027329 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021463  [    1/   40]\n",
            "loss: 0.022537  [   21/   40]\n",
            "Test Error:  Avg loss: 0.025977 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.033133  [    1/   40]\n",
            "loss: 0.035898  [   21/   40]\n",
            "Test Error:  Avg loss: 0.025751 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019894  [    1/   40]\n",
            "loss: 0.030521  [   21/   40]\n",
            "Test Error:  Avg loss: 0.025815 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.024709  [    1/   40]\n",
            "loss: 0.026014  [   21/   40]\n",
            "Test Error:  Avg loss: 0.025312 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.031321  [    1/   40]\n",
            "loss: 0.017349  [   21/   40]\n",
            "Test Error:  Avg loss: 0.025185 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.030472  [    1/   40]\n",
            "loss: 0.024930  [   21/   40]\n",
            "Test Error:  Avg loss: 0.025384 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.034023  [    1/   40]\n",
            "loss: 0.031359  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024688 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.031699  [    1/   40]\n",
            "loss: 0.026450  [   21/   40]\n",
            "Test Error:  Avg loss: 0.023988 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.030144  [    1/   40]\n",
            "loss: 0.025618  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024483 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.046168  [    1/   40]\n",
            "loss: 0.019880  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024424 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.035089  [    1/   40]\n",
            "loss: 0.031821  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024294 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023819  [    1/   40]\n",
            "loss: 0.027471  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024497 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.026704  [    1/   40]\n",
            "loss: 0.027726  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024308 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021341  [    1/   40]\n",
            "loss: 0.022734  [   21/   40]\n",
            "Test Error:  Avg loss: 0.024265 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019868  [    1/   40]\n",
            "loss: 0.018097  [   21/   40]\n",
            "Test Error:  Avg loss: 0.023693 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.028930  [    1/   40]\n",
            "loss: 0.025441  [   21/   40]\n",
            "Test Error:  Avg loss: 0.023150 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014252  [    1/   40]\n",
            "loss: 0.032270  [   21/   40]\n",
            "Test Error:  Avg loss: 0.022678 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.042234  [    1/   40]\n",
            "loss: 0.020928  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021892 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.042784  [    1/   40]\n",
            "loss: 0.012551  [   21/   40]\n",
            "Test Error:  Avg loss: 0.022779 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.032676  [    1/   40]\n",
            "loss: 0.027313  [   21/   40]\n",
            "Test Error:  Avg loss: 0.022587 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020021  [    1/   40]\n",
            "loss: 0.020685  [   21/   40]\n",
            "Test Error:  Avg loss: 0.022175 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017443  [    1/   40]\n",
            "loss: 0.019474  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021699 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027354  [    1/   40]\n",
            "loss: 0.028176  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021687 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027265  [    1/   40]\n",
            "loss: 0.022965  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021880 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023735  [    1/   40]\n",
            "loss: 0.026417  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021910 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012395  [    1/   40]\n",
            "loss: 0.026333  [   21/   40]\n",
            "Test Error:  Avg loss: 0.022293 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017651  [    1/   40]\n",
            "loss: 0.024885  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021476 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.037501  [    1/   40]\n",
            "loss: 0.017270  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020828 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027206  [    1/   40]\n",
            "loss: 0.015217  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021492 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018248  [    1/   40]\n",
            "loss: 0.030329  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021215 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023784  [    1/   40]\n",
            "loss: 0.016421  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021669 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.023385  [    1/   40]\n",
            "loss: 0.020410  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021168 \n",
            "\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021204  [    1/   40]\n",
            "loss: 0.018046  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021602 \n",
            "\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.025209  [    1/   40]\n",
            "loss: 0.025416  [   21/   40]\n",
            "Test Error:  Avg loss: 0.021142 \n",
            "\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017561  [    1/   40]\n",
            "loss: 0.026486  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020554 \n",
            "\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.027150  [    1/   40]\n",
            "loss: 0.023133  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020361 \n",
            "\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020679  [    1/   40]\n",
            "loss: 0.017453  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020094 \n",
            "\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017712  [    1/   40]\n",
            "loss: 0.017066  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019371 \n",
            "\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018588  [    1/   40]\n",
            "loss: 0.021693  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020216 \n",
            "\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010873  [    1/   40]\n",
            "loss: 0.016898  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020488 \n",
            "\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018425  [    1/   40]\n",
            "loss: 0.017700  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020737 \n",
            "\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.021468  [    1/   40]\n",
            "loss: 0.026440  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019994 \n",
            "\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012254  [    1/   40]\n",
            "loss: 0.019305  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019662 \n",
            "\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011560  [    1/   40]\n",
            "loss: 0.010996  [   21/   40]\n",
            "Test Error:  Avg loss: 0.020013 \n",
            "\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017934  [    1/   40]\n",
            "loss: 0.010074  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019813 \n",
            "\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.022875  [    1/   40]\n",
            "loss: 0.011204  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019310 \n",
            "\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012678  [    1/   40]\n",
            "loss: 0.011968  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019343 \n",
            "\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.020815  [    1/   40]\n",
            "loss: 0.017896  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019432 \n",
            "\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015283  [    1/   40]\n",
            "loss: 0.017468  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019395 \n",
            "\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012504  [    1/   40]\n",
            "loss: 0.013703  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018373 \n",
            "\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015946  [    1/   40]\n",
            "loss: 0.027537  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019815 \n",
            "\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016254  [    1/   40]\n",
            "loss: 0.019858  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019497 \n",
            "\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017106  [    1/   40]\n",
            "loss: 0.018085  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018442 \n",
            "\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014685  [    1/   40]\n",
            "loss: 0.014633  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019445 \n",
            "\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011792  [    1/   40]\n",
            "loss: 0.018791  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018610 \n",
            "\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016199  [    1/   40]\n",
            "loss: 0.022780  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018153 \n",
            "\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013341  [    1/   40]\n",
            "loss: 0.017820  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018446 \n",
            "\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011410  [    1/   40]\n",
            "loss: 0.009121  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019045 \n",
            "\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008473  [    1/   40]\n",
            "loss: 0.010399  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018465 \n",
            "\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017221  [    1/   40]\n",
            "loss: 0.025432  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018253 \n",
            "\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013779  [    1/   40]\n",
            "loss: 0.020767  [   21/   40]\n",
            "Test Error:  Avg loss: 0.019090 \n",
            "\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012569  [    1/   40]\n",
            "loss: 0.009496  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018166 \n",
            "\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009317  [    1/   40]\n",
            "loss: 0.017565  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018231 \n",
            "\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014504  [    1/   40]\n",
            "loss: 0.009605  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018350 \n",
            "\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.025349  [    1/   40]\n",
            "loss: 0.008658  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018491 \n",
            "\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010299  [    1/   40]\n",
            "loss: 0.018087  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017704 \n",
            "\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014545  [    1/   40]\n",
            "loss: 0.015593  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018852 \n",
            "\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018644  [    1/   40]\n",
            "loss: 0.019194  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017790 \n",
            "\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014760  [    1/   40]\n",
            "loss: 0.008707  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017540 \n",
            "\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018318  [    1/   40]\n",
            "loss: 0.014304  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018021 \n",
            "\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009314  [    1/   40]\n",
            "loss: 0.018840  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018534 \n",
            "\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010753  [    1/   40]\n",
            "loss: 0.016456  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017174 \n",
            "\n",
            "Epoch 101\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008695  [    1/   40]\n",
            "loss: 0.015209  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018378 \n",
            "\n",
            "Epoch 102\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008286  [    1/   40]\n",
            "loss: 0.009140  [   21/   40]\n",
            "Test Error:  Avg loss: 0.018101 \n",
            "\n",
            "Epoch 103\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010474  [    1/   40]\n",
            "loss: 0.007860  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017813 \n",
            "\n",
            "Epoch 104\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016572  [    1/   40]\n",
            "loss: 0.014667  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017611 \n",
            "\n",
            "Epoch 105\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019035  [    1/   40]\n",
            "loss: 0.013555  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017899 \n",
            "\n",
            "Epoch 106\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009808  [    1/   40]\n",
            "loss: 0.009312  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017226 \n",
            "\n",
            "Epoch 107\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018258  [    1/   40]\n",
            "loss: 0.021967  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016853 \n",
            "\n",
            "Epoch 108\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015279  [    1/   40]\n",
            "loss: 0.016106  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017620 \n",
            "\n",
            "Epoch 109\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014189  [    1/   40]\n",
            "loss: 0.023279  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017248 \n",
            "\n",
            "Epoch 110\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012048  [    1/   40]\n",
            "loss: 0.019853  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017939 \n",
            "\n",
            "Epoch 111\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015767  [    1/   40]\n",
            "loss: 0.010713  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017692 \n",
            "\n",
            "Epoch 112\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017647  [    1/   40]\n",
            "loss: 0.016678  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017424 \n",
            "\n",
            "Epoch 113\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017731  [    1/   40]\n",
            "loss: 0.019021  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016981 \n",
            "\n",
            "Epoch 114\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014066  [    1/   40]\n",
            "loss: 0.021057  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017279 \n",
            "\n",
            "Epoch 115\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013048  [    1/   40]\n",
            "loss: 0.013864  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017485 \n",
            "\n",
            "Epoch 116\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009260  [    1/   40]\n",
            "loss: 0.008607  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017274 \n",
            "\n",
            "Epoch 117\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011635  [    1/   40]\n",
            "loss: 0.012207  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017204 \n",
            "\n",
            "Epoch 118\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015780  [    1/   40]\n",
            "loss: 0.021004  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017995 \n",
            "\n",
            "Epoch 119\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011390  [    1/   40]\n",
            "loss: 0.013742  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016828 \n",
            "\n",
            "Epoch 120\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013277  [    1/   40]\n",
            "loss: 0.016340  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017681 \n",
            "\n",
            "Epoch 121\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015368  [    1/   40]\n",
            "loss: 0.009822  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016962 \n",
            "\n",
            "Epoch 122\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013245  [    1/   40]\n",
            "loss: 0.017390  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016562 \n",
            "\n",
            "Epoch 123\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009210  [    1/   40]\n",
            "loss: 0.012183  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016999 \n",
            "\n",
            "Epoch 124\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013896  [    1/   40]\n",
            "loss: 0.018397  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017690 \n",
            "\n",
            "Epoch 125\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015701  [    1/   40]\n",
            "loss: 0.010130  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016647 \n",
            "\n",
            "Epoch 126\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016508  [    1/   40]\n",
            "loss: 0.014968  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016820 \n",
            "\n",
            "Epoch 127\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009825  [    1/   40]\n",
            "loss: 0.009529  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016737 \n",
            "\n",
            "Epoch 128\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017646  [    1/   40]\n",
            "loss: 0.011450  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016947 \n",
            "\n",
            "Epoch 129\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019982  [    1/   40]\n",
            "loss: 0.020783  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016370 \n",
            "\n",
            "Epoch 130\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013094  [    1/   40]\n",
            "loss: 0.017721  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016463 \n",
            "\n",
            "Epoch 131\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014431  [    1/   40]\n",
            "loss: 0.014036  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016585 \n",
            "\n",
            "Epoch 132\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018790  [    1/   40]\n",
            "loss: 0.014152  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016520 \n",
            "\n",
            "Epoch 133\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012418  [    1/   40]\n",
            "loss: 0.014200  [   21/   40]\n",
            "Test Error:  Avg loss: 0.017117 \n",
            "\n",
            "Epoch 134\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011676  [    1/   40]\n",
            "loss: 0.013887  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015469 \n",
            "\n",
            "Epoch 135\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011362  [    1/   40]\n",
            "loss: 0.021746  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016633 \n",
            "\n",
            "Epoch 136\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010094  [    1/   40]\n",
            "loss: 0.020479  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016759 \n",
            "\n",
            "Epoch 137\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010009  [    1/   40]\n",
            "loss: 0.012894  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015952 \n",
            "\n",
            "Epoch 138\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010810  [    1/   40]\n",
            "loss: 0.008453  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016282 \n",
            "\n",
            "Epoch 139\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014974  [    1/   40]\n",
            "loss: 0.008004  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016469 \n",
            "\n",
            "Epoch 140\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009431  [    1/   40]\n",
            "loss: 0.020259  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016270 \n",
            "\n",
            "Epoch 141\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008451  [    1/   40]\n",
            "loss: 0.009440  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016269 \n",
            "\n",
            "Epoch 142\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017916  [    1/   40]\n",
            "loss: 0.013693  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015941 \n",
            "\n",
            "Epoch 143\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013021  [    1/   40]\n",
            "loss: 0.007779  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016189 \n",
            "\n",
            "Epoch 144\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012629  [    1/   40]\n",
            "loss: 0.014440  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016506 \n",
            "\n",
            "Epoch 145\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008013  [    1/   40]\n",
            "loss: 0.017492  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016026 \n",
            "\n",
            "Epoch 146\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008195  [    1/   40]\n",
            "loss: 0.012762  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015696 \n",
            "\n",
            "Epoch 147\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010788  [    1/   40]\n",
            "loss: 0.008946  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016650 \n",
            "\n",
            "Epoch 148\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014503  [    1/   40]\n",
            "loss: 0.014664  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015941 \n",
            "\n",
            "Epoch 149\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008207  [    1/   40]\n",
            "loss: 0.011413  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016433 \n",
            "\n",
            "Epoch 150\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014628  [    1/   40]\n",
            "loss: 0.008102  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016213 \n",
            "\n",
            "Epoch 151\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018573  [    1/   40]\n",
            "loss: 0.011567  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015903 \n",
            "\n",
            "Epoch 152\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008251  [    1/   40]\n",
            "loss: 0.008173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016209 \n",
            "\n",
            "Epoch 153\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008364  [    1/   40]\n",
            "loss: 0.011342  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015967 \n",
            "\n",
            "Epoch 154\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018590  [    1/   40]\n",
            "loss: 0.007773  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015598 \n",
            "\n",
            "Epoch 155\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018182  [    1/   40]\n",
            "loss: 0.015649  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016078 \n",
            "\n",
            "Epoch 156\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014336  [    1/   40]\n",
            "loss: 0.008973  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016079 \n",
            "\n",
            "Epoch 157\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006646  [    1/   40]\n",
            "loss: 0.011387  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015646 \n",
            "\n",
            "Epoch 158\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008456  [    1/   40]\n",
            "loss: 0.010693  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016117 \n",
            "\n",
            "Epoch 159\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017354  [    1/   40]\n",
            "loss: 0.019879  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016814 \n",
            "\n",
            "Epoch 160\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007018  [    1/   40]\n",
            "loss: 0.012314  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015628 \n",
            "\n",
            "Epoch 161\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013541  [    1/   40]\n",
            "loss: 0.015698  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016405 \n",
            "\n",
            "Epoch 162\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013327  [    1/   40]\n",
            "loss: 0.013551  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015963 \n",
            "\n",
            "Epoch 163\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006717  [    1/   40]\n",
            "loss: 0.016756  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016235 \n",
            "\n",
            "Epoch 164\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011994  [    1/   40]\n",
            "loss: 0.010401  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015824 \n",
            "\n",
            "Epoch 165\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014803  [    1/   40]\n",
            "loss: 0.011122  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015283 \n",
            "\n",
            "Epoch 166\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009889  [    1/   40]\n",
            "loss: 0.017007  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016109 \n",
            "\n",
            "Epoch 167\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007946  [    1/   40]\n",
            "loss: 0.013645  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016041 \n",
            "\n",
            "Epoch 168\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008548  [    1/   40]\n",
            "loss: 0.013804  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015575 \n",
            "\n",
            "Epoch 169\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008411  [    1/   40]\n",
            "loss: 0.011401  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015854 \n",
            "\n",
            "Epoch 170\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007460  [    1/   40]\n",
            "loss: 0.019346  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015843 \n",
            "\n",
            "Epoch 171\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014334  [    1/   40]\n",
            "loss: 0.017781  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015303 \n",
            "\n",
            "Epoch 172\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007460  [    1/   40]\n",
            "loss: 0.015613  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015896 \n",
            "\n",
            "Epoch 173\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017794  [    1/   40]\n",
            "loss: 0.013924  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015064 \n",
            "\n",
            "Epoch 174\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013518  [    1/   40]\n",
            "loss: 0.008400  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016053 \n",
            "\n",
            "Epoch 175\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010007  [    1/   40]\n",
            "loss: 0.012292  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015783 \n",
            "\n",
            "Epoch 176\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018987  [    1/   40]\n",
            "loss: 0.014954  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015312 \n",
            "\n",
            "Epoch 177\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014203  [    1/   40]\n",
            "loss: 0.011669  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015823 \n",
            "\n",
            "Epoch 178\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011901  [    1/   40]\n",
            "loss: 0.016773  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015904 \n",
            "\n",
            "Epoch 179\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007354  [    1/   40]\n",
            "loss: 0.012436  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015561 \n",
            "\n",
            "Epoch 180\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015482  [    1/   40]\n",
            "loss: 0.007200  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015330 \n",
            "\n",
            "Epoch 181\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013994  [    1/   40]\n",
            "loss: 0.013547  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015587 \n",
            "\n",
            "Epoch 182\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014536  [    1/   40]\n",
            "loss: 0.019514  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015328 \n",
            "\n",
            "Epoch 183\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011414  [    1/   40]\n",
            "loss: 0.008076  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014810 \n",
            "\n",
            "Epoch 184\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012112  [    1/   40]\n",
            "loss: 0.016517  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016205 \n",
            "\n",
            "Epoch 185\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011347  [    1/   40]\n",
            "loss: 0.016644  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015897 \n",
            "\n",
            "Epoch 186\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014172  [    1/   40]\n",
            "loss: 0.010734  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016181 \n",
            "\n",
            "Epoch 187\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018616  [    1/   40]\n",
            "loss: 0.013099  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015043 \n",
            "\n",
            "Epoch 188\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012127  [    1/   40]\n",
            "loss: 0.008306  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015358 \n",
            "\n",
            "Epoch 189\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014585  [    1/   40]\n",
            "loss: 0.008105  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015377 \n",
            "\n",
            "Epoch 190\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013023  [    1/   40]\n",
            "loss: 0.014851  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015684 \n",
            "\n",
            "Epoch 191\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018015  [    1/   40]\n",
            "loss: 0.020137  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015083 \n",
            "\n",
            "Epoch 192\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013638  [    1/   40]\n",
            "loss: 0.014402  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015796 \n",
            "\n",
            "Epoch 193\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019093  [    1/   40]\n",
            "loss: 0.013132  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015359 \n",
            "\n",
            "Epoch 194\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014606  [    1/   40]\n",
            "loss: 0.007832  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015908 \n",
            "\n",
            "Epoch 195\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010139  [    1/   40]\n",
            "loss: 0.014713  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015900 \n",
            "\n",
            "Epoch 196\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006521  [    1/   40]\n",
            "loss: 0.012598  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016039 \n",
            "\n",
            "Epoch 197\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015702  [    1/   40]\n",
            "loss: 0.009204  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015701 \n",
            "\n",
            "Epoch 198\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008051  [    1/   40]\n",
            "loss: 0.006613  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015124 \n",
            "\n",
            "Epoch 199\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011583  [    1/   40]\n",
            "loss: 0.013807  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015624 \n",
            "\n",
            "Epoch 200\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013650  [    1/   40]\n",
            "loss: 0.017866  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014867 \n",
            "\n",
            "Epoch 201\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016714  [    1/   40]\n",
            "loss: 0.013721  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015533 \n",
            "\n",
            "Epoch 202\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008406  [    1/   40]\n",
            "loss: 0.009566  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015150 \n",
            "\n",
            "Epoch 203\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013483  [    1/   40]\n",
            "loss: 0.014502  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014470 \n",
            "\n",
            "Epoch 204\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010547  [    1/   40]\n",
            "loss: 0.010204  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014993 \n",
            "\n",
            "Epoch 205\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009120  [    1/   40]\n",
            "loss: 0.014402  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014900 \n",
            "\n",
            "Epoch 206\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018350  [    1/   40]\n",
            "loss: 0.011412  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014620 \n",
            "\n",
            "Epoch 207\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013643  [    1/   40]\n",
            "loss: 0.006363  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014962 \n",
            "\n",
            "Epoch 208\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009927  [    1/   40]\n",
            "loss: 0.013577  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014956 \n",
            "\n",
            "Epoch 209\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007101  [    1/   40]\n",
            "loss: 0.010158  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016111 \n",
            "\n",
            "Epoch 210\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010493  [    1/   40]\n",
            "loss: 0.016243  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015589 \n",
            "\n",
            "Epoch 211\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013237  [    1/   40]\n",
            "loss: 0.015686  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014598 \n",
            "\n",
            "Epoch 212\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019373  [    1/   40]\n",
            "loss: 0.010230  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014899 \n",
            "\n",
            "Epoch 213\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011953  [    1/   40]\n",
            "loss: 0.012038  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015434 \n",
            "\n",
            "Epoch 214\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012612  [    1/   40]\n",
            "loss: 0.012909  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014759 \n",
            "\n",
            "Epoch 215\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011756  [    1/   40]\n",
            "loss: 0.017382  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014714 \n",
            "\n",
            "Epoch 216\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006927  [    1/   40]\n",
            "loss: 0.013666  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014758 \n",
            "\n",
            "Epoch 217\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009852  [    1/   40]\n",
            "loss: 0.006439  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014952 \n",
            "\n",
            "Epoch 218\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006428  [    1/   40]\n",
            "loss: 0.012224  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015077 \n",
            "\n",
            "Epoch 219\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009171  [    1/   40]\n",
            "loss: 0.017706  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014703 \n",
            "\n",
            "Epoch 220\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012460  [    1/   40]\n",
            "loss: 0.017179  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014667 \n",
            "\n",
            "Epoch 221\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009515  [    1/   40]\n",
            "loss: 0.007430  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014704 \n",
            "\n",
            "Epoch 222\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009217  [    1/   40]\n",
            "loss: 0.017966  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014805 \n",
            "\n",
            "Epoch 223\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013418  [    1/   40]\n",
            "loss: 0.013016  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014777 \n",
            "\n",
            "Epoch 224\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013649  [    1/   40]\n",
            "loss: 0.016432  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014631 \n",
            "\n",
            "Epoch 225\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015188  [    1/   40]\n",
            "loss: 0.012181  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014893 \n",
            "\n",
            "Epoch 226\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007576  [    1/   40]\n",
            "loss: 0.007771  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014156 \n",
            "\n",
            "Epoch 227\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011508  [    1/   40]\n",
            "loss: 0.008409  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014580 \n",
            "\n",
            "Epoch 228\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014975  [    1/   40]\n",
            "loss: 0.007673  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014638 \n",
            "\n",
            "Epoch 229\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010017  [    1/   40]\n",
            "loss: 0.015569  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015037 \n",
            "\n",
            "Epoch 230\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007089  [    1/   40]\n",
            "loss: 0.006822  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014796 \n",
            "\n",
            "Epoch 231\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008123  [    1/   40]\n",
            "loss: 0.011688  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014654 \n",
            "\n",
            "Epoch 232\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013381  [    1/   40]\n",
            "loss: 0.007253  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014421 \n",
            "\n",
            "Epoch 233\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.019002  [    1/   40]\n",
            "loss: 0.006760  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014662 \n",
            "\n",
            "Epoch 234\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015079  [    1/   40]\n",
            "loss: 0.013984  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014807 \n",
            "\n",
            "Epoch 235\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014800  [    1/   40]\n",
            "loss: 0.014949  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014993 \n",
            "\n",
            "Epoch 236\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.018219  [    1/   40]\n",
            "loss: 0.007051  [   21/   40]\n",
            "Test Error:  Avg loss: 0.016187 \n",
            "\n",
            "Epoch 237\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016024  [    1/   40]\n",
            "loss: 0.009733  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014426 \n",
            "\n",
            "Epoch 238\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009261  [    1/   40]\n",
            "loss: 0.016617  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015215 \n",
            "\n",
            "Epoch 239\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016892  [    1/   40]\n",
            "loss: 0.012801  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014019 \n",
            "\n",
            "Epoch 240\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007252  [    1/   40]\n",
            "loss: 0.015455  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014866 \n",
            "\n",
            "Epoch 241\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012236  [    1/   40]\n",
            "loss: 0.007116  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014293 \n",
            "\n",
            "Epoch 242\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011996  [    1/   40]\n",
            "loss: 0.018510  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015396 \n",
            "\n",
            "Epoch 243\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009911  [    1/   40]\n",
            "loss: 0.011894  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015298 \n",
            "\n",
            "Epoch 244\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014655  [    1/   40]\n",
            "loss: 0.012890  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014649 \n",
            "\n",
            "Epoch 245\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011127  [    1/   40]\n",
            "loss: 0.019891  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014856 \n",
            "\n",
            "Epoch 246\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009714  [    1/   40]\n",
            "loss: 0.009659  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014151 \n",
            "\n",
            "Epoch 247\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008078  [    1/   40]\n",
            "loss: 0.006568  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014517 \n",
            "\n",
            "Epoch 248\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011124  [    1/   40]\n",
            "loss: 0.009813  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014914 \n",
            "\n",
            "Epoch 249\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009719  [    1/   40]\n",
            "loss: 0.006939  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014436 \n",
            "\n",
            "Epoch 250\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015184  [    1/   40]\n",
            "loss: 0.007049  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014610 \n",
            "\n",
            "Epoch 251\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006108  [    1/   40]\n",
            "loss: 0.012442  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014581 \n",
            "\n",
            "Epoch 252\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009908  [    1/   40]\n",
            "loss: 0.010630  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014567 \n",
            "\n",
            "Epoch 253\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006819  [    1/   40]\n",
            "loss: 0.006908  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014532 \n",
            "\n",
            "Epoch 254\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010985  [    1/   40]\n",
            "loss: 0.014068  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014158 \n",
            "\n",
            "Epoch 255\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007561  [    1/   40]\n",
            "loss: 0.007542  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015337 \n",
            "\n",
            "Epoch 256\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012856  [    1/   40]\n",
            "loss: 0.011318  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014988 \n",
            "\n",
            "Epoch 257\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012954  [    1/   40]\n",
            "loss: 0.011125  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014744 \n",
            "\n",
            "Epoch 258\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005632  [    1/   40]\n",
            "loss: 0.011291  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014352 \n",
            "\n",
            "Epoch 259\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008678  [    1/   40]\n",
            "loss: 0.009173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014115 \n",
            "\n",
            "Epoch 260\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012268  [    1/   40]\n",
            "loss: 0.013417  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014231 \n",
            "\n",
            "Epoch 261\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008240  [    1/   40]\n",
            "loss: 0.011985  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014871 \n",
            "\n",
            "Epoch 262\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005806  [    1/   40]\n",
            "loss: 0.008973  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015114 \n",
            "\n",
            "Epoch 263\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017101  [    1/   40]\n",
            "loss: 0.009169  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014684 \n",
            "\n",
            "Epoch 264\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006569  [    1/   40]\n",
            "loss: 0.006977  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014547 \n",
            "\n",
            "Epoch 265\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013222  [    1/   40]\n",
            "loss: 0.011807  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014605 \n",
            "\n",
            "Epoch 266\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012810  [    1/   40]\n",
            "loss: 0.007535  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014219 \n",
            "\n",
            "Epoch 267\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009147  [    1/   40]\n",
            "loss: 0.009548  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014730 \n",
            "\n",
            "Epoch 268\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011239  [    1/   40]\n",
            "loss: 0.006673  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014330 \n",
            "\n",
            "Epoch 269\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013226  [    1/   40]\n",
            "loss: 0.005374  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014287 \n",
            "\n",
            "Epoch 270\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015942  [    1/   40]\n",
            "loss: 0.007798  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014658 \n",
            "\n",
            "Epoch 271\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006692  [    1/   40]\n",
            "loss: 0.007186  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014259 \n",
            "\n",
            "Epoch 272\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009728  [    1/   40]\n",
            "loss: 0.007395  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015128 \n",
            "\n",
            "Epoch 273\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010754  [    1/   40]\n",
            "loss: 0.009150  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014611 \n",
            "\n",
            "Epoch 274\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010183  [    1/   40]\n",
            "loss: 0.018631  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013517 \n",
            "\n",
            "Epoch 275\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015351  [    1/   40]\n",
            "loss: 0.007980  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014501 \n",
            "\n",
            "Epoch 276\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006817  [    1/   40]\n",
            "loss: 0.013378  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014822 \n",
            "\n",
            "Epoch 277\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015686  [    1/   40]\n",
            "loss: 0.011246  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014618 \n",
            "\n",
            "Epoch 278\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006714  [    1/   40]\n",
            "loss: 0.007111  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013938 \n",
            "\n",
            "Epoch 279\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014455  [    1/   40]\n",
            "loss: 0.010716  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014581 \n",
            "\n",
            "Epoch 280\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011153  [    1/   40]\n",
            "loss: 0.010784  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015137 \n",
            "\n",
            "Epoch 281\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010952  [    1/   40]\n",
            "loss: 0.012624  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014076 \n",
            "\n",
            "Epoch 282\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012520  [    1/   40]\n",
            "loss: 0.010688  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014812 \n",
            "\n",
            "Epoch 283\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011967  [    1/   40]\n",
            "loss: 0.010251  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013927 \n",
            "\n",
            "Epoch 284\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005753  [    1/   40]\n",
            "loss: 0.011081  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014540 \n",
            "\n",
            "Epoch 285\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009383  [    1/   40]\n",
            "loss: 0.008384  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014264 \n",
            "\n",
            "Epoch 286\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017083  [    1/   40]\n",
            "loss: 0.011083  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014790 \n",
            "\n",
            "Epoch 287\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010892  [    1/   40]\n",
            "loss: 0.006287  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014781 \n",
            "\n",
            "Epoch 288\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007906  [    1/   40]\n",
            "loss: 0.006306  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013766 \n",
            "\n",
            "Epoch 289\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012574  [    1/   40]\n",
            "loss: 0.010067  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014766 \n",
            "\n",
            "Epoch 290\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008240  [    1/   40]\n",
            "loss: 0.009271  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014301 \n",
            "\n",
            "Epoch 291\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012946  [    1/   40]\n",
            "loss: 0.009152  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014424 \n",
            "\n",
            "Epoch 292\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011262  [    1/   40]\n",
            "loss: 0.010105  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014876 \n",
            "\n",
            "Epoch 293\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010988  [    1/   40]\n",
            "loss: 0.006123  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014196 \n",
            "\n",
            "Epoch 294\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007198  [    1/   40]\n",
            "loss: 0.017170  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014315 \n",
            "\n",
            "Epoch 295\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005990  [    1/   40]\n",
            "loss: 0.009956  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013904 \n",
            "\n",
            "Epoch 296\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012771  [    1/   40]\n",
            "loss: 0.012192  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014000 \n",
            "\n",
            "Epoch 297\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015426  [    1/   40]\n",
            "loss: 0.015207  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014845 \n",
            "\n",
            "Epoch 298\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006188  [    1/   40]\n",
            "loss: 0.011601  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014030 \n",
            "\n",
            "Epoch 299\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009933  [    1/   40]\n",
            "loss: 0.012212  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013871 \n",
            "\n",
            "Epoch 300\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010946  [    1/   40]\n",
            "loss: 0.009348  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014512 \n",
            "\n",
            "Epoch 301\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.017366  [    1/   40]\n",
            "loss: 0.010008  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014841 \n",
            "\n",
            "Epoch 302\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010102  [    1/   40]\n",
            "loss: 0.017996  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014359 \n",
            "\n",
            "Epoch 303\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006922  [    1/   40]\n",
            "loss: 0.007329  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013854 \n",
            "\n",
            "Epoch 304\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005894  [    1/   40]\n",
            "loss: 0.010473  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014235 \n",
            "\n",
            "Epoch 305\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011401  [    1/   40]\n",
            "loss: 0.005564  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013940 \n",
            "\n",
            "Epoch 306\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015100  [    1/   40]\n",
            "loss: 0.009140  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014091 \n",
            "\n",
            "Epoch 307\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013201  [    1/   40]\n",
            "loss: 0.006992  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014025 \n",
            "\n",
            "Epoch 308\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008971  [    1/   40]\n",
            "loss: 0.009800  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014592 \n",
            "\n",
            "Epoch 309\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009505  [    1/   40]\n",
            "loss: 0.012154  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014193 \n",
            "\n",
            "Epoch 310\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008342  [    1/   40]\n",
            "loss: 0.009000  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014398 \n",
            "\n",
            "Epoch 311\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008344  [    1/   40]\n",
            "loss: 0.007298  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013869 \n",
            "\n",
            "Epoch 312\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006280  [    1/   40]\n",
            "loss: 0.012744  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014069 \n",
            "\n",
            "Epoch 313\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005326  [    1/   40]\n",
            "loss: 0.009936  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015378 \n",
            "\n",
            "Epoch 314\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014535  [    1/   40]\n",
            "loss: 0.010678  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014054 \n",
            "\n",
            "Epoch 315\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011910  [    1/   40]\n",
            "loss: 0.011783  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014341 \n",
            "\n",
            "Epoch 316\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008688  [    1/   40]\n",
            "loss: 0.014218  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013843 \n",
            "\n",
            "Epoch 317\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006254  [    1/   40]\n",
            "loss: 0.017317  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013511 \n",
            "\n",
            "Epoch 318\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007610  [    1/   40]\n",
            "loss: 0.015494  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013848 \n",
            "\n",
            "Epoch 319\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008285  [    1/   40]\n",
            "loss: 0.006166  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013681 \n",
            "\n",
            "Epoch 320\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008081  [    1/   40]\n",
            "loss: 0.011735  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013789 \n",
            "\n",
            "Epoch 321\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011649  [    1/   40]\n",
            "loss: 0.007173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014855 \n",
            "\n",
            "Epoch 322\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010179  [    1/   40]\n",
            "loss: 0.017099  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013474 \n",
            "\n",
            "Epoch 323\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012899  [    1/   40]\n",
            "loss: 0.006725  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014234 \n",
            "\n",
            "Epoch 324\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006280  [    1/   40]\n",
            "loss: 0.008376  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013994 \n",
            "\n",
            "Epoch 325\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012382  [    1/   40]\n",
            "loss: 0.010064  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014368 \n",
            "\n",
            "Epoch 326\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009603  [    1/   40]\n",
            "loss: 0.006107  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013959 \n",
            "\n",
            "Epoch 327\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011916  [    1/   40]\n",
            "loss: 0.017077  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014392 \n",
            "\n",
            "Epoch 328\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014832  [    1/   40]\n",
            "loss: 0.016724  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014255 \n",
            "\n",
            "Epoch 329\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011662  [    1/   40]\n",
            "loss: 0.007743  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013535 \n",
            "\n",
            "Epoch 330\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013067  [    1/   40]\n",
            "loss: 0.006453  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014673 \n",
            "\n",
            "Epoch 331\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016628  [    1/   40]\n",
            "loss: 0.010224  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014115 \n",
            "\n",
            "Epoch 332\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.004575  [    1/   40]\n",
            "loss: 0.010532  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013880 \n",
            "\n",
            "Epoch 333\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007026  [    1/   40]\n",
            "loss: 0.011911  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013774 \n",
            "\n",
            "Epoch 334\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005511  [    1/   40]\n",
            "loss: 0.009524  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014783 \n",
            "\n",
            "Epoch 335\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015718  [    1/   40]\n",
            "loss: 0.007103  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014102 \n",
            "\n",
            "Epoch 336\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008686  [    1/   40]\n",
            "loss: 0.010838  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014535 \n",
            "\n",
            "Epoch 337\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006195  [    1/   40]\n",
            "loss: 0.005164  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014344 \n",
            "\n",
            "Epoch 338\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005356  [    1/   40]\n",
            "loss: 0.012038  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013818 \n",
            "\n",
            "Epoch 339\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013128  [    1/   40]\n",
            "loss: 0.008695  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014411 \n",
            "\n",
            "Epoch 340\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010459  [    1/   40]\n",
            "loss: 0.012660  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013778 \n",
            "\n",
            "Epoch 341\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010424  [    1/   40]\n",
            "loss: 0.005659  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014242 \n",
            "\n",
            "Epoch 342\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008385  [    1/   40]\n",
            "loss: 0.010078  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014665 \n",
            "\n",
            "Epoch 343\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009319  [    1/   40]\n",
            "loss: 0.012173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014013 \n",
            "\n",
            "Epoch 344\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014065  [    1/   40]\n",
            "loss: 0.012517  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014339 \n",
            "\n",
            "Epoch 345\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009077  [    1/   40]\n",
            "loss: 0.006563  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013703 \n",
            "\n",
            "Epoch 346\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006531  [    1/   40]\n",
            "loss: 0.010892  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014061 \n",
            "\n",
            "Epoch 347\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012766  [    1/   40]\n",
            "loss: 0.009860  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014415 \n",
            "\n",
            "Epoch 348\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014770  [    1/   40]\n",
            "loss: 0.009548  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014969 \n",
            "\n",
            "Epoch 349\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012276  [    1/   40]\n",
            "loss: 0.013809  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014124 \n",
            "\n",
            "Epoch 350\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009267  [    1/   40]\n",
            "loss: 0.005649  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014412 \n",
            "\n",
            "Epoch 351\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010375  [    1/   40]\n",
            "loss: 0.013309  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013940 \n",
            "\n",
            "Epoch 352\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011268  [    1/   40]\n",
            "loss: 0.009983  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014220 \n",
            "\n",
            "Epoch 353\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013678  [    1/   40]\n",
            "loss: 0.009903  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014281 \n",
            "\n",
            "Epoch 354\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010719  [    1/   40]\n",
            "loss: 0.011949  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014692 \n",
            "\n",
            "Epoch 355\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005991  [    1/   40]\n",
            "loss: 0.012254  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014391 \n",
            "\n",
            "Epoch 356\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010720  [    1/   40]\n",
            "loss: 0.012554  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014351 \n",
            "\n",
            "Epoch 357\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009254  [    1/   40]\n",
            "loss: 0.006116  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014212 \n",
            "\n",
            "Epoch 358\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016032  [    1/   40]\n",
            "loss: 0.009892  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014001 \n",
            "\n",
            "Epoch 359\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005737  [    1/   40]\n",
            "loss: 0.008525  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014109 \n",
            "\n",
            "Epoch 360\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010405  [    1/   40]\n",
            "loss: 0.009044  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014413 \n",
            "\n",
            "Epoch 361\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007795  [    1/   40]\n",
            "loss: 0.010800  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013858 \n",
            "\n",
            "Epoch 362\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010301  [    1/   40]\n",
            "loss: 0.004724  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014170 \n",
            "\n",
            "Epoch 363\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011987  [    1/   40]\n",
            "loss: 0.006807  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014011 \n",
            "\n",
            "Epoch 364\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010701  [    1/   40]\n",
            "loss: 0.011837  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014396 \n",
            "\n",
            "Epoch 365\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010192  [    1/   40]\n",
            "loss: 0.011661  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014687 \n",
            "\n",
            "Epoch 366\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009944  [    1/   40]\n",
            "loss: 0.011849  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014845 \n",
            "\n",
            "Epoch 367\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012685  [    1/   40]\n",
            "loss: 0.006698  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013834 \n",
            "\n",
            "Epoch 368\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009338  [    1/   40]\n",
            "loss: 0.009160  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014006 \n",
            "\n",
            "Epoch 369\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006473  [    1/   40]\n",
            "loss: 0.005511  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014459 \n",
            "\n",
            "Epoch 370\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011128  [    1/   40]\n",
            "loss: 0.012290  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013998 \n",
            "\n",
            "Epoch 371\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010660  [    1/   40]\n",
            "loss: 0.013380  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013299 \n",
            "\n",
            "Epoch 372\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007382  [    1/   40]\n",
            "loss: 0.008965  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014277 \n",
            "\n",
            "Epoch 373\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005484  [    1/   40]\n",
            "loss: 0.011833  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013655 \n",
            "\n",
            "Epoch 374\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008249  [    1/   40]\n",
            "loss: 0.009280  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014511 \n",
            "\n",
            "Epoch 375\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.016595  [    1/   40]\n",
            "loss: 0.014597  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014545 \n",
            "\n",
            "Epoch 376\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006627  [    1/   40]\n",
            "loss: 0.013337  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013961 \n",
            "\n",
            "Epoch 377\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013281  [    1/   40]\n",
            "loss: 0.009442  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013872 \n",
            "\n",
            "Epoch 378\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011780  [    1/   40]\n",
            "loss: 0.006880  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014514 \n",
            "\n",
            "Epoch 379\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007058  [    1/   40]\n",
            "loss: 0.007984  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014339 \n",
            "\n",
            "Epoch 380\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012511  [    1/   40]\n",
            "loss: 0.005324  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014387 \n",
            "\n",
            "Epoch 381\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006912  [    1/   40]\n",
            "loss: 0.008184  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014113 \n",
            "\n",
            "Epoch 382\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010438  [    1/   40]\n",
            "loss: 0.009615  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014862 \n",
            "\n",
            "Epoch 383\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005971  [    1/   40]\n",
            "loss: 0.009064  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014279 \n",
            "\n",
            "Epoch 384\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007286  [    1/   40]\n",
            "loss: 0.010876  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013884 \n",
            "\n",
            "Epoch 385\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009499  [    1/   40]\n",
            "loss: 0.010878  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015033 \n",
            "\n",
            "Epoch 386\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014200  [    1/   40]\n",
            "loss: 0.008023  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014376 \n",
            "\n",
            "Epoch 387\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009904  [    1/   40]\n",
            "loss: 0.013165  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014127 \n",
            "\n",
            "Epoch 388\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009352  [    1/   40]\n",
            "loss: 0.016002  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014661 \n",
            "\n",
            "Epoch 389\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011173  [    1/   40]\n",
            "loss: 0.010853  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014079 \n",
            "\n",
            "Epoch 390\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010896  [    1/   40]\n",
            "loss: 0.014419  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014725 \n",
            "\n",
            "Epoch 391\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009672  [    1/   40]\n",
            "loss: 0.006087  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014435 \n",
            "\n",
            "Epoch 392\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006454  [    1/   40]\n",
            "loss: 0.006240  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014592 \n",
            "\n",
            "Epoch 393\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008623  [    1/   40]\n",
            "loss: 0.007726  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014476 \n",
            "\n",
            "Epoch 394\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011219  [    1/   40]\n",
            "loss: 0.006278  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013736 \n",
            "\n",
            "Epoch 395\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008678  [    1/   40]\n",
            "loss: 0.010813  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014266 \n",
            "\n",
            "Epoch 396\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009294  [    1/   40]\n",
            "loss: 0.011016  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013905 \n",
            "\n",
            "Epoch 397\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012266  [    1/   40]\n",
            "loss: 0.006610  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014490 \n",
            "\n",
            "Epoch 398\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013548  [    1/   40]\n",
            "loss: 0.011419  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014236 \n",
            "\n",
            "Epoch 399\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006607  [    1/   40]\n",
            "loss: 0.008681  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014063 \n",
            "\n",
            "Epoch 400\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015690  [    1/   40]\n",
            "loss: 0.008080  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014144 \n",
            "\n",
            "Epoch 401\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014460  [    1/   40]\n",
            "loss: 0.007313  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014008 \n",
            "\n",
            "Epoch 402\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015149  [    1/   40]\n",
            "loss: 0.012646  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013857 \n",
            "\n",
            "Epoch 403\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006667  [    1/   40]\n",
            "loss: 0.011916  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014091 \n",
            "\n",
            "Epoch 404\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008304  [    1/   40]\n",
            "loss: 0.006484  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014201 \n",
            "\n",
            "Epoch 405\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005432  [    1/   40]\n",
            "loss: 0.007008  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013777 \n",
            "\n",
            "Epoch 406\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010221  [    1/   40]\n",
            "loss: 0.009381  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013844 \n",
            "\n",
            "Epoch 407\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011769  [    1/   40]\n",
            "loss: 0.015225  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014500 \n",
            "\n",
            "Epoch 408\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.015445  [    1/   40]\n",
            "loss: 0.011024  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014329 \n",
            "\n",
            "Epoch 409\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007705  [    1/   40]\n",
            "loss: 0.005103  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014613 \n",
            "\n",
            "Epoch 410\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008974  [    1/   40]\n",
            "loss: 0.011383  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014774 \n",
            "\n",
            "Epoch 411\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011206  [    1/   40]\n",
            "loss: 0.008029  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013984 \n",
            "\n",
            "Epoch 412\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011665  [    1/   40]\n",
            "loss: 0.005954  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014135 \n",
            "\n",
            "Epoch 413\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009908  [    1/   40]\n",
            "loss: 0.008706  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014060 \n",
            "\n",
            "Epoch 414\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.004908  [    1/   40]\n",
            "loss: 0.013185  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013912 \n",
            "\n",
            "Epoch 415\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008873  [    1/   40]\n",
            "loss: 0.014646  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014053 \n",
            "\n",
            "Epoch 416\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006900  [    1/   40]\n",
            "loss: 0.011290  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014827 \n",
            "\n",
            "Epoch 417\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012137  [    1/   40]\n",
            "loss: 0.011964  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013913 \n",
            "\n",
            "Epoch 418\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013312  [    1/   40]\n",
            "loss: 0.008710  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014221 \n",
            "\n",
            "Epoch 419\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005369  [    1/   40]\n",
            "loss: 0.009360  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014289 \n",
            "\n",
            "Epoch 420\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005591  [    1/   40]\n",
            "loss: 0.012287  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013813 \n",
            "\n",
            "Epoch 421\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011077  [    1/   40]\n",
            "loss: 0.005716  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014301 \n",
            "\n",
            "Epoch 422\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006363  [    1/   40]\n",
            "loss: 0.008952  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014221 \n",
            "\n",
            "Epoch 423\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006150  [    1/   40]\n",
            "loss: 0.013731  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014371 \n",
            "\n",
            "Epoch 424\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008958  [    1/   40]\n",
            "loss: 0.009037  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014562 \n",
            "\n",
            "Epoch 425\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011863  [    1/   40]\n",
            "loss: 0.008148  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013598 \n",
            "\n",
            "Epoch 426\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005724  [    1/   40]\n",
            "loss: 0.011346  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014267 \n",
            "\n",
            "Epoch 427\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009402  [    1/   40]\n",
            "loss: 0.012508  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014196 \n",
            "\n",
            "Epoch 428\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014071  [    1/   40]\n",
            "loss: 0.008645  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014620 \n",
            "\n",
            "Epoch 429\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011009  [    1/   40]\n",
            "loss: 0.005025  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014052 \n",
            "\n",
            "Epoch 430\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010976  [    1/   40]\n",
            "loss: 0.005496  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014347 \n",
            "\n",
            "Epoch 431\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006977  [    1/   40]\n",
            "loss: 0.011866  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013610 \n",
            "\n",
            "Epoch 432\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011368  [    1/   40]\n",
            "loss: 0.007208  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014356 \n",
            "\n",
            "Epoch 433\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008341  [    1/   40]\n",
            "loss: 0.005652  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014500 \n",
            "\n",
            "Epoch 434\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012846  [    1/   40]\n",
            "loss: 0.008504  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014036 \n",
            "\n",
            "Epoch 435\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008073  [    1/   40]\n",
            "loss: 0.010636  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013950 \n",
            "\n",
            "Epoch 436\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.014996  [    1/   40]\n",
            "loss: 0.008852  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014065 \n",
            "\n",
            "Epoch 437\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007498  [    1/   40]\n",
            "loss: 0.008853  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014150 \n",
            "\n",
            "Epoch 438\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006100  [    1/   40]\n",
            "loss: 0.013659  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013480 \n",
            "\n",
            "Epoch 439\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011188  [    1/   40]\n",
            "loss: 0.008351  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014860 \n",
            "\n",
            "Epoch 440\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005831  [    1/   40]\n",
            "loss: 0.008581  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014412 \n",
            "\n",
            "Epoch 441\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005814  [    1/   40]\n",
            "loss: 0.009156  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014662 \n",
            "\n",
            "Epoch 442\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010078  [    1/   40]\n",
            "loss: 0.006212  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014210 \n",
            "\n",
            "Epoch 443\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009116  [    1/   40]\n",
            "loss: 0.008062  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013864 \n",
            "\n",
            "Epoch 444\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008938  [    1/   40]\n",
            "loss: 0.007689  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014078 \n",
            "\n",
            "Epoch 445\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012296  [    1/   40]\n",
            "loss: 0.005722  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013717 \n",
            "\n",
            "Epoch 446\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009337  [    1/   40]\n",
            "loss: 0.014224  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014227 \n",
            "\n",
            "Epoch 447\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009944  [    1/   40]\n",
            "loss: 0.004543  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014421 \n",
            "\n",
            "Epoch 448\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005820  [    1/   40]\n",
            "loss: 0.010444  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014397 \n",
            "\n",
            "Epoch 449\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008544  [    1/   40]\n",
            "loss: 0.008660  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013759 \n",
            "\n",
            "Epoch 450\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007879  [    1/   40]\n",
            "loss: 0.010721  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013769 \n",
            "\n",
            "Epoch 451\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008438  [    1/   40]\n",
            "loss: 0.005659  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014459 \n",
            "\n",
            "Epoch 452\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009146  [    1/   40]\n",
            "loss: 0.007995  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014781 \n",
            "\n",
            "Epoch 453\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010454  [    1/   40]\n",
            "loss: 0.010026  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014419 \n",
            "\n",
            "Epoch 454\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005668  [    1/   40]\n",
            "loss: 0.004883  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013952 \n",
            "\n",
            "Epoch 455\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011205  [    1/   40]\n",
            "loss: 0.010688  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014059 \n",
            "\n",
            "Epoch 456\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011665  [    1/   40]\n",
            "loss: 0.010224  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014945 \n",
            "\n",
            "Epoch 457\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011910  [    1/   40]\n",
            "loss: 0.007742  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014519 \n",
            "\n",
            "Epoch 458\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006149  [    1/   40]\n",
            "loss: 0.011194  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014494 \n",
            "\n",
            "Epoch 459\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006541  [    1/   40]\n",
            "loss: 0.009097  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014254 \n",
            "\n",
            "Epoch 460\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009520  [    1/   40]\n",
            "loss: 0.004189  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014646 \n",
            "\n",
            "Epoch 461\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011493  [    1/   40]\n",
            "loss: 0.012010  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014151 \n",
            "\n",
            "Epoch 462\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005746  [    1/   40]\n",
            "loss: 0.010957  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014377 \n",
            "\n",
            "Epoch 463\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009468  [    1/   40]\n",
            "loss: 0.005722  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014146 \n",
            "\n",
            "Epoch 464\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008158  [    1/   40]\n",
            "loss: 0.008293  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013916 \n",
            "\n",
            "Epoch 465\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010163  [    1/   40]\n",
            "loss: 0.007856  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013877 \n",
            "\n",
            "Epoch 466\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005847  [    1/   40]\n",
            "loss: 0.008335  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014140 \n",
            "\n",
            "Epoch 467\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006087  [    1/   40]\n",
            "loss: 0.006755  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013911 \n",
            "\n",
            "Epoch 468\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006587  [    1/   40]\n",
            "loss: 0.011671  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014671 \n",
            "\n",
            "Epoch 469\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011565  [    1/   40]\n",
            "loss: 0.007019  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013997 \n",
            "\n",
            "Epoch 470\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007308  [    1/   40]\n",
            "loss: 0.011343  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015244 \n",
            "\n",
            "Epoch 471\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008700  [    1/   40]\n",
            "loss: 0.012265  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014029 \n",
            "\n",
            "Epoch 472\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012577  [    1/   40]\n",
            "loss: 0.008173  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013501 \n",
            "\n",
            "Epoch 473\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.010387  [    1/   40]\n",
            "loss: 0.009333  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014668 \n",
            "\n",
            "Epoch 474\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.004977  [    1/   40]\n",
            "loss: 0.005252  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014561 \n",
            "\n",
            "Epoch 475\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006326  [    1/   40]\n",
            "loss: 0.010918  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014054 \n",
            "\n",
            "Epoch 476\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011142  [    1/   40]\n",
            "loss: 0.007545  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014059 \n",
            "\n",
            "Epoch 477\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008301  [    1/   40]\n",
            "loss: 0.009263  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015184 \n",
            "\n",
            "Epoch 478\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005275  [    1/   40]\n",
            "loss: 0.005858  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014316 \n",
            "\n",
            "Epoch 479\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007914  [    1/   40]\n",
            "loss: 0.011501  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013635 \n",
            "\n",
            "Epoch 480\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008390  [    1/   40]\n",
            "loss: 0.010853  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014191 \n",
            "\n",
            "Epoch 481\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005000  [    1/   40]\n",
            "loss: 0.010883  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014681 \n",
            "\n",
            "Epoch 482\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.007745  [    1/   40]\n",
            "loss: 0.005199  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014534 \n",
            "\n",
            "Epoch 483\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008942  [    1/   40]\n",
            "loss: 0.008448  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014946 \n",
            "\n",
            "Epoch 484\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.006630  [    1/   40]\n",
            "loss: 0.005897  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014617 \n",
            "\n",
            "Epoch 485\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011141  [    1/   40]\n",
            "loss: 0.009592  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014780 \n",
            "\n",
            "Epoch 486\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011164  [    1/   40]\n",
            "loss: 0.010184  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014640 \n",
            "\n",
            "Epoch 487\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.013913  [    1/   40]\n",
            "loss: 0.008632  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013913 \n",
            "\n",
            "Epoch 488\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012890  [    1/   40]\n",
            "loss: 0.007817  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014497 \n",
            "\n",
            "Epoch 489\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.009053  [    1/   40]\n",
            "loss: 0.007251  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013940 \n",
            "\n",
            "Epoch 490\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005796  [    1/   40]\n",
            "loss: 0.005877  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014146 \n",
            "\n",
            "Epoch 491\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008793  [    1/   40]\n",
            "loss: 0.005875  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014048 \n",
            "\n",
            "Epoch 492\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.004956  [    1/   40]\n",
            "loss: 0.010138  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014004 \n",
            "\n",
            "Epoch 493\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005432  [    1/   40]\n",
            "loss: 0.011971  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014388 \n",
            "\n",
            "Epoch 494\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008160  [    1/   40]\n",
            "loss: 0.005996  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014461 \n",
            "\n",
            "Epoch 495\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008567  [    1/   40]\n",
            "loss: 0.008065  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014333 \n",
            "\n",
            "Epoch 496\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.012480  [    1/   40]\n",
            "loss: 0.006359  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014348 \n",
            "\n",
            "Epoch 497\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.011810  [    1/   40]\n",
            "loss: 0.007211  [   21/   40]\n",
            "Test Error:  Avg loss: 0.013888 \n",
            "\n",
            "Epoch 498\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005953  [    1/   40]\n",
            "loss: 0.006656  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014320 \n",
            "\n",
            "Epoch 499\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.008147  [    1/   40]\n",
            "loss: 0.009919  [   21/   40]\n",
            "Test Error:  Avg loss: 0.015078 \n",
            "\n",
            "Epoch 500\n",
            "-------------------------------\n",
            "40\n",
            "loss: 0.005747  [    1/   40]\n",
            "loss: 0.006554  [   21/   40]\n",
            "Test Error:  Avg loss: 0.014190 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001,momentum=0.9)\n",
        "\n",
        "epochs = 500\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(loader, loss_fn, optimizer)\n",
        "    val_loop(val_loader, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh4tsrGLuTEy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "torch.save(model.state_dict(), 'saved_data_model.pt')\n",
        "torch.save(optimizer.state_dict(), 'saved_data_optimizer.pt')\n",
        "\n",
        "\n",
        "model = Autoencoder()\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "#optimizer = torch.optim.Adam(params_to_optimize, lr=0.0001, weight_decay=1e-05)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "model.load_state_dict(torch.load('saved_data_model.pt'))\n",
        "optimizer.load_state_dict(torch.load('saved_data_optimizer.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfKln-1tup6v",
        "outputId": "90a61e5a-ffa4-4363-8190-48f57370e7a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "x tensor([[[-2.6127e-01, -4.6763e-01,  6.2371e-02,  5.2001e-01, -7.0590e-03,\n",
            "          -5.2496e-01,  1.4165e-01, -5.2515e-01,  2.6908e-02,  3.4106e-03,\n",
            "          -5.3161e-01,  5.9588e-01,  6.6905e-02,  3.2951e-01,  4.4232e-01,\n",
            "          -2.1317e-02, -1.8266e-01,  4.8799e-01,  2.1765e-02, -2.6534e-02,\n",
            "           1.9717e-01, -4.2183e-01, -1.4312e-02,  4.1642e-01,  2.6945e-01,\n",
            "           6.4946e-01,  2.3988e-01,  4.8178e-02, -2.6797e-02, -3.7925e-01,\n",
            "          -5.7487e-01, -2.4094e-01, -2.5117e-01,  7.0525e-01,  1.3802e-01,\n",
            "           3.7031e-01, -5.4238e-01,  5.7693e-01,  5.1506e-01,  4.7791e-01,\n",
            "           2.0435e-02,  3.5112e-02,  7.6267e-02,  6.1888e-01, -2.5576e-02,\n",
            "          -4.8947e-01, -4.7947e-01, -3.6862e-01, -4.5220e-01,  2.7477e-01,\n",
            "          -2.1003e-01,  7.2061e-02, -2.0308e-02, -3.9764e-01,  4.9790e-01,\n",
            "           5.3874e-02, -1.4719e-02,  5.0923e-01,  1.0956e-01, -4.2560e-01,\n",
            "          -4.1023e-01,  1.6442e-01,  1.7020e-01,  3.6917e-02, -2.5307e-01,\n",
            "           4.4099e-01,  7.4603e-05, -3.9181e-01, -1.8042e-01,  1.0617e-01,\n",
            "           5.3088e-01, -2.0616e-01,  5.6170e-01,  1.7280e-01,  4.5557e-01,\n",
            "          -4.1374e-02, -5.7587e-01,  6.8499e-02, -4.1197e-01, -1.6708e-01,\n",
            "          -1.9162e-01,  4.8799e-01,  3.8270e-01, -4.5628e-01,  1.4194e-01,\n",
            "           7.3972e-02, -1.5844e-01, -2.7551e-01, -4.5341e-01,  3.8624e-01,\n",
            "           1.7708e-01,  6.0247e-02, -3.0290e-02,  6.3880e-02,  1.2618e-02,\n",
            "           4.4718e-01, -2.0038e-01,  4.0293e-01,  5.5030e-01,  4.1853e-01]]])\n",
            "y tensor([[[-0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
            "          -0.4000,  0.2000,  0.0000, -0.4000,  0.5000,  0.0000,  0.3000,\n",
            "           0.5000,  0.0000, -0.2000,  0.6000,  0.0000,  0.0000,  0.2000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,  0.0000,\n",
            "           0.0000, -0.4000, -0.4000, -0.2000, -0.2000,  0.6000,  0.2000,\n",
            "           0.5000, -0.4000,  0.5000,  0.6000,  0.5000,  0.0000,  0.0000,\n",
            "           0.2000,  0.6000,  0.0000, -0.4000, -0.4000, -0.2000, -0.4000,\n",
            "           0.5000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
            "           0.0000,  0.6000,  0.0000, -0.4000, -0.4000,  0.3000,  0.2000,\n",
            "           0.0000, -0.2000,  0.5000,  0.0000, -0.4000, -0.2000,  0.2000,\n",
            "           0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
            "           0.0000, -0.4000, -0.2000, -0.2000,  0.6000,  0.3000, -0.4000,\n",
            "           0.2000,  0.0000, -0.2000, -0.2000, -0.4000,  0.5000,  0.2000,\n",
            "           0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,  0.5000,\n",
            "           0.5000,  0.6000]]])\n",
            "pred tensor([[[-0.0919, -0.5676,  0.2141,  0.3703, -0.0159, -0.4967,  0.1384,\n",
            "          -0.3195,  0.0948,  0.0085, -0.4299,  0.6498,  0.0750,  0.3857,\n",
            "           0.6117, -0.0246, -0.2347,  0.4857,  0.0440, -0.1314,  0.2353,\n",
            "          -0.5114,  0.0949,  0.4259,  0.5351,  0.5233,  0.3036,  0.0444,\n",
            "           0.0075, -0.3666, -0.3673, -0.2141, -0.2376,  0.5350,  0.2181,\n",
            "           0.1399, -0.4078,  0.5700,  0.5273,  0.3818,  0.1374, -0.2512,\n",
            "           0.1908,  0.5175,  0.0909, -0.5089, -0.3431, -0.3722, -0.1392,\n",
            "           0.2092, -0.0700, -0.0129,  0.0190, -0.3537,  0.5889, -0.0222,\n",
            "           0.0482,  0.3785,  0.1271, -0.3284, -0.3138,  0.2488,  0.1045,\n",
            "           0.0568, -0.2404,  0.4796, -0.0149, -0.3277, -0.1494,  0.1947,\n",
            "           0.5383, -0.1697,  0.5220,  0.1401,  0.5145, -0.0579, -0.4653,\n",
            "           0.0600, -0.3315, -0.1786, -0.2328,  0.5447,  0.4786, -0.3109,\n",
            "           0.2184,  0.0690, -0.1903, -0.3779, -0.2954,  0.2700,  0.3478,\n",
            "          -0.0685,  0.0799, -0.1064,  0.1071,  0.3718, -0.1229,  0.2948,\n",
            "           0.7021,  0.4228]]])\n",
            "x tensor([[[ 0.0328, -0.4573, -0.4725, -0.0247,  0.0540,  0.0859,  0.4515,\n",
            "           0.1295, -0.2223, -0.0262,  0.0335, -0.4611,  0.4399, -0.0082,\n",
            "          -0.4945, -0.3633, -0.2324,  0.5773,  0.6022,  0.2374,  0.0691,\n",
            "          -0.0194,  0.4261,  0.1651, -0.2748,  0.5246,  0.0692,  0.4750,\n",
            "           0.1088,  0.0235,  0.1454,  0.3780,  0.3894,  0.4578, -0.4706,\n",
            "           0.5107,  0.1996,  0.4277,  0.6494,  0.1303,  0.5510,  0.4877,\n",
            "           0.4463,  0.4929, -0.0060,  0.0577, -0.4770,  0.0755,  0.0523,\n",
            "          -0.5525,  0.0088,  0.4627,  0.0546,  0.5075,  0.6435,  0.1435,\n",
            "           0.4656, -0.3355,  0.5981,  0.1251,  0.1323,  0.1832, -0.4654,\n",
            "          -0.2253,  0.5101, -0.0036,  0.3210,  0.0282,  0.1331,  0.5667,\n",
            "           0.2723, -0.1965, -0.2652,  0.4898,  0.0672,  0.4451,  0.0650,\n",
            "          -0.0210,  0.0807,  0.0530, -0.5118, -0.4409,  0.1643, -0.2564,\n",
            "          -0.4262,  0.2940, -0.5093,  0.3116, -0.3308, -0.5235,  0.4505,\n",
            "           0.2786,  0.5227,  0.5679, -0.1540, -0.2014,  0.0721, -0.2696,\n",
            "           0.4109,  0.0056]]])\n",
            "y tensor([[[ 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "           0.0000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
            "          -0.4000, -0.4000, -0.2000,  0.6000,  0.6000,  0.3000,  0.0000,\n",
            "           0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,  0.5000,\n",
            "           0.0000,  0.0000,  0.2000,  0.5000,  0.3000,  0.5000, -0.4000,\n",
            "           0.5000,  0.2000,  0.5000,  0.6000,  0.2000,  0.5000,  0.5000,\n",
            "           0.5000,  0.5000,  0.0000,  0.0000, -0.4000,  0.0000,  0.0000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,\n",
            "           0.6000, -0.4000,  0.6000,  0.2000,  0.0000,  0.2000, -0.4000,\n",
            "          -0.2000,  0.6000,  0.0000,  0.3000,  0.0000,  0.2000,  0.6000,\n",
            "           0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000, -0.4000, -0.4000,  0.2000, -0.2000,\n",
            "          -0.4000,  0.3000, -0.4000,  0.3000, -0.2000, -0.4000,  0.5000,\n",
            "           0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000, -0.2000,\n",
            "           0.5000,  0.0000]]])\n",
            "pred tensor([[[ 0.0361, -0.2364, -0.3748, -0.0588,  0.0209,  0.0653,  0.5114,\n",
            "           0.1018, -0.3004, -0.0399, -0.0144, -0.2982,  0.4093,  0.0639,\n",
            "          -0.4247, -0.3688, -0.2127,  0.5454,  0.7060,  0.2659,  0.0246,\n",
            "          -0.0993,  0.4911,  0.1435, -0.1822,  0.3222,  0.1744,  0.4967,\n",
            "           0.2293, -0.1403,  0.1720,  0.3891,  0.3205,  0.4614, -0.3521,\n",
            "           0.5086,  0.2183,  0.5407,  0.6904,  0.1804,  0.7136,  0.3588,\n",
            "           0.4111,  0.3208,  0.0984,  0.0118, -0.4216,  0.0276, -0.0295,\n",
            "          -0.5362,  0.0189,  0.5821,  0.1190,  0.6282,  0.5869,  0.2449,\n",
            "           0.5089, -0.3548,  0.6946,  0.1826,  0.0469,  0.2558, -0.4990,\n",
            "          -0.1953,  0.4184,  0.0737,  0.3552,  0.0538,  0.0338,  0.5325,\n",
            "           0.2728, -0.2547, -0.2124,  0.3286,  0.2524,  0.3935,  0.1587,\n",
            "          -0.1348,  0.0473,  0.0332, -0.3952, -0.5283,  0.2414, -0.1729,\n",
            "          -0.4089,  0.2221, -0.2915,  0.3475, -0.3335, -0.4446,  0.4010,\n",
            "           0.2713,  0.4466,  0.6630, -0.1423, -0.1590,  0.0636, -0.1686,\n",
            "           0.4215,  0.0274]]])\n",
            "x tensor([[[ 1.5040e-01,  2.7753e-02,  2.6380e-01, -4.1143e-01,  5.8431e-01,\n",
            "           3.5105e-01,  3.9801e-02,  6.2555e-02,  4.4214e-01, -4.6540e-01,\n",
            "           1.9462e-01,  5.3590e-02, -4.6464e-01, -3.0317e-01,  1.7399e-02,\n",
            "           1.5962e-03, -2.5415e-01,  3.5795e-01, -3.8097e-01,  4.9359e-01,\n",
            "           4.8813e-01,  4.1093e-01, -2.1811e-01,  8.8324e-02,  2.7562e-01,\n",
            "          -7.1405e-02,  4.5740e-01,  3.1607e-01,  2.5520e-01,  3.7994e-02,\n",
            "           5.7717e-01,  1.2074e-02,  6.7664e-02, -6.1348e-03, -4.8868e-02,\n",
            "          -6.5389e-02, -5.4172e-01,  5.6659e-01,  3.2106e-02,  1.2346e-01,\n",
            "           7.8257e-02,  4.5159e-01,  4.8174e-01,  3.4322e-02, -6.3859e-02,\n",
            "           8.9762e-02, -3.6590e-01,  2.7255e-01,  5.2111e-02, -2.3775e-01,\n",
            "           2.0834e-01, -4.0831e-01,  2.9185e-01, -6.6780e-06,  7.1918e-02,\n",
            "          -3.3397e-02,  4.5936e-01, -2.7687e-01,  2.5574e-01, -5.4087e-03,\n",
            "           1.0142e-01, -4.8906e-01,  2.7102e-02,  6.3380e-02, -2.2070e-01,\n",
            "          -2.2836e-02, -5.0412e-02,  4.2620e-01, -4.2748e-01,  4.2188e-01,\n",
            "          -5.3945e-01, -3.4004e-01, -4.1858e-01, -3.2877e-01, -2.2333e-01,\n",
            "           5.4785e-02,  3.2787e-01,  2.5013e-01,  8.4432e-02,  4.8181e-01,\n",
            "           1.7438e-02, -5.4653e-01,  4.1048e-02, -4.2961e-01,  3.5564e-01,\n",
            "           3.8394e-03,  1.0678e-01,  5.4504e-02, -4.8490e-01,  3.6136e-02,\n",
            "           4.7570e-01,  1.6878e-01,  4.7856e-01,  2.5320e-02,  6.2321e-02,\n",
            "           2.8568e-02, -5.0877e-01, -2.3411e-01,  2.7215e-01, -4.1985e-01]]])\n",
            "y tensor([[[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
            "           0.0000,  0.5000, -0.4000,  0.2000,  0.0000, -0.4000, -0.2000,\n",
            "           0.0000,  0.0000, -0.2000,  0.5000, -0.4000,  0.5000,  0.5000,\n",
            "           0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,  0.3000,\n",
            "           0.3000,  0.0000,  0.6000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000, -0.4000,  0.6000,  0.0000,  0.2000,  0.2000,  0.5000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000, -0.4000,  0.3000,  0.0000,\n",
            "          -0.2000,  0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
            "           0.5000, -0.2000,  0.2000,  0.0000,  0.2000, -0.4000,  0.0000,\n",
            "           0.0000, -0.2000,  0.0000,  0.0000,  0.6000, -0.4000,  0.5000,\n",
            "          -0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
            "           0.3000,  0.2000,  0.5000,  0.0000, -0.4000,  0.0000, -0.4000,\n",
            "           0.3000,  0.0000,  0.0000,  0.0000, -0.4000,  0.0000,  0.5000,\n",
            "           0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000, -0.2000,\n",
            "           0.3000, -0.4000]]])\n",
            "pred tensor([[[ 0.0565,  0.2073,  0.3030, -0.2549,  0.4801,  0.4336, -0.0534,\n",
            "           0.1518,  0.4217, -0.4105,  0.1069,  0.0603, -0.3737, -0.2323,\n",
            "           0.0078,  0.0655, -0.2938,  0.3153, -0.4719,  0.5728,  0.4235,\n",
            "           0.3752, -0.2749,  0.0707,  0.2858, -0.1101,  0.5450,  0.2820,\n",
            "           0.3405, -0.0587,  0.6738, -0.1076,  0.0813, -0.0092, -0.0615,\n",
            "          -0.1203, -0.4123,  0.6057, -0.0118,  0.1176,  0.1428,  0.4942,\n",
            "           0.5919,  0.0233, -0.0723,  0.0458, -0.3029,  0.3467, -0.0677,\n",
            "          -0.1618,  0.2056, -0.2131,  0.2642,  0.0883,  0.0348, -0.0618,\n",
            "           0.4264, -0.2782,  0.2486, -0.0154,  0.0501, -0.4258, -0.0848,\n",
            "           0.1280, -0.1270,  0.0097, -0.1122,  0.3119, -0.3433,  0.3131,\n",
            "          -0.4474, -0.4388, -0.4449, -0.4374, -0.2070,  0.0047,  0.4467,\n",
            "           0.2859,  0.1154,  0.4303,  0.0766, -0.3876,  0.0557, -0.3064,\n",
            "           0.3123,  0.1581,  0.0694,  0.2275, -0.5417,  0.1376,  0.4049,\n",
            "           0.3384,  0.3651,  0.1172, -0.0732,  0.0618, -0.5036, -0.1887,\n",
            "           0.1346, -0.3127]]])\n",
            "x tensor([[[ 0.0618,  0.4910,  0.0659,  0.0617,  0.1002,  0.0078,  0.3823,\n",
            "           0.6141,  0.2087,  0.2565,  0.0369,  0.0386,  0.0204,  0.0185,\n",
            "           0.3893, -0.4769,  0.5229,  0.4450, -0.4036,  0.0044, -0.0042,\n",
            "           0.0559,  0.0243,  0.4507, -0.1911,  0.3623, -0.4669,  0.2460,\n",
            "           0.4909, -0.0077, -0.0043,  0.0563, -0.1680,  0.0477, -0.1491,\n",
            "           0.1000,  0.0429, -0.0586, -0.1763,  0.4750, -0.5252, -0.0102,\n",
            "           0.0044,  0.3419, -0.4832,  0.5341,  0.0817, -0.0328,  0.2554,\n",
            "           0.0550,  0.1156, -0.4956, -0.4484, -0.5622,  0.3319, -0.3887,\n",
            "          -0.4813,  0.1118,  0.1446,  0.0871,  0.3897,  0.0433, -0.4223,\n",
            "          -0.0112, -0.0103,  0.0583,  0.0187,  0.4850,  0.0213,  0.5210,\n",
            "           0.1450,  0.0028, -0.2643,  0.1555, -0.4445, -0.5146, -0.4895,\n",
            "          -0.0116,  0.2272,  0.6472,  0.1607, -0.5420,  0.0293, -0.4902,\n",
            "          -0.5888, -0.0096,  0.5049, -0.0019,  0.0395,  0.5038, -0.0426,\n",
            "          -0.1720, -0.4981,  0.0987,  0.0627,  0.0377,  0.1711,  0.6532,\n",
            "          -0.0529,  0.4699]]])\n",
            "y tensor([[[ 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
            "           0.6000,  0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.5000, -0.4000,  0.6000,  0.6000, -0.4000,  0.0000,  0.0000,\n",
            "           0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,  0.3000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
            "           0.0000,  0.0000,  0.0000, -0.2000,  0.6000, -0.4000,  0.6000,\n",
            "           0.0000,  0.3000, -0.4000,  0.6000,  0.0000,  0.0000,  0.3000,\n",
            "           0.0000,  0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000,\n",
            "          -0.4000,  0.0000,  0.2000,  0.0000,  0.3000,  0.0000, -0.4000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.5000,\n",
            "           0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
            "           0.0000,  0.2000,  0.6000,  0.2000, -0.4000,  0.0000, -0.4000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.0000,  0.5000,  0.0000,\n",
            "          -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,  0.6000,\n",
            "           0.0000,  0.5000]]])\n",
            "pred tensor([[[ 6.2774e-02,  5.4926e-01, -6.5387e-02,  9.8089e-02,  1.4914e-01,\n",
            "          -4.1116e-02,  4.7802e-01,  5.2669e-01,  2.2279e-01,  2.0503e-01,\n",
            "           1.3216e-01, -6.9067e-02, -2.5684e-03, -1.1005e-01,  5.2756e-01,\n",
            "          -4.8318e-01,  5.9509e-01,  4.8103e-01, -1.9968e-01, -5.1140e-02,\n",
            "           8.9390e-03,  6.0114e-04,  5.2054e-03,  3.0677e-01, -1.6220e-01,\n",
            "           3.1823e-01, -4.4443e-01,  1.7459e-01,  6.4889e-01, -9.5728e-02,\n",
            "           9.7498e-02, -1.2356e-01, -1.0395e-02, -1.9575e-01,  2.1706e-02,\n",
            "          -5.6482e-02,  2.2433e-01, -1.9189e-01, -1.3683e-01,  3.2593e-01,\n",
            "          -3.2798e-01, -1.5433e-02, -9.2505e-03,  3.1393e-01, -4.1143e-01,\n",
            "           5.6422e-01,  1.1017e-01, -1.3658e-01,  2.5428e-01,  2.1570e-02,\n",
            "           4.1455e-02, -4.1556e-01, -3.4807e-01, -5.6341e-01,  3.0267e-01,\n",
            "          -4.0255e-01, -4.0352e-01,  1.3848e-01,  1.1986e-01,  3.9446e-02,\n",
            "           4.2635e-01,  1.3228e-02, -3.5121e-01, -7.3094e-02,  1.3963e-01,\n",
            "          -1.0037e-03,  5.7872e-02,  4.6510e-01,  4.0614e-02,  4.8869e-01,\n",
            "           2.1302e-01, -7.7691e-02, -1.2841e-01,  4.1262e-02, -3.1986e-01,\n",
            "          -4.0232e-01, -3.9791e-01, -1.7544e-01,  3.4860e-01,  6.1971e-01,\n",
            "           1.7742e-01, -5.1354e-01,  2.6508e-02, -4.1649e-01, -3.6795e-01,\n",
            "          -1.3363e-02,  5.8137e-01, -1.3091e-02,  1.3376e-01,  4.1593e-01,\n",
            "          -1.3841e-01, -2.5114e-01, -3.7767e-01,  5.7410e-03,  2.1531e-01,\n",
            "          -3.6704e-02,  3.5247e-01,  5.3297e-01,  1.0226e-01,  2.7679e-01]]])\n",
            "x tensor([[[ 1.5207e-01, -4.5816e-01, -3.2321e-01,  1.2050e-02,  1.2225e-01,\n",
            "          -4.5844e-01,  8.9613e-02, -2.7273e-01,  1.1309e-01, -2.3914e-02,\n",
            "           5.0364e-01,  1.5636e-01,  4.9550e-02, -2.3475e-01, -2.3575e-01,\n",
            "           1.8438e-01,  3.8710e-01, -4.4908e-01,  2.7194e-01,  3.4552e-01,\n",
            "           1.2505e-01, -3.8047e-02,  1.4039e-02,  4.5893e-01,  6.1139e-02,\n",
            "          -3.2275e-01, -5.0564e-01,  2.7278e-01, -1.5879e-02,  3.7046e-01,\n",
            "          -4.4813e-01,  1.6258e-01,  4.9993e-01, -4.5045e-01,  2.6548e-02,\n",
            "           5.0095e-01, -7.1420e-03,  8.5321e-02, -4.7502e-01, -5.1078e-01,\n",
            "          -7.2781e-02, -4.2473e-01,  1.7987e-01,  1.8869e-03,  3.7284e-02,\n",
            "           9.7761e-02, -2.4007e-01, -1.9300e-02, -2.1564e-01, -4.7269e-01,\n",
            "          -4.6971e-01, -3.4520e-01,  4.3727e-01,  5.0005e-01, -2.6470e-01,\n",
            "           8.2406e-02,  7.9866e-02,  5.3849e-02, -2.4046e-01,  1.3397e-01,\n",
            "          -2.0271e-01,  5.6790e-01,  2.6949e-01,  7.5627e-02,  4.4006e-01,\n",
            "           5.0170e-01, -4.4758e-01, -4.6685e-01,  1.3127e-01,  4.5948e-02,\n",
            "          -2.7190e-01,  7.3369e-02,  6.3947e-02,  1.2952e-01,  4.9203e-01,\n",
            "           7.1726e-02, -2.7601e-02, -4.5895e-01, -4.7017e-01,  4.4787e-02,\n",
            "          -2.5383e-02,  4.4729e-01,  4.4143e-01, -2.3552e-01, -4.8626e-01,\n",
            "           3.9564e-01, -1.8680e-01, -8.8002e-03,  1.1407e-01, -2.9984e-01,\n",
            "          -3.4489e-02, -5.1843e-01,  2.4105e-02,  6.2846e-01,  8.5015e-02,\n",
            "           3.6069e-02, -2.1637e-04, -4.3929e-02, -5.0672e-01, -3.9851e-01]]])\n",
            "y tensor([[[ 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
            "          -0.2000,  0.2000,  0.0000,  0.5000,  0.2000,  0.0000, -0.2000,\n",
            "          -0.2000,  0.2000,  0.5000, -0.4000,  0.3000,  0.3000,  0.0000,\n",
            "           0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,  0.3000,\n",
            "           0.0000,  0.3000, -0.4000,  0.2000,  0.5000, -0.4000,  0.0000,\n",
            "           0.5000,  0.0000,  0.2000, -0.4000, -0.4000,  0.0000, -0.4000,\n",
            "           0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
            "          -0.4000, -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,\n",
            "           0.0000,  0.0000, -0.2000,  0.2000, -0.2000,  0.6000,  0.3000,\n",
            "           0.0000,  0.5000,  0.6000, -0.4000, -0.4000,  0.2000,  0.0000,\n",
            "          -0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
            "          -0.4000, -0.4000,  0.0000,  0.0000,  0.5000,  0.5000, -0.2000,\n",
            "          -0.4000,  0.3000, -0.2000,  0.0000,  0.0000, -0.2000,  0.0000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,  0.0000,\n",
            "          -0.4000, -0.4000]]])\n",
            "pred tensor([[[ 0.0058, -0.3279, -0.2728, -0.0172, -0.0115, -0.3564,  0.0352,\n",
            "          -0.1525, -0.0396,  0.0533,  0.5167,  0.2252, -0.0247, -0.1571,\n",
            "          -0.2648,  0.2388,  0.3895, -0.3029,  0.3175,  0.4568,  0.0820,\n",
            "           0.0013, -0.0625,  0.5544,  0.0402, -0.2975, -0.3851,  0.2462,\n",
            "           0.0093,  0.3169, -0.3647,  0.0421,  0.6693, -0.3960,  0.1317,\n",
            "           0.5362,  0.0661,  0.0260, -0.4666, -0.4511,  0.0011, -0.4166,\n",
            "           0.2066, -0.0108,  0.0849,  0.0360, -0.1871, -0.0831, -0.1674,\n",
            "          -0.3957, -0.3586, -0.2874,  0.4475,  0.4892, -0.1663,  0.0678,\n",
            "           0.2344, -0.0497, -0.2321, -0.0416, -0.1457,  0.6290,  0.4337,\n",
            "           0.0216,  0.4073,  0.5065, -0.3904, -0.4069,  0.2001,  0.0686,\n",
            "          -0.3157, -0.0090,  0.1194,  0.1151,  0.5512,  0.0949,  0.0272,\n",
            "          -0.4140, -0.4723, -0.0365,  0.0181,  0.4211,  0.4453, -0.1887,\n",
            "          -0.4774,  0.3419, -0.1364,  0.0226,  0.0566, -0.2485,  0.0295,\n",
            "          -0.4057,  0.0596,  0.5367,  0.2096,  0.0658,  0.0537, -0.0689,\n",
            "          -0.2674, -0.3340]]])\n",
            "Test Error:  Avg loss: 0.011204 \n",
            "\n",
            "[tensor([[[ 1.5040e-01,  2.7753e-02,  2.6380e-01, -4.1143e-01,  5.8431e-01,\n",
            "           3.5105e-01,  3.9801e-02,  6.2555e-02,  4.4214e-01, -4.6540e-01,\n",
            "           1.9462e-01,  5.3590e-02, -4.6464e-01, -3.0317e-01,  1.7399e-02,\n",
            "           1.5962e-03, -2.5415e-01,  3.5795e-01, -3.8097e-01,  4.9359e-01,\n",
            "           4.8813e-01,  4.1093e-01, -2.1811e-01,  8.8324e-02,  2.7562e-01,\n",
            "          -7.1405e-02,  4.5740e-01,  3.1607e-01,  2.5520e-01,  3.7994e-02,\n",
            "           5.7717e-01,  1.2074e-02,  6.7664e-02, -6.1348e-03, -4.8868e-02,\n",
            "          -6.5389e-02, -5.4172e-01,  5.6659e-01,  3.2106e-02,  1.2346e-01,\n",
            "           7.8257e-02,  4.5159e-01,  4.8174e-01,  3.4322e-02, -6.3859e-02,\n",
            "           8.9762e-02, -3.6590e-01,  2.7255e-01,  5.2111e-02, -2.3775e-01,\n",
            "           2.0834e-01, -4.0831e-01,  2.9185e-01, -6.6780e-06,  7.1918e-02,\n",
            "          -3.3397e-02,  4.5936e-01, -2.7687e-01,  2.5574e-01, -5.4087e-03,\n",
            "           1.0142e-01, -4.8906e-01,  2.7102e-02,  6.3380e-02, -2.2070e-01,\n",
            "          -2.2836e-02, -5.0412e-02,  4.2620e-01, -4.2748e-01,  4.2188e-01,\n",
            "          -5.3945e-01, -3.4004e-01, -4.1858e-01, -3.2877e-01, -2.2333e-01,\n",
            "           5.4785e-02,  3.2787e-01,  2.5013e-01,  8.4432e-02,  4.8181e-01,\n",
            "           1.7438e-02, -5.4653e-01,  4.1048e-02, -4.2961e-01,  3.5564e-01,\n",
            "           3.8394e-03,  1.0678e-01,  5.4504e-02, -4.8490e-01,  3.6136e-02,\n",
            "           4.7570e-01,  1.6878e-01,  4.7856e-01,  2.5320e-02,  6.2321e-02,\n",
            "           2.8568e-02, -5.0877e-01, -2.3411e-01,  2.7215e-01, -4.1985e-01]]]), tensor([[[-2.6127e-01, -4.6763e-01,  6.2371e-02,  5.2001e-01, -7.0590e-03,\n",
            "          -5.2496e-01,  1.4165e-01, -5.2515e-01,  2.6908e-02,  3.4106e-03,\n",
            "          -5.3161e-01,  5.9588e-01,  6.6905e-02,  3.2951e-01,  4.4232e-01,\n",
            "          -2.1317e-02, -1.8266e-01,  4.8799e-01,  2.1765e-02, -2.6534e-02,\n",
            "           1.9717e-01, -4.2183e-01, -1.4312e-02,  4.1642e-01,  2.6945e-01,\n",
            "           6.4946e-01,  2.3988e-01,  4.8178e-02, -2.6797e-02, -3.7925e-01,\n",
            "          -5.7487e-01, -2.4094e-01, -2.5117e-01,  7.0525e-01,  1.3802e-01,\n",
            "           3.7031e-01, -5.4238e-01,  5.7693e-01,  5.1506e-01,  4.7791e-01,\n",
            "           2.0435e-02,  3.5112e-02,  7.6267e-02,  6.1888e-01, -2.5576e-02,\n",
            "          -4.8947e-01, -4.7947e-01, -3.6862e-01, -4.5220e-01,  2.7477e-01,\n",
            "          -2.1003e-01,  7.2061e-02, -2.0308e-02, -3.9764e-01,  4.9790e-01,\n",
            "           5.3874e-02, -1.4719e-02,  5.0923e-01,  1.0956e-01, -4.2560e-01,\n",
            "          -4.1023e-01,  1.6442e-01,  1.7020e-01,  3.6917e-02, -2.5307e-01,\n",
            "           4.4099e-01,  7.4603e-05, -3.9181e-01, -1.8042e-01,  1.0617e-01,\n",
            "           5.3088e-01, -2.0616e-01,  5.6170e-01,  1.7280e-01,  4.5557e-01,\n",
            "          -4.1374e-02, -5.7587e-01,  6.8499e-02, -4.1197e-01, -1.6708e-01,\n",
            "          -1.9162e-01,  4.8799e-01,  3.8270e-01, -4.5628e-01,  1.4194e-01,\n",
            "           7.3972e-02, -1.5844e-01, -2.7551e-01, -4.5341e-01,  3.8624e-01,\n",
            "           1.7708e-01,  6.0247e-02, -3.0290e-02,  6.3880e-02,  1.2618e-02,\n",
            "           4.4718e-01, -2.0038e-01,  4.0293e-01,  5.5030e-01,  4.1853e-01]]]), tensor([[[ 0.0618,  0.4910,  0.0659,  0.0617,  0.1002,  0.0078,  0.3823,\n",
            "           0.6141,  0.2087,  0.2565,  0.0369,  0.0386,  0.0204,  0.0185,\n",
            "           0.3893, -0.4769,  0.5229,  0.4450, -0.4036,  0.0044, -0.0042,\n",
            "           0.0559,  0.0243,  0.4507, -0.1911,  0.3623, -0.4669,  0.2460,\n",
            "           0.4909, -0.0077, -0.0043,  0.0563, -0.1680,  0.0477, -0.1491,\n",
            "           0.1000,  0.0429, -0.0586, -0.1763,  0.4750, -0.5252, -0.0102,\n",
            "           0.0044,  0.3419, -0.4832,  0.5341,  0.0817, -0.0328,  0.2554,\n",
            "           0.0550,  0.1156, -0.4956, -0.4484, -0.5622,  0.3319, -0.3887,\n",
            "          -0.4813,  0.1118,  0.1446,  0.0871,  0.3897,  0.0433, -0.4223,\n",
            "          -0.0112, -0.0103,  0.0583,  0.0187,  0.4850,  0.0213,  0.5210,\n",
            "           0.1450,  0.0028, -0.2643,  0.1555, -0.4445, -0.5146, -0.4895,\n",
            "          -0.0116,  0.2272,  0.6472,  0.1607, -0.5420,  0.0293, -0.4902,\n",
            "          -0.5888, -0.0096,  0.5049, -0.0019,  0.0395,  0.5038, -0.0426,\n",
            "          -0.1720, -0.4981,  0.0987,  0.0627,  0.0377,  0.1711,  0.6532,\n",
            "          -0.0529,  0.4699]]]), tensor([[[ 0.0328, -0.4573, -0.4725, -0.0247,  0.0540,  0.0859,  0.4515,\n",
            "           0.1295, -0.2223, -0.0262,  0.0335, -0.4611,  0.4399, -0.0082,\n",
            "          -0.4945, -0.3633, -0.2324,  0.5773,  0.6022,  0.2374,  0.0691,\n",
            "          -0.0194,  0.4261,  0.1651, -0.2748,  0.5246,  0.0692,  0.4750,\n",
            "           0.1088,  0.0235,  0.1454,  0.3780,  0.3894,  0.4578, -0.4706,\n",
            "           0.5107,  0.1996,  0.4277,  0.6494,  0.1303,  0.5510,  0.4877,\n",
            "           0.4463,  0.4929, -0.0060,  0.0577, -0.4770,  0.0755,  0.0523,\n",
            "          -0.5525,  0.0088,  0.4627,  0.0546,  0.5075,  0.6435,  0.1435,\n",
            "           0.4656, -0.3355,  0.5981,  0.1251,  0.1323,  0.1832, -0.4654,\n",
            "          -0.2253,  0.5101, -0.0036,  0.3210,  0.0282,  0.1331,  0.5667,\n",
            "           0.2723, -0.1965, -0.2652,  0.4898,  0.0672,  0.4451,  0.0650,\n",
            "          -0.0210,  0.0807,  0.0530, -0.5118, -0.4409,  0.1643, -0.2564,\n",
            "          -0.4262,  0.2940, -0.5093,  0.3116, -0.3308, -0.5235,  0.4505,\n",
            "           0.2786,  0.5227,  0.5679, -0.1540, -0.2014,  0.0721, -0.2696,\n",
            "           0.4109,  0.0056]]]), tensor([[[ 1.5207e-01, -4.5816e-01, -3.2321e-01,  1.2050e-02,  1.2225e-01,\n",
            "          -4.5844e-01,  8.9613e-02, -2.7273e-01,  1.1309e-01, -2.3914e-02,\n",
            "           5.0364e-01,  1.5636e-01,  4.9550e-02, -2.3475e-01, -2.3575e-01,\n",
            "           1.8438e-01,  3.8710e-01, -4.4908e-01,  2.7194e-01,  3.4552e-01,\n",
            "           1.2505e-01, -3.8047e-02,  1.4039e-02,  4.5893e-01,  6.1139e-02,\n",
            "          -3.2275e-01, -5.0564e-01,  2.7278e-01, -1.5879e-02,  3.7046e-01,\n",
            "          -4.4813e-01,  1.6258e-01,  4.9993e-01, -4.5045e-01,  2.6548e-02,\n",
            "           5.0095e-01, -7.1420e-03,  8.5321e-02, -4.7502e-01, -5.1078e-01,\n",
            "          -7.2781e-02, -4.2473e-01,  1.7987e-01,  1.8869e-03,  3.7284e-02,\n",
            "           9.7761e-02, -2.4007e-01, -1.9300e-02, -2.1564e-01, -4.7269e-01,\n",
            "          -4.6971e-01, -3.4520e-01,  4.3727e-01,  5.0005e-01, -2.6470e-01,\n",
            "           8.2406e-02,  7.9866e-02,  5.3849e-02, -2.4046e-01,  1.3397e-01,\n",
            "          -2.0271e-01,  5.6790e-01,  2.6949e-01,  7.5627e-02,  4.4006e-01,\n",
            "           5.0170e-01, -4.4758e-01, -4.6685e-01,  1.3127e-01,  4.5948e-02,\n",
            "          -2.7190e-01,  7.3369e-02,  6.3947e-02,  1.2952e-01,  4.9203e-01,\n",
            "           7.1726e-02, -2.7601e-02, -4.5895e-01, -4.7017e-01,  4.4787e-02,\n",
            "          -2.5383e-02,  4.4729e-01,  4.4143e-01, -2.3552e-01, -4.8626e-01,\n",
            "           3.9564e-01, -1.8680e-01, -8.8002e-03,  1.1407e-01, -2.9984e-01,\n",
            "          -3.4489e-02, -5.1843e-01,  2.4105e-02,  6.2846e-01,  8.5015e-02,\n",
            "           3.6069e-02, -2.1637e-04, -4.3929e-02, -5.0672e-01, -3.9851e-01]]]), tensor([[[-2.6127e-01, -4.6763e-01,  6.2371e-02,  5.2001e-01, -7.0590e-03,\n",
            "          -5.2496e-01,  1.4165e-01, -5.2515e-01,  2.6908e-02,  3.4106e-03,\n",
            "          -5.3161e-01,  5.9588e-01,  6.6905e-02,  3.2951e-01,  4.4232e-01,\n",
            "          -2.1317e-02, -1.8266e-01,  4.8799e-01,  2.1765e-02, -2.6534e-02,\n",
            "           1.9717e-01, -4.2183e-01, -1.4312e-02,  4.1642e-01,  2.6945e-01,\n",
            "           6.4946e-01,  2.3988e-01,  4.8178e-02, -2.6797e-02, -3.7925e-01,\n",
            "          -5.7487e-01, -2.4094e-01, -2.5117e-01,  7.0525e-01,  1.3802e-01,\n",
            "           3.7031e-01, -5.4238e-01,  5.7693e-01,  5.1506e-01,  4.7791e-01,\n",
            "           2.0435e-02,  3.5112e-02,  7.6267e-02,  6.1888e-01, -2.5576e-02,\n",
            "          -4.8947e-01, -4.7947e-01, -3.6862e-01, -4.5220e-01,  2.7477e-01,\n",
            "          -2.1003e-01,  7.2061e-02, -2.0308e-02, -3.9764e-01,  4.9790e-01,\n",
            "           5.3874e-02, -1.4719e-02,  5.0923e-01,  1.0956e-01, -4.2560e-01,\n",
            "          -4.1023e-01,  1.6442e-01,  1.7020e-01,  3.6917e-02, -2.5307e-01,\n",
            "           4.4099e-01,  7.4603e-05, -3.9181e-01, -1.8042e-01,  1.0617e-01,\n",
            "           5.3088e-01, -2.0616e-01,  5.6170e-01,  1.7280e-01,  4.5557e-01,\n",
            "          -4.1374e-02, -5.7587e-01,  6.8499e-02, -4.1197e-01, -1.6708e-01,\n",
            "          -1.9162e-01,  4.8799e-01,  3.8270e-01, -4.5628e-01,  1.4194e-01,\n",
            "           7.3972e-02, -1.5844e-01, -2.7551e-01, -4.5341e-01,  3.8624e-01,\n",
            "           1.7708e-01,  6.0247e-02, -3.0290e-02,  6.3880e-02,  1.2618e-02,\n",
            "           4.4718e-01, -2.0038e-01,  4.0293e-01,  5.5030e-01,  4.1853e-01]]]), tensor([[[ 0.0328, -0.4573, -0.4725, -0.0247,  0.0540,  0.0859,  0.4515,\n",
            "           0.1295, -0.2223, -0.0262,  0.0335, -0.4611,  0.4399, -0.0082,\n",
            "          -0.4945, -0.3633, -0.2324,  0.5773,  0.6022,  0.2374,  0.0691,\n",
            "          -0.0194,  0.4261,  0.1651, -0.2748,  0.5246,  0.0692,  0.4750,\n",
            "           0.1088,  0.0235,  0.1454,  0.3780,  0.3894,  0.4578, -0.4706,\n",
            "           0.5107,  0.1996,  0.4277,  0.6494,  0.1303,  0.5510,  0.4877,\n",
            "           0.4463,  0.4929, -0.0060,  0.0577, -0.4770,  0.0755,  0.0523,\n",
            "          -0.5525,  0.0088,  0.4627,  0.0546,  0.5075,  0.6435,  0.1435,\n",
            "           0.4656, -0.3355,  0.5981,  0.1251,  0.1323,  0.1832, -0.4654,\n",
            "          -0.2253,  0.5101, -0.0036,  0.3210,  0.0282,  0.1331,  0.5667,\n",
            "           0.2723, -0.1965, -0.2652,  0.4898,  0.0672,  0.4451,  0.0650,\n",
            "          -0.0210,  0.0807,  0.0530, -0.5118, -0.4409,  0.1643, -0.2564,\n",
            "          -0.4262,  0.2940, -0.5093,  0.3116, -0.3308, -0.5235,  0.4505,\n",
            "           0.2786,  0.5227,  0.5679, -0.1540, -0.2014,  0.0721, -0.2696,\n",
            "           0.4109,  0.0056]]]), tensor([[[ 1.5040e-01,  2.7753e-02,  2.6380e-01, -4.1143e-01,  5.8431e-01,\n",
            "           3.5105e-01,  3.9801e-02,  6.2555e-02,  4.4214e-01, -4.6540e-01,\n",
            "           1.9462e-01,  5.3590e-02, -4.6464e-01, -3.0317e-01,  1.7399e-02,\n",
            "           1.5962e-03, -2.5415e-01,  3.5795e-01, -3.8097e-01,  4.9359e-01,\n",
            "           4.8813e-01,  4.1093e-01, -2.1811e-01,  8.8324e-02,  2.7562e-01,\n",
            "          -7.1405e-02,  4.5740e-01,  3.1607e-01,  2.5520e-01,  3.7994e-02,\n",
            "           5.7717e-01,  1.2074e-02,  6.7664e-02, -6.1348e-03, -4.8868e-02,\n",
            "          -6.5389e-02, -5.4172e-01,  5.6659e-01,  3.2106e-02,  1.2346e-01,\n",
            "           7.8257e-02,  4.5159e-01,  4.8174e-01,  3.4322e-02, -6.3859e-02,\n",
            "           8.9762e-02, -3.6590e-01,  2.7255e-01,  5.2111e-02, -2.3775e-01,\n",
            "           2.0834e-01, -4.0831e-01,  2.9185e-01, -6.6780e-06,  7.1918e-02,\n",
            "          -3.3397e-02,  4.5936e-01, -2.7687e-01,  2.5574e-01, -5.4087e-03,\n",
            "           1.0142e-01, -4.8906e-01,  2.7102e-02,  6.3380e-02, -2.2070e-01,\n",
            "          -2.2836e-02, -5.0412e-02,  4.2620e-01, -4.2748e-01,  4.2188e-01,\n",
            "          -5.3945e-01, -3.4004e-01, -4.1858e-01, -3.2877e-01, -2.2333e-01,\n",
            "           5.4785e-02,  3.2787e-01,  2.5013e-01,  8.4432e-02,  4.8181e-01,\n",
            "           1.7438e-02, -5.4653e-01,  4.1048e-02, -4.2961e-01,  3.5564e-01,\n",
            "           3.8394e-03,  1.0678e-01,  5.4504e-02, -4.8490e-01,  3.6136e-02,\n",
            "           4.7570e-01,  1.6878e-01,  4.7856e-01,  2.5320e-02,  6.2321e-02,\n",
            "           2.8568e-02, -5.0877e-01, -2.3411e-01,  2.7215e-01, -4.1985e-01]]]), tensor([[[ 0.0618,  0.4910,  0.0659,  0.0617,  0.1002,  0.0078,  0.3823,\n",
            "           0.6141,  0.2087,  0.2565,  0.0369,  0.0386,  0.0204,  0.0185,\n",
            "           0.3893, -0.4769,  0.5229,  0.4450, -0.4036,  0.0044, -0.0042,\n",
            "           0.0559,  0.0243,  0.4507, -0.1911,  0.3623, -0.4669,  0.2460,\n",
            "           0.4909, -0.0077, -0.0043,  0.0563, -0.1680,  0.0477, -0.1491,\n",
            "           0.1000,  0.0429, -0.0586, -0.1763,  0.4750, -0.5252, -0.0102,\n",
            "           0.0044,  0.3419, -0.4832,  0.5341,  0.0817, -0.0328,  0.2554,\n",
            "           0.0550,  0.1156, -0.4956, -0.4484, -0.5622,  0.3319, -0.3887,\n",
            "          -0.4813,  0.1118,  0.1446,  0.0871,  0.3897,  0.0433, -0.4223,\n",
            "          -0.0112, -0.0103,  0.0583,  0.0187,  0.4850,  0.0213,  0.5210,\n",
            "           0.1450,  0.0028, -0.2643,  0.1555, -0.4445, -0.5146, -0.4895,\n",
            "          -0.0116,  0.2272,  0.6472,  0.1607, -0.5420,  0.0293, -0.4902,\n",
            "          -0.5888, -0.0096,  0.5049, -0.0019,  0.0395,  0.5038, -0.0426,\n",
            "          -0.1720, -0.4981,  0.0987,  0.0627,  0.0377,  0.1711,  0.6532,\n",
            "          -0.0529,  0.4699]]]), tensor([[[ 1.5207e-01, -4.5816e-01, -3.2321e-01,  1.2050e-02,  1.2225e-01,\n",
            "          -4.5844e-01,  8.9613e-02, -2.7273e-01,  1.1309e-01, -2.3914e-02,\n",
            "           5.0364e-01,  1.5636e-01,  4.9550e-02, -2.3475e-01, -2.3575e-01,\n",
            "           1.8438e-01,  3.8710e-01, -4.4908e-01,  2.7194e-01,  3.4552e-01,\n",
            "           1.2505e-01, -3.8047e-02,  1.4039e-02,  4.5893e-01,  6.1139e-02,\n",
            "          -3.2275e-01, -5.0564e-01,  2.7278e-01, -1.5879e-02,  3.7046e-01,\n",
            "          -4.4813e-01,  1.6258e-01,  4.9993e-01, -4.5045e-01,  2.6548e-02,\n",
            "           5.0095e-01, -7.1420e-03,  8.5321e-02, -4.7502e-01, -5.1078e-01,\n",
            "          -7.2781e-02, -4.2473e-01,  1.7987e-01,  1.8869e-03,  3.7284e-02,\n",
            "           9.7761e-02, -2.4007e-01, -1.9300e-02, -2.1564e-01, -4.7269e-01,\n",
            "          -4.6971e-01, -3.4520e-01,  4.3727e-01,  5.0005e-01, -2.6470e-01,\n",
            "           8.2406e-02,  7.9866e-02,  5.3849e-02, -2.4046e-01,  1.3397e-01,\n",
            "          -2.0271e-01,  5.6790e-01,  2.6949e-01,  7.5627e-02,  4.4006e-01,\n",
            "           5.0170e-01, -4.4758e-01, -4.6685e-01,  1.3127e-01,  4.5948e-02,\n",
            "          -2.7190e-01,  7.3369e-02,  6.3947e-02,  1.2952e-01,  4.9203e-01,\n",
            "           7.1726e-02, -2.7601e-02, -4.5895e-01, -4.7017e-01,  4.4787e-02,\n",
            "          -2.5383e-02,  4.4729e-01,  4.4143e-01, -2.3552e-01, -4.8626e-01,\n",
            "           3.9564e-01, -1.8680e-01, -8.8002e-03,  1.1407e-01, -2.9984e-01,\n",
            "          -3.4489e-02, -5.1843e-01,  2.4105e-02,  6.2846e-01,  8.5015e-02,\n",
            "           3.6069e-02, -2.1637e-04, -4.3929e-02, -5.0672e-01, -3.9851e-01]]])]\n",
            "[tensor([[[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
            "           0.0000,  0.5000, -0.4000,  0.2000,  0.0000, -0.4000, -0.2000,\n",
            "           0.0000,  0.0000, -0.2000,  0.5000, -0.4000,  0.5000,  0.5000,\n",
            "           0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,  0.3000,\n",
            "           0.3000,  0.0000,  0.6000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000, -0.4000,  0.6000,  0.0000,  0.2000,  0.2000,  0.5000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000, -0.4000,  0.3000,  0.0000,\n",
            "          -0.2000,  0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
            "           0.5000, -0.2000,  0.2000,  0.0000,  0.2000, -0.4000,  0.0000,\n",
            "           0.0000, -0.2000,  0.0000,  0.0000,  0.6000, -0.4000,  0.5000,\n",
            "          -0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
            "           0.3000,  0.2000,  0.5000,  0.0000, -0.4000,  0.0000, -0.4000,\n",
            "           0.3000,  0.0000,  0.0000,  0.0000, -0.4000,  0.0000,  0.5000,\n",
            "           0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000, -0.2000,\n",
            "           0.3000, -0.4000]]]), tensor([[[-0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
            "          -0.4000,  0.2000,  0.0000, -0.4000,  0.5000,  0.0000,  0.3000,\n",
            "           0.5000,  0.0000, -0.2000,  0.6000,  0.0000,  0.0000,  0.2000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,  0.0000,\n",
            "           0.0000, -0.4000, -0.4000, -0.2000, -0.2000,  0.6000,  0.2000,\n",
            "           0.5000, -0.4000,  0.5000,  0.6000,  0.5000,  0.0000,  0.0000,\n",
            "           0.2000,  0.6000,  0.0000, -0.4000, -0.4000, -0.2000, -0.4000,\n",
            "           0.5000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
            "           0.0000,  0.6000,  0.0000, -0.4000, -0.4000,  0.3000,  0.2000,\n",
            "           0.0000, -0.2000,  0.5000,  0.0000, -0.4000, -0.2000,  0.2000,\n",
            "           0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
            "           0.0000, -0.4000, -0.2000, -0.2000,  0.6000,  0.3000, -0.4000,\n",
            "           0.2000,  0.0000, -0.2000, -0.2000, -0.4000,  0.5000,  0.2000,\n",
            "           0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,  0.5000,\n",
            "           0.5000,  0.6000]]]), tensor([[[ 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
            "           0.6000,  0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.5000, -0.4000,  0.6000,  0.6000, -0.4000,  0.0000,  0.0000,\n",
            "           0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,  0.3000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
            "           0.0000,  0.0000,  0.0000, -0.2000,  0.6000, -0.4000,  0.6000,\n",
            "           0.0000,  0.3000, -0.4000,  0.6000,  0.0000,  0.0000,  0.3000,\n",
            "           0.0000,  0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000,\n",
            "          -0.4000,  0.0000,  0.2000,  0.0000,  0.3000,  0.0000, -0.4000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.5000,\n",
            "           0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
            "           0.0000,  0.2000,  0.6000,  0.2000, -0.4000,  0.0000, -0.4000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.0000,  0.5000,  0.0000,\n",
            "          -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,  0.6000,\n",
            "           0.0000,  0.5000]]]), tensor([[[ 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "           0.0000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
            "          -0.4000, -0.4000, -0.2000,  0.6000,  0.6000,  0.3000,  0.0000,\n",
            "           0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,  0.5000,\n",
            "           0.0000,  0.0000,  0.2000,  0.5000,  0.3000,  0.5000, -0.4000,\n",
            "           0.5000,  0.2000,  0.5000,  0.6000,  0.2000,  0.5000,  0.5000,\n",
            "           0.5000,  0.5000,  0.0000,  0.0000, -0.4000,  0.0000,  0.0000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,\n",
            "           0.6000, -0.4000,  0.6000,  0.2000,  0.0000,  0.2000, -0.4000,\n",
            "          -0.2000,  0.6000,  0.0000,  0.3000,  0.0000,  0.2000,  0.6000,\n",
            "           0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000, -0.4000, -0.4000,  0.2000, -0.2000,\n",
            "          -0.4000,  0.3000, -0.4000,  0.3000, -0.2000, -0.4000,  0.5000,\n",
            "           0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000, -0.2000,\n",
            "           0.5000,  0.0000]]]), tensor([[[ 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
            "          -0.2000,  0.2000,  0.0000,  0.5000,  0.2000,  0.0000, -0.2000,\n",
            "          -0.2000,  0.2000,  0.5000, -0.4000,  0.3000,  0.3000,  0.0000,\n",
            "           0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,  0.3000,\n",
            "           0.0000,  0.3000, -0.4000,  0.2000,  0.5000, -0.4000,  0.0000,\n",
            "           0.5000,  0.0000,  0.2000, -0.4000, -0.4000,  0.0000, -0.4000,\n",
            "           0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
            "          -0.4000, -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,\n",
            "           0.0000,  0.0000, -0.2000,  0.2000, -0.2000,  0.6000,  0.3000,\n",
            "           0.0000,  0.5000,  0.6000, -0.4000, -0.4000,  0.2000,  0.0000,\n",
            "          -0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
            "          -0.4000, -0.4000,  0.0000,  0.0000,  0.5000,  0.5000, -0.2000,\n",
            "          -0.4000,  0.3000, -0.2000,  0.0000,  0.0000, -0.2000,  0.0000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,  0.0000,\n",
            "          -0.4000, -0.4000]]]), tensor([[[-0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
            "          -0.4000,  0.2000,  0.0000, -0.4000,  0.5000,  0.0000,  0.3000,\n",
            "           0.5000,  0.0000, -0.2000,  0.6000,  0.0000,  0.0000,  0.2000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,  0.0000,\n",
            "           0.0000, -0.4000, -0.4000, -0.2000, -0.2000,  0.6000,  0.2000,\n",
            "           0.5000, -0.4000,  0.5000,  0.6000,  0.5000,  0.0000,  0.0000,\n",
            "           0.2000,  0.6000,  0.0000, -0.4000, -0.4000, -0.2000, -0.4000,\n",
            "           0.5000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
            "           0.0000,  0.6000,  0.0000, -0.4000, -0.4000,  0.3000,  0.2000,\n",
            "           0.0000, -0.2000,  0.5000,  0.0000, -0.4000, -0.2000,  0.2000,\n",
            "           0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
            "           0.0000, -0.4000, -0.2000, -0.2000,  0.6000,  0.3000, -0.4000,\n",
            "           0.2000,  0.0000, -0.2000, -0.2000, -0.4000,  0.5000,  0.2000,\n",
            "           0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,  0.5000,\n",
            "           0.5000,  0.6000]]]), tensor([[[ 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
            "           0.0000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
            "          -0.4000, -0.4000, -0.2000,  0.6000,  0.6000,  0.3000,  0.0000,\n",
            "           0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,  0.5000,\n",
            "           0.0000,  0.0000,  0.2000,  0.5000,  0.3000,  0.5000, -0.4000,\n",
            "           0.5000,  0.2000,  0.5000,  0.6000,  0.2000,  0.5000,  0.5000,\n",
            "           0.5000,  0.5000,  0.0000,  0.0000, -0.4000,  0.0000,  0.0000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,\n",
            "           0.6000, -0.4000,  0.6000,  0.2000,  0.0000,  0.2000, -0.4000,\n",
            "          -0.2000,  0.6000,  0.0000,  0.3000,  0.0000,  0.2000,  0.6000,\n",
            "           0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
            "           0.0000,  0.0000,  0.0000, -0.4000, -0.4000,  0.2000, -0.2000,\n",
            "          -0.4000,  0.3000, -0.4000,  0.3000, -0.2000, -0.4000,  0.5000,\n",
            "           0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000, -0.2000,\n",
            "           0.5000,  0.0000]]]), tensor([[[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
            "           0.0000,  0.5000, -0.4000,  0.2000,  0.0000, -0.4000, -0.2000,\n",
            "           0.0000,  0.0000, -0.2000,  0.5000, -0.4000,  0.5000,  0.5000,\n",
            "           0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,  0.3000,\n",
            "           0.3000,  0.0000,  0.6000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.0000, -0.4000,  0.6000,  0.0000,  0.2000,  0.2000,  0.5000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000, -0.4000,  0.3000,  0.0000,\n",
            "          -0.2000,  0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
            "           0.5000, -0.2000,  0.2000,  0.0000,  0.2000, -0.4000,  0.0000,\n",
            "           0.0000, -0.2000,  0.0000,  0.0000,  0.6000, -0.4000,  0.5000,\n",
            "          -0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
            "           0.3000,  0.2000,  0.5000,  0.0000, -0.4000,  0.0000, -0.4000,\n",
            "           0.3000,  0.0000,  0.0000,  0.0000, -0.4000,  0.0000,  0.5000,\n",
            "           0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000, -0.2000,\n",
            "           0.3000, -0.4000]]]), tensor([[[ 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
            "           0.6000,  0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
            "           0.5000, -0.4000,  0.6000,  0.6000, -0.4000,  0.0000,  0.0000,\n",
            "           0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,  0.3000,\n",
            "           0.6000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
            "           0.0000,  0.0000,  0.0000, -0.2000,  0.6000, -0.4000,  0.6000,\n",
            "           0.0000,  0.3000, -0.4000,  0.6000,  0.0000,  0.0000,  0.3000,\n",
            "           0.0000,  0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000,\n",
            "          -0.4000,  0.0000,  0.2000,  0.0000,  0.3000,  0.0000, -0.4000,\n",
            "           0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.5000,\n",
            "           0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
            "           0.0000,  0.2000,  0.6000,  0.2000, -0.4000,  0.0000, -0.4000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.0000,  0.5000,  0.0000,\n",
            "          -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,  0.6000,\n",
            "           0.0000,  0.5000]]]), tensor([[[ 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
            "          -0.2000,  0.2000,  0.0000,  0.5000,  0.2000,  0.0000, -0.2000,\n",
            "          -0.2000,  0.2000,  0.5000, -0.4000,  0.3000,  0.3000,  0.0000,\n",
            "           0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,  0.3000,\n",
            "           0.0000,  0.3000, -0.4000,  0.2000,  0.5000, -0.4000,  0.0000,\n",
            "           0.5000,  0.0000,  0.2000, -0.4000, -0.4000,  0.0000, -0.4000,\n",
            "           0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
            "          -0.4000, -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,\n",
            "           0.0000,  0.0000, -0.2000,  0.2000, -0.2000,  0.6000,  0.3000,\n",
            "           0.0000,  0.5000,  0.6000, -0.4000, -0.4000,  0.2000,  0.0000,\n",
            "          -0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
            "          -0.4000, -0.4000,  0.0000,  0.0000,  0.5000,  0.5000, -0.2000,\n",
            "          -0.4000,  0.3000, -0.2000,  0.0000,  0.0000, -0.2000,  0.0000,\n",
            "          -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,  0.0000,\n",
            "          -0.4000, -0.4000]]])]\n",
            "[tensor([[[ 5.7436e-02,  2.2370e-01,  2.3466e-01, -3.2815e-01,  5.6173e-01,\n",
            "           4.7796e-01, -1.5464e-01,  1.0859e-01,  4.4504e-01, -3.8595e-01,\n",
            "           1.3479e-01,  6.6532e-02, -3.5885e-01, -2.7072e-01, -7.6161e-02,\n",
            "           8.6731e-02, -3.2130e-01,  3.4720e-01, -4.4290e-01,  5.2130e-01,\n",
            "           4.3984e-01,  4.5052e-01, -2.3021e-01, -1.0327e-03,  2.7528e-01,\n",
            "          -1.4235e-01,  5.7158e-01,  2.9010e-01,  2.6858e-01,  9.9598e-03,\n",
            "           6.6094e-01, -6.7855e-02,  5.6398e-02,  2.3905e-02, -1.4065e-02,\n",
            "          -1.7169e-01, -3.7579e-01,  5.7835e-01, -1.0642e-02,  1.3467e-01,\n",
            "           1.9136e-01,  4.2321e-01,  5.0369e-01, -1.6870e-02, -4.9640e-02,\n",
            "           1.4753e-01, -3.1203e-01,  3.2356e-01, -5.9141e-02, -1.5446e-01,\n",
            "           2.0437e-01, -2.1685e-01,  2.3185e-01,  1.2006e-01, -1.5925e-03,\n",
            "          -5.9082e-02,  4.6265e-01, -3.1291e-01,  2.5252e-01, -5.0269e-02,\n",
            "           1.1198e-01, -4.3535e-01,  2.2458e-02,  7.5268e-02, -2.0073e-01,\n",
            "          -1.7058e-02, -9.6329e-05,  3.4961e-01, -3.0812e-01,  3.3486e-01,\n",
            "          -3.7750e-01, -3.1850e-01, -4.3817e-01, -3.0849e-01, -1.8115e-01,\n",
            "          -7.7210e-03,  3.2701e-01,  2.5567e-01,  9.2094e-02,  4.0513e-01,\n",
            "          -1.1251e-02, -3.5534e-01,  5.4288e-02, -3.1954e-01,  2.8574e-01,\n",
            "           1.4637e-01, -4.8585e-03,  1.4428e-01, -4.4045e-01,  7.5334e-02,\n",
            "           3.9348e-01,  3.0534e-01,  3.4720e-01,  1.5648e-01, -5.3018e-02,\n",
            "           9.4106e-02, -5.3193e-01, -6.1131e-02,  1.9558e-01, -3.8964e-01]]]), tensor([[[-0.0594, -0.5215,  0.1370,  0.5361, -0.0225, -0.5466,  0.0621,\n",
            "          -0.4407, -0.0037,  0.0292, -0.4290,  0.6348,  0.0753,  0.2530,\n",
            "           0.6083, -0.0720, -0.1971,  0.4157,  0.0421, -0.1210,  0.2455,\n",
            "          -0.4704,  0.1930,  0.3944,  0.4274,  0.5287,  0.2602,  0.0705,\n",
            "           0.0687, -0.3202, -0.4632, -0.2620, -0.1851,  0.5576,  0.2223,\n",
            "           0.1390, -0.3099,  0.5526,  0.6916,  0.2921,  0.1373, -0.2177,\n",
            "           0.2625,  0.5453, -0.0206, -0.4355, -0.3470, -0.3339, -0.2579,\n",
            "           0.1679, -0.0587, -0.0212,  0.0083, -0.3961,  0.5446,  0.0793,\n",
            "           0.0897,  0.4238,  0.1005, -0.3928, -0.3168,  0.2314,  0.1165,\n",
            "           0.0607, -0.2534,  0.4625,  0.0235, -0.3566, -0.1395,  0.2052,\n",
            "           0.6205, -0.2244,  0.5834,  0.1282,  0.5269,  0.0367, -0.5422,\n",
            "           0.1801, -0.3633, -0.2088, -0.2768,  0.4853,  0.5370, -0.3465,\n",
            "           0.2005,  0.0661, -0.1848, -0.3360, -0.2862,  0.2843,  0.2455,\n",
            "          -0.0312,  0.0626, -0.0777,  0.1092,  0.4096, -0.1320,  0.2588,\n",
            "           0.6131,  0.2876]]]), tensor([[[ 0.0407,  0.5595, -0.0152,  0.1061,  0.1103, -0.0227,  0.4366,\n",
            "           0.5553,  0.2624,  0.2380,  0.0543, -0.0432, -0.0013, -0.1038,\n",
            "           0.4486, -0.4083,  0.5802,  0.3777, -0.2061, -0.1449, -0.0194,\n",
            "          -0.0391,  0.0397,  0.3224, -0.1541,  0.3205, -0.3855,  0.1325,\n",
            "           0.7085, -0.0680,  0.1401, -0.1442, -0.0519, -0.1516,  0.0238,\n",
            "          -0.0292,  0.2365, -0.2208, -0.1702,  0.3554, -0.2938,  0.0157,\n",
            "          -0.0392,  0.1810, -0.3888,  0.5361,  0.0620,  0.0072,  0.2958,\n",
            "          -0.0256,  0.1763, -0.4554, -0.3131, -0.4886,  0.2883, -0.3420,\n",
            "          -0.3718,  0.1592,  0.1653,  0.0535,  0.4855,  0.0312, -0.4200,\n",
            "          -0.0777,  0.1034, -0.0239,  0.0421,  0.5393,  0.0689,  0.4824,\n",
            "           0.3058, -0.0838, -0.1069,  0.0502, -0.4362, -0.4282, -0.3819,\n",
            "          -0.0868,  0.3549,  0.6407,  0.2118, -0.5327,  0.1164, -0.4147,\n",
            "          -0.4514, -0.0625,  0.5608, -0.0475,  0.0486,  0.3881, -0.0530,\n",
            "          -0.1972, -0.3199, -0.0178,  0.2015, -0.1726,  0.2209,  0.5080,\n",
            "           0.1847,  0.4112]]]), tensor([[[ 0.0634, -0.3412, -0.4220, -0.0092, -0.0192,  0.0360,  0.5976,\n",
            "           0.0518, -0.2238, -0.0759,  0.0147, -0.3240,  0.4157,  0.0703,\n",
            "          -0.4488, -0.3054, -0.2274,  0.5669,  0.6335,  0.2592,  0.0384,\n",
            "          -0.1339,  0.5698,  0.1561, -0.1924,  0.3780,  0.1754,  0.4732,\n",
            "           0.1712, -0.1031,  0.1116,  0.3522,  0.3506,  0.5113, -0.3350,\n",
            "           0.5008,  0.1087,  0.4992,  0.6338,  0.1197,  0.7067,  0.4183,\n",
            "           0.4087,  0.3047,  0.0188, -0.0254, -0.3178,  0.0059,  0.0666,\n",
            "          -0.4899,  0.0019,  0.5581,  0.0128,  0.5517,  0.6477,  0.2578,\n",
            "           0.4206, -0.3256,  0.6878,  0.1778,  0.1240,  0.2700, -0.4602,\n",
            "          -0.2144,  0.5022,  0.0760,  0.3740,  0.0371,  0.0669,  0.5406,\n",
            "           0.3903, -0.1997, -0.2868,  0.4478,  0.1395,  0.4630,  0.0376,\n",
            "          -0.1889,  0.0804,  0.0424, -0.3910, -0.4959,  0.2309, -0.2273,\n",
            "          -0.3251,  0.1913, -0.2425,  0.3997, -0.3064, -0.4495,  0.3844,\n",
            "           0.2293,  0.3882,  0.7333, -0.1398, -0.1298,  0.0510, -0.2115,\n",
            "           0.4383, -0.0167]]]), tensor([[[-2.8265e-03, -2.8765e-01, -3.2015e-01,  4.7233e-02,  4.9811e-04,\n",
            "          -3.4982e-01,  7.9856e-02, -2.1451e-01,  4.5645e-02,  5.6670e-02,\n",
            "           5.5470e-01,  1.9335e-01, -3.3189e-02, -1.7014e-01, -2.6706e-01,\n",
            "           1.4488e-01,  2.9316e-01, -2.5236e-01,  2.7031e-01,  4.1200e-01,\n",
            "           1.9225e-02, -1.6902e-02, -1.1470e-01,  4.7857e-01,  9.0849e-02,\n",
            "          -3.3199e-01, -4.5778e-01,  2.5282e-01,  8.0738e-02,  4.0547e-01,\n",
            "          -3.4456e-01,  9.8296e-02,  6.4028e-01, -4.3161e-01,  4.8399e-02,\n",
            "           4.6373e-01,  4.0219e-02, -6.1252e-02, -4.1794e-01, -4.5012e-01,\n",
            "           3.6362e-02, -3.7679e-01,  2.7419e-01,  1.3580e-02,  5.8942e-02,\n",
            "           9.3856e-02, -1.7166e-01, -2.4154e-02, -2.4050e-01, -4.5668e-01,\n",
            "          -3.6342e-01, -3.2878e-01,  5.5574e-01,  5.7151e-01, -2.4617e-01,\n",
            "           5.6133e-02,  1.8740e-01, -6.3060e-02, -2.1618e-01, -1.5261e-02,\n",
            "          -1.5702e-01,  6.7180e-01,  5.2809e-01,  3.5822e-02,  4.1825e-01,\n",
            "           5.0448e-01, -4.2310e-01, -4.0209e-01,  2.1192e-01,  2.2223e-02,\n",
            "          -3.0848e-01,  2.3095e-02,  5.3313e-02,  1.2901e-01,  6.1508e-01,\n",
            "           8.3557e-02,  4.7251e-02, -3.8764e-01, -3.8602e-01, -4.7666e-03,\n",
            "          -2.5767e-02,  4.1654e-01,  4.8869e-01, -2.0504e-01, -4.4253e-01,\n",
            "           3.2933e-01, -1.6218e-01,  8.7315e-02,  1.6138e-01, -2.7391e-01,\n",
            "           2.4325e-02, -4.5794e-01,  5.2328e-02,  5.5330e-01,  1.1532e-01,\n",
            "           4.7061e-03,  9.4172e-02, -4.9378e-02, -2.6644e-01, -2.3743e-01]]]), tensor([[[-0.0919, -0.5676,  0.2141,  0.3703, -0.0159, -0.4967,  0.1384,\n",
            "          -0.3195,  0.0948,  0.0085, -0.4299,  0.6498,  0.0750,  0.3857,\n",
            "           0.6117, -0.0246, -0.2347,  0.4857,  0.0440, -0.1314,  0.2353,\n",
            "          -0.5114,  0.0949,  0.4259,  0.5351,  0.5233,  0.3036,  0.0444,\n",
            "           0.0075, -0.3666, -0.3673, -0.2141, -0.2376,  0.5350,  0.2181,\n",
            "           0.1399, -0.4078,  0.5700,  0.5273,  0.3818,  0.1374, -0.2512,\n",
            "           0.1908,  0.5175,  0.0909, -0.5089, -0.3431, -0.3722, -0.1392,\n",
            "           0.2092, -0.0700, -0.0129,  0.0190, -0.3537,  0.5889, -0.0222,\n",
            "           0.0482,  0.3785,  0.1271, -0.3284, -0.3138,  0.2488,  0.1045,\n",
            "           0.0568, -0.2404,  0.4796, -0.0149, -0.3277, -0.1494,  0.1947,\n",
            "           0.5383, -0.1697,  0.5220,  0.1401,  0.5145, -0.0579, -0.4653,\n",
            "           0.0600, -0.3315, -0.1786, -0.2328,  0.5447,  0.4786, -0.3109,\n",
            "           0.2184,  0.0690, -0.1903, -0.3779, -0.2954,  0.2700,  0.3478,\n",
            "          -0.0685,  0.0799, -0.1064,  0.1071,  0.3718, -0.1229,  0.2948,\n",
            "           0.7021,  0.4228]]]), tensor([[[ 0.0361, -0.2364, -0.3748, -0.0588,  0.0209,  0.0653,  0.5114,\n",
            "           0.1018, -0.3004, -0.0399, -0.0144, -0.2982,  0.4093,  0.0639,\n",
            "          -0.4247, -0.3688, -0.2127,  0.5454,  0.7060,  0.2659,  0.0246,\n",
            "          -0.0993,  0.4911,  0.1435, -0.1822,  0.3222,  0.1744,  0.4967,\n",
            "           0.2293, -0.1403,  0.1720,  0.3891,  0.3205,  0.4614, -0.3521,\n",
            "           0.5086,  0.2183,  0.5407,  0.6904,  0.1804,  0.7136,  0.3588,\n",
            "           0.4111,  0.3208,  0.0984,  0.0118, -0.4216,  0.0276, -0.0295,\n",
            "          -0.5362,  0.0189,  0.5821,  0.1190,  0.6282,  0.5869,  0.2449,\n",
            "           0.5089, -0.3548,  0.6946,  0.1826,  0.0469,  0.2558, -0.4990,\n",
            "          -0.1953,  0.4184,  0.0737,  0.3552,  0.0538,  0.0338,  0.5325,\n",
            "           0.2728, -0.2547, -0.2124,  0.3286,  0.2524,  0.3935,  0.1587,\n",
            "          -0.1348,  0.0473,  0.0332, -0.3952, -0.5283,  0.2414, -0.1729,\n",
            "          -0.4089,  0.2221, -0.2915,  0.3475, -0.3335, -0.4446,  0.4010,\n",
            "           0.2713,  0.4466,  0.6630, -0.1423, -0.1590,  0.0636, -0.1686,\n",
            "           0.4215,  0.0274]]]), tensor([[[ 0.0565,  0.2073,  0.3030, -0.2549,  0.4801,  0.4336, -0.0534,\n",
            "           0.1518,  0.4217, -0.4105,  0.1069,  0.0603, -0.3737, -0.2323,\n",
            "           0.0078,  0.0655, -0.2938,  0.3153, -0.4719,  0.5728,  0.4235,\n",
            "           0.3752, -0.2749,  0.0707,  0.2858, -0.1101,  0.5450,  0.2820,\n",
            "           0.3405, -0.0587,  0.6738, -0.1076,  0.0813, -0.0092, -0.0615,\n",
            "          -0.1203, -0.4123,  0.6057, -0.0118,  0.1176,  0.1428,  0.4942,\n",
            "           0.5919,  0.0233, -0.0723,  0.0458, -0.3029,  0.3467, -0.0677,\n",
            "          -0.1618,  0.2056, -0.2131,  0.2642,  0.0883,  0.0348, -0.0618,\n",
            "           0.4264, -0.2782,  0.2486, -0.0154,  0.0501, -0.4258, -0.0848,\n",
            "           0.1280, -0.1270,  0.0097, -0.1122,  0.3119, -0.3433,  0.3131,\n",
            "          -0.4474, -0.4388, -0.4449, -0.4374, -0.2070,  0.0047,  0.4467,\n",
            "           0.2859,  0.1154,  0.4303,  0.0766, -0.3876,  0.0557, -0.3064,\n",
            "           0.3123,  0.1581,  0.0694,  0.2275, -0.5417,  0.1376,  0.4049,\n",
            "           0.3384,  0.3651,  0.1172, -0.0732,  0.0618, -0.5036, -0.1887,\n",
            "           0.1346, -0.3127]]]), tensor([[[ 6.2774e-02,  5.4926e-01, -6.5387e-02,  9.8089e-02,  1.4914e-01,\n",
            "          -4.1116e-02,  4.7802e-01,  5.2669e-01,  2.2279e-01,  2.0503e-01,\n",
            "           1.3216e-01, -6.9067e-02, -2.5684e-03, -1.1005e-01,  5.2756e-01,\n",
            "          -4.8318e-01,  5.9509e-01,  4.8103e-01, -1.9968e-01, -5.1140e-02,\n",
            "           8.9390e-03,  6.0114e-04,  5.2054e-03,  3.0677e-01, -1.6220e-01,\n",
            "           3.1823e-01, -4.4443e-01,  1.7459e-01,  6.4889e-01, -9.5728e-02,\n",
            "           9.7498e-02, -1.2356e-01, -1.0395e-02, -1.9575e-01,  2.1706e-02,\n",
            "          -5.6482e-02,  2.2433e-01, -1.9189e-01, -1.3683e-01,  3.2593e-01,\n",
            "          -3.2798e-01, -1.5433e-02, -9.2505e-03,  3.1393e-01, -4.1143e-01,\n",
            "           5.6422e-01,  1.1017e-01, -1.3658e-01,  2.5428e-01,  2.1570e-02,\n",
            "           4.1455e-02, -4.1556e-01, -3.4807e-01, -5.6341e-01,  3.0267e-01,\n",
            "          -4.0255e-01, -4.0352e-01,  1.3848e-01,  1.1986e-01,  3.9446e-02,\n",
            "           4.2635e-01,  1.3228e-02, -3.5121e-01, -7.3094e-02,  1.3963e-01,\n",
            "          -1.0037e-03,  5.7872e-02,  4.6510e-01,  4.0614e-02,  4.8869e-01,\n",
            "           2.1302e-01, -7.7691e-02, -1.2841e-01,  4.1262e-02, -3.1986e-01,\n",
            "          -4.0232e-01, -3.9791e-01, -1.7544e-01,  3.4860e-01,  6.1971e-01,\n",
            "           1.7742e-01, -5.1354e-01,  2.6508e-02, -4.1649e-01, -3.6795e-01,\n",
            "          -1.3363e-02,  5.8137e-01, -1.3091e-02,  1.3376e-01,  4.1593e-01,\n",
            "          -1.3841e-01, -2.5114e-01, -3.7767e-01,  5.7410e-03,  2.1531e-01,\n",
            "          -3.6704e-02,  3.5247e-01,  5.3297e-01,  1.0226e-01,  2.7679e-01]]]), tensor([[[ 0.0058, -0.3279, -0.2728, -0.0172, -0.0115, -0.3564,  0.0352,\n",
            "          -0.1525, -0.0396,  0.0533,  0.5167,  0.2252, -0.0247, -0.1571,\n",
            "          -0.2648,  0.2388,  0.3895, -0.3029,  0.3175,  0.4568,  0.0820,\n",
            "           0.0013, -0.0625,  0.5544,  0.0402, -0.2975, -0.3851,  0.2462,\n",
            "           0.0093,  0.3169, -0.3647,  0.0421,  0.6693, -0.3960,  0.1317,\n",
            "           0.5362,  0.0661,  0.0260, -0.4666, -0.4511,  0.0011, -0.4166,\n",
            "           0.2066, -0.0108,  0.0849,  0.0360, -0.1871, -0.0831, -0.1674,\n",
            "          -0.3957, -0.3586, -0.2874,  0.4475,  0.4892, -0.1663,  0.0678,\n",
            "           0.2344, -0.0497, -0.2321, -0.0416, -0.1457,  0.6290,  0.4337,\n",
            "           0.0216,  0.4073,  0.5065, -0.3904, -0.4069,  0.2001,  0.0686,\n",
            "          -0.3157, -0.0090,  0.1194,  0.1151,  0.5512,  0.0949,  0.0272,\n",
            "          -0.4140, -0.4723, -0.0365,  0.0181,  0.4211,  0.4453, -0.1887,\n",
            "          -0.4774,  0.3419, -0.1364,  0.0226,  0.0566, -0.2485,  0.0295,\n",
            "          -0.4057,  0.0596,  0.5367,  0.2096,  0.0658,  0.0537, -0.0689,\n",
            "          -0.2674, -0.3340]]])]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    #train_loop(loader, loss_fn, optimizer)\n",
        "    test_loop(test_loader, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4UZ27YGus_w",
        "outputId": "bb5c6bda-fb92-4c87-d27e-e4dc8cd743db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.0.0+cu118)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uTZtxT7yF0I",
        "outputId": "36b16747-f06d-4883-f67c-22603d95be6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.2782)"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "from torchmetrics import PeakSignalNoiseRatio\n",
        "psnr = PeakSignalNoiseRatio()\n",
        "\n",
        "\n",
        "preds = torch.tensor([[0.0980,  0.4330,  0.1457,  0.0904,  0.2434, -0.1734, -0.1242,\n",
        "           0.4321,  0.2529,  0.3255,  0.1906,  0.0252,  0.0913, -0.0062,\n",
        "           0.3859, -0.4331,  0.6133,  0.3074, -0.3169, -0.1089,  0.0711,\n",
        "          -0.0064,  0.0744,  0.3301, -0.1384,  0.2718, -0.3473,  0.3270,\n",
        "           0.5976, -0.1173,  0.0380, -0.1381, -0.1261,  0.0990, -0.0599,\n",
        "           0.1124,  0.2800, -0.2361, -0.0100,  0.2620, -0.2193, -0.0746,\n",
        "           0.1566,  0.1539, -0.3888,  0.5083,  0.2460, -0.2194,  0.0291,\n",
        "           0.0329, -0.0960, -0.2312, -0.2565, -0.4096,  0.3304, -0.2551,\n",
        "          -0.2204,  0.2448,  0.1025,  0.0168,  0.3218, -0.1197, -0.3750,\n",
        "           0.0220,  0.0542, -0.0646,  0.1130,  0.3986,  0.1664,  0.4000,\n",
        "           0.3596, -0.1648, -0.0987, -0.0324, -0.4346, -0.0464, -0.0591,\n",
        "           0.0458,  0.2637,  0.6255,  0.2451, -0.4407, -0.1169, -0.2540,\n",
        "          -0.3107, -0.0855,  0.5172, -0.1262,  0.0821,  0.3751,  0.0706,\n",
        "          -0.2589, -0.4015,  0.0011,  0.1701, -0.1222,  0.3744,  0.4785,\n",
        "           0.0319,  0.2547, 0.0690, -0.3011, -0.3555,  0.0625,  0.0438, -0.3609, -0.0440,\n",
        "          -0.1412, -0.0305,  0.0228,  0.4866,  0.2783, -0.1031, -0.2354,\n",
        "          -0.3531,  0.3790,  0.3660, -0.1383,  0.1853,  0.2122,  0.1207,\n",
        "          -0.1081, -0.1151,  0.4324,  0.1182, -0.2723, -0.3551,  0.1235,\n",
        "           0.1807,  0.3659, -0.2635, -0.0512,  0.6177, -0.3933,  0.0704,\n",
        "           0.4535,  0.0564, -0.0205, -0.1917, -0.1614, -0.0405, -0.3225,\n",
        "           0.1485, -0.0108,  0.0511,  0.0941, -0.2414, -0.0350, -0.3360,\n",
        "          -0.3408, -0.3896, -0.4156,  0.4908,  0.4056, -0.1776, -0.0705,\n",
        "           0.2187, -0.1097, -0.1751,  0.1372, -0.1083,  0.5032,  0.4223,\n",
        "           0.0502,  0.4592,  0.5744, -0.4562, -0.3289,  0.1811, -0.0253,\n",
        "          -0.3576,  0.0136,  0.1024,  0.1208,  0.5492, -0.0388,  0.0055,\n",
        "          -0.4392, -0.4094, -0.0593, -0.0469,  0.5289,  0.4858, -0.1977,\n",
        "          -0.4095,  0.2480, -0.1135, -0.1101,  0.1607, -0.3320, -0.0491,\n",
        "          -0.4401, -0.0133,  0.6057,  0.1467,  0.0833, -0.1601, -0.0806,\n",
        "          -0.1427, -0.0771,-3.9717e-02,  1.7581e-01,  1.2830e-01, -2.3615e-01,  4.4820e-01,\n",
        "           5.6353e-01, -8.6581e-02,  8.2354e-02,  3.5195e-01, -3.3636e-01,\n",
        "           2.5019e-01, -3.7720e-02, -3.6734e-01, -3.6469e-01, -1.5250e-01,\n",
        "           1.0426e-01, -2.3434e-01,  1.7122e-01, -2.8829e-01,  3.6648e-01,\n",
        "           3.4235e-01,  2.9034e-01, -2.1544e-01,  5.1203e-02,  3.2702e-01,\n",
        "          -1.1378e-01,  4.2630e-01,  3.3672e-01,  2.8503e-01,  9.5946e-02,\n",
        "           5.6543e-01,  2.6033e-02,  1.1234e-01,  1.6272e-02, -1.2731e-01,\n",
        "          -1.2175e-01, -3.8189e-01,  5.9223e-01,  1.4440e-01, -2.6686e-02,\n",
        "           2.4652e-01,  3.8677e-01,  4.5562e-01, -8.5319e-02,  3.1251e-02,\n",
        "           1.3269e-01, -2.8639e-01,  4.2293e-01,  1.0712e-02, -1.0476e-01,\n",
        "           1.1252e-01, -2.9153e-01,  1.3953e-01,  1.9346e-01, -9.4576e-02,\n",
        "           9.2342e-02,  3.9497e-01, -2.4393e-01,  1.6777e-01,  2.5282e-01,\n",
        "           3.0696e-04, -4.3190e-01,  1.0090e-01, -4.1502e-02, -1.7704e-01,\n",
        "          -1.1672e-01,  1.3521e-01,  3.0426e-01, -2.6574e-01,  1.3128e-01,\n",
        "          -2.0123e-01, -3.5194e-01, -3.6899e-01, -2.9534e-01, -3.0158e-01,\n",
        "           6.8348e-02,  3.2899e-01,  2.0397e-01,  2.4414e-02,  4.8273e-01,\n",
        "          -7.3207e-02, -4.1246e-01,  1.0315e-01, -2.6599e-01,  3.5019e-01,\n",
        "           3.9841e-02,  1.0623e-01,  1.1501e-01, -5.0804e-01,  1.6624e-01,\n",
        "           3.5566e-01,  2.6810e-01,  4.2947e-01,  2.7253e-01, -4.1923e-02,\n",
        "          -4.8222e-02, -5.0292e-01, -1.3459e-01,  2.1186e-01, -1.5473e-01,-0.0493, -0.2676, -0.4431, -0.0605,  0.0009,  0.0869,  0.5050,\n",
        "           0.1614, -0.2267, -0.1832,  0.0925, -0.3865,  0.4400, -0.0439,\n",
        "          -0.4652, -0.3120, -0.3292,  0.4818,  0.5898,  0.4188,  0.0093,\n",
        "          -0.0820,  0.4689,  0.0310, -0.2353,  0.3816,  0.2548,  0.3393,\n",
        "           0.2185, -0.0648,  0.2844,  0.2901,  0.3708,  0.5307, -0.3185,\n",
        "           0.5416,  0.3117,  0.3148,  0.6954,  0.0200,  0.6447,  0.4518,\n",
        "           0.4400,  0.5212,  0.0061, -0.0477, -0.4007,  0.0807,  0.0026,\n",
        "          -0.3722,  0.0899,  0.3074,  0.0584,  0.4814,  0.6229,  0.3253,\n",
        "           0.2672, -0.2114,  0.4876,  0.0967,  0.1498,  0.2989, -0.4961,\n",
        "          -0.1456,  0.4534, -0.0159,  0.3482, -0.0826,  0.0850,  0.5291,\n",
        "           0.0744, -0.2317, -0.3656,  0.4186,  0.1757,  0.4485,  0.2339,\n",
        "          -0.2325,  0.2262, -0.0536, -0.2853, -0.3675,  0.2197, -0.2379,\n",
        "          -0.3637,  0.3273, -0.3391,  0.4158, -0.3262, -0.3465,  0.5706,\n",
        "           0.1950,  0.4620,  0.5697, -0.1875, -0.0771, -0.0160, -0.2210,\n",
        "           0.2732,  0.1100,-0.2058, -0.4286,  0.1079,  0.4283,  0.0519, -0.4201,  0.1570,\n",
        "          -0.4604,  0.1756,  0.1222, -0.3269,  0.5958,  0.0574,  0.3627,\n",
        "           0.5399, -0.1381, -0.0858,  0.3130,  0.1505, -0.0984,  0.3068,\n",
        "          -0.4490, -0.1219,  0.4175,  0.3381,  0.5975,  0.2749,  0.0336,\n",
        "          -0.0019, -0.3696, -0.3080, -0.2910, -0.1098,  0.4250,  0.3481,\n",
        "           0.0954, -0.3341,  0.5027,  0.6221,  0.5082,  0.1990, -0.1026,\n",
        "           0.2789,  0.5434, -0.0386, -0.2823, -0.2060, -0.3595, -0.2865,\n",
        "           0.1437, -0.0440,  0.0407,  0.1277, -0.3370,  0.5129,  0.0191,\n",
        "           0.0903,  0.3444,  0.0552, -0.0652, -0.4124,  0.1663,  0.2356,\n",
        "          -0.1098, -0.1860,  0.3579, -0.0181, -0.2507, -0.2536, -0.0497,\n",
        "           0.4993, -0.2060,  0.6411,  0.2673,  0.4118, -0.0340, -0.5425,\n",
        "           0.1226, -0.4487, -0.0383, -0.2710,  0.5527,  0.4136, -0.3016,\n",
        "           0.1451, -0.0214, -0.1112, -0.3928, -0.2824,  0.1727,  0.3348,\n",
        "          -0.0872,  0.1049, -0.0532,  0.1322,  0.2880, -0.0112,  0.3771,\n",
        "           0.5991,  0.3118]])\n",
        "\n",
        "Y = torch.tensor([[ 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,  0.6000,\n",
        "          0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5000, -0.4000,\n",
        "          0.6000,  0.6000, -0.4000,  0.0000,  0.0000,  0.2000,  0.0000,  0.5000,\n",
        "         -0.2000,  0.3000, -0.4000,  0.3000,  0.6000,  0.0000,  0.0000,  0.0000,\n",
        "         -0.2000,  0.0000, -0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.6000,\n",
        "         -0.4000,  0.6000,  0.0000,  0.3000, -0.4000,  0.6000,  0.0000,  0.0000,\n",
        "          0.3000,  0.0000,  0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000,\n",
        "         -0.4000,  0.0000,  0.2000,  0.0000,  0.3000,  0.0000, -0.4000,  0.0000,\n",
        "          0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.5000,  0.2000,  0.0000,\n",
        "         -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,  0.0000,  0.2000,  0.6000,\n",
        "          0.2000, -0.4000,  0.0000, -0.4000, -0.4000,  0.0000,  0.5000,  0.0000,\n",
        "          0.0000,  0.5000,  0.0000, -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,\n",
        "          0.2000,  0.6000,  0.0000,  0.5000,0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000, -0.2000,\n",
        "          0.2000,  0.0000,  0.5000,  0.2000,  0.0000, -0.2000, -0.2000,  0.2000,\n",
        "          0.5000, -0.4000,  0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
        "          0.0000, -0.4000, -0.4000,  0.3000,  0.0000,  0.3000, -0.4000,  0.2000,\n",
        "          0.5000, -0.4000,  0.0000,  0.5000,  0.0000,  0.2000, -0.4000, -0.4000,\n",
        "          0.0000, -0.4000,  0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000,\n",
        "         -0.2000, -0.4000, -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,\n",
        "          0.0000,  0.0000, -0.2000,  0.2000, -0.2000,  0.6000,  0.3000,  0.0000,\n",
        "          0.5000,  0.6000, -0.4000, -0.4000,  0.2000,  0.0000, -0.2000,  0.0000,\n",
        "          0.0000,  0.2000,  0.5000,  0.0000,  0.0000, -0.4000, -0.4000,  0.0000,\n",
        "          0.0000,  0.5000,  0.5000, -0.2000, -0.4000,  0.3000, -0.2000,  0.0000,\n",
        "          0.0000, -0.2000,  0.0000, -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,\n",
        "          0.0000,  0.0000, -0.4000, -0.4000,0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,  0.0000,\n",
        "          0.5000, -0.4000,  0.2000,  0.0000, -0.4000, -0.2000,  0.0000,  0.0000,\n",
        "         -0.2000,  0.5000, -0.4000,  0.5000,  0.5000,  0.5000, -0.2000,  0.0000,\n",
        "          0.3000,  0.0000,  0.5000,  0.3000,  0.3000,  0.0000,  0.6000,  0.0000,\n",
        "          0.0000,  0.0000,  0.0000,  0.0000, -0.4000,  0.6000,  0.0000,  0.2000,\n",
        "          0.2000,  0.5000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000,  0.3000,\n",
        "          0.0000, -0.2000,  0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
        "          0.5000, -0.2000,  0.2000,  0.0000,  0.2000, -0.4000,  0.0000,  0.0000,\n",
        "         -0.2000,  0.0000,  0.0000,  0.6000, -0.4000,  0.5000, -0.4000, -0.4000,\n",
        "         -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,  0.3000,  0.2000,  0.5000,\n",
        "          0.0000, -0.4000,  0.0000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
        "         -0.4000,  0.0000,  0.5000,  0.2000,  0.6000,  0.0000,  0.0000,  0.0000,\n",
        "         -0.4000, -0.2000,  0.3000, -0.4000,  0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,\n",
        "         -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000, -0.4000, -0.4000,\n",
        "         -0.2000,  0.6000,  0.6000,  0.3000,  0.0000,  0.0000,  0.5000,  0.2000,\n",
        "         -0.2000,  0.5000,  0.0000,  0.5000,  0.0000,  0.0000,  0.2000,  0.5000,\n",
        "          0.3000,  0.5000, -0.4000,  0.5000,  0.2000,  0.5000,  0.6000,  0.2000,\n",
        "          0.5000,  0.5000,  0.5000,  0.5000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
        "          0.0000, -0.4000,  0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,\n",
        "          0.6000, -0.4000,  0.6000,  0.2000,  0.0000,  0.2000, -0.4000, -0.2000,\n",
        "          0.6000,  0.0000,  0.3000,  0.0000,  0.2000,  0.6000,  0.3000, -0.2000,\n",
        "         -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
        "         -0.4000, -0.4000,  0.2000, -0.2000, -0.4000,  0.3000, -0.4000,  0.3000,\n",
        "         -0.2000, -0.4000,  0.5000,  0.3000,  0.5000,  0.6000, -0.2000, -0.2000,\n",
        "          0.0000, -0.2000,  0.5000,  0.0000,-0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000, -0.4000,\n",
        "          0.2000,  0.0000, -0.4000,  0.5000,  0.0000,  0.3000,  0.5000,  0.0000,\n",
        "         -0.2000,  0.6000,  0.0000,  0.0000,  0.2000, -0.4000,  0.0000,  0.5000,\n",
        "          0.3000,  0.6000,  0.2000,  0.0000,  0.0000, -0.4000, -0.4000, -0.2000,\n",
        "         -0.2000,  0.6000,  0.2000,  0.5000, -0.4000,  0.5000,  0.6000,  0.5000,\n",
        "          0.0000,  0.0000,  0.2000,  0.6000,  0.0000, -0.4000, -0.4000, -0.2000,\n",
        "         -0.4000,  0.5000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
        "          0.0000,  0.6000,  0.0000, -0.4000, -0.4000,  0.3000,  0.2000,  0.0000,\n",
        "         -0.2000,  0.5000,  0.0000, -0.4000, -0.2000,  0.2000,  0.5000, -0.2000,\n",
        "          0.6000,  0.2000,  0.5000,  0.0000, -0.4000,  0.0000, -0.4000, -0.2000,\n",
        "         -0.2000,  0.6000,  0.3000, -0.4000,  0.2000,  0.0000, -0.2000, -0.2000,\n",
        "         -0.4000,  0.5000,  0.2000,  0.0000,  0.0000,  0.6000,  0.0000,  0.5000,\n",
        "         -0.2000,  0.5000,  0.5000,  0.6000]])\n",
        "\n",
        "psnr(preds,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF2xP-R4yJDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d3794e-6021-453e-b648-3c34e86b0115"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18.9591)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "#4layers psnr\n",
        "predic = torch.tensor([[-0.0207,  0.1810,  0.3633, -0.2479,  0.6622,  0.4718, -0.0114,\n",
        "           0.1296,  0.4204, -0.3056,  0.0962,  0.0166, -0.4646, -0.2522,\n",
        "          -0.0138, -0.0147, -0.1760,  0.3858, -0.3291,  0.5315,  0.5110,\n",
        "           0.4770, -0.2574,  0.0888,  0.3968, -0.0182,  0.4732,  0.3894,\n",
        "           0.2196,  0.0390,  0.5770, -0.0043,  0.0816,  0.0445, -0.0727,\n",
        "          -0.0800, -0.4367,  0.5932,  0.0186,  0.1516,  0.0878,  0.5425,\n",
        "           0.5929,  0.1295, -0.0356,  0.0297, -0.2448,  0.3284, -0.0316,\n",
        "          -0.3143,  0.2354, -0.3224,  0.2995,  0.0900, -0.0099, -0.0766,\n",
        "           0.4498, -0.2367,  0.3565, -0.0758,  0.2246, -0.4318,  0.0179,\n",
        "           0.1720, -0.2885, -0.1050, -0.0537,  0.4113, -0.3954,  0.2994,\n",
        "          -0.5761, -0.2612, -0.5032, -0.3268, -0.2032,  0.0750,  0.3225,\n",
        "           0.3517,  0.0530,  0.5653, -0.0162, -0.5088,  0.0291, -0.2516,\n",
        "           0.3652, -0.0334,  0.1240,  0.1145, -0.4712, -0.0175,  0.5621,\n",
        "           0.1308,  0.4747,  0.0168,  0.0007,  0.1498, -0.5557, -0.1464,\n",
        "           0.2414, -0.1873 , 0.1102,  0.4069,  0.1036, -0.0311,  0.0446,  0.0744,  0.3796,\n",
        "           0.6837,  0.2498,  0.3552,  0.0390,  0.0345,  0.0701, -0.0130,\n",
        "           0.5248, -0.3041,  0.7109,  0.3595, -0.2475, -0.1196,  0.0384,\n",
        "          -0.0890,  0.0619,  0.3337, -0.0950,  0.3572, -0.3991,  0.1397,\n",
        "           0.6271, -0.1743,  0.1438,  0.0222,  0.0409, -0.0935, -0.0986,\n",
        "          -0.0539,  0.1511, -0.0688, -0.1225,  0.4050, -0.3919,  0.0525,\n",
        "          -0.1003,  0.0971, -0.4445,  0.5713,  0.2559, -0.1163,  0.2976,\n",
        "           0.1133,  0.1291, -0.4882, -0.3475, -0.4599,  0.3940, -0.4969,\n",
        "          -0.4184,  0.1498,  0.2406,  0.0115,  0.4180,  0.0170, -0.3239,\n",
        "          -0.0789,  0.1281,  0.0150,  0.0780,  0.3359,  0.0743,  0.4017,\n",
        "           0.1866, -0.1793, -0.1378,  0.1015, -0.3377, -0.4651, -0.4105,\n",
        "          -0.0906,  0.3038,  0.6977,  0.2837, -0.4966,  0.1104, -0.2817,\n",
        "          -0.4323, -0.0471,  0.6040, -0.0249,  0.0404,  0.3806,  0.0343,\n",
        "          -0.2414, -0.4061,  0.0454,  0.1147, -0.0130,  0.2453,  0.5205,\n",
        "           0.1641,  0.5520, 1.9512e-01, -4.7098e-01, -2.3206e-01,  4.1773e-02, -8.4268e-03,\n",
        "          -2.3763e-01,  4.0556e-02, -1.5227e-01, -1.6652e-01, -7.0769e-04,\n",
        "           4.5431e-01,  1.4204e-01, -1.5527e-02, -1.0078e-01, -2.8119e-01,\n",
        "           1.9540e-01,  3.9129e-01, -2.8585e-01,  3.3151e-01,  4.5519e-01,\n",
        "           7.8885e-02, -1.5782e-01, -3.4423e-02,  4.3259e-01,  6.4715e-02,\n",
        "          -2.3858e-01, -4.2813e-01,  2.8054e-01, -1.1322e-02,  4.2298e-01,\n",
        "          -3.0033e-01,  1.3123e-01,  5.7171e-01, -4.3111e-01,  8.6197e-02,\n",
        "           5.0196e-01,  2.1162e-01,  9.1524e-02, -4.1428e-01, -4.6830e-01,\n",
        "           1.1778e-01, -3.4076e-01,  3.2136e-01, -4.7159e-02,  5.6393e-03,\n",
        "           1.2103e-01, -2.6506e-01, -6.4008e-02, -1.8437e-01, -3.7246e-01,\n",
        "          -3.3579e-01, -3.3959e-01,  5.1774e-01,  5.9716e-01, -1.9229e-01,\n",
        "           4.6184e-02,  9.6242e-02, -4.0020e-02, -2.8932e-01,  7.3592e-02,\n",
        "          -1.8815e-01,  6.4022e-01,  3.4313e-01,  2.0856e-02,  4.1103e-01,\n",
        "           3.8102e-01, -3.7059e-01, -4.4603e-01,  1.3391e-01, -1.6248e-01,\n",
        "          -2.9770e-01,  5.6614e-02,  1.1165e-01,  1.8280e-01,  6.8752e-01,\n",
        "           3.5388e-02,  4.3538e-02, -4.9104e-01, -3.9032e-01,  6.1200e-02,\n",
        "          -2.7420e-02,  5.7896e-01,  5.2084e-01, -1.9168e-01, -4.1365e-01,\n",
        "           2.4115e-01, -1.1974e-01, -4.6052e-02,  2.3215e-01, -1.7548e-01,\n",
        "           1.3191e-04, -3.8948e-01, -5.3571e-02,  5.7901e-01,  1.4563e-01,\n",
        "           7.4949e-02,  4.2233e-02, -5.3525e-02, -3.2480e-01, -1.7591e-01,-9.9854e-02, -1.3788e-01, -3.8776e-01,  8.0480e-02,  3.0448e-02,\n",
        "           1.6202e-01,  5.4454e-01,  2.4381e-01, -2.2894e-01, -7.9946e-02,\n",
        "           7.6652e-05, -3.1508e-01,  4.0713e-01, -4.8687e-02, -4.4852e-01,\n",
        "          -1.9063e-01, -1.7528e-01,  5.0893e-01,  6.1102e-01,  2.9981e-01,\n",
        "           6.0131e-02, -2.0119e-01,  5.2424e-01,  1.6484e-01, -1.6629e-01,\n",
        "           4.4177e-01,  2.7642e-01,  3.9361e-01,  1.9986e-01, -6.1014e-02,\n",
        "           2.4779e-01,  2.4557e-01,  3.5013e-01,  6.3128e-01, -3.5783e-01,\n",
        "           3.8251e-01,  3.7870e-01,  3.7362e-01,  6.2361e-01,  1.2774e-02,\n",
        "           6.7880e-01,  5.4181e-01,  5.0542e-01,  3.6556e-01, -4.5032e-02,\n",
        "           2.1086e-01, -3.6942e-01,  1.1498e-01, -4.8986e-02, -3.0461e-01,\n",
        "          -6.6247e-03,  4.7824e-01,  1.1167e-01,  4.8855e-01,  7.1798e-01,\n",
        "           1.6879e-01,  4.8181e-01, -1.8942e-01,  7.0944e-01,  1.2955e-01,\n",
        "           3.2217e-01,  1.5427e-01, -3.8143e-01, -1.9836e-01,  4.0671e-01,\n",
        "           8.5481e-02,  3.2295e-01,  1.0239e-01, -9.0008e-03,  6.4038e-01,\n",
        "           2.8305e-01, -2.3493e-01, -3.2789e-01,  4.3777e-01,  1.7797e-01,\n",
        "           4.4773e-01,  1.4170e-01, -1.9895e-01,  1.9787e-01, -7.3833e-03,\n",
        "          -4.1867e-01, -3.8572e-01,  1.9006e-01, -2.8322e-01, -2.9506e-01,\n",
        "           2.4363e-01, -2.7715e-01,  2.1411e-01, -2.0644e-01, -4.0481e-01,\n",
        "           3.8130e-01,  4.1470e-01,  3.9903e-01,  5.7948e-01, -1.2489e-01,\n",
        "          -1.5513e-01,  1.5290e-01, -2.0956e-01,  3.6139e-01, -1.0739e-01, 0.0040, -0.5271,  0.1332,  0.4900,  0.0356, -0.5799,  0.1349,\n",
        "          -0.3593, -0.0319,  0.0324, -0.4557,  0.7101,  0.0469,  0.3834,\n",
        "           0.5740,  0.1243, -0.2093,  0.3873,  0.0895, -0.0631,  0.1749,\n",
        "          -0.4688,  0.2214,  0.4076,  0.3450,  0.5634,  0.4353, -0.0031,\n",
        "           0.0941, -0.4475, -0.3160, -0.0950, -0.0538,  0.6370,  0.3079,\n",
        "           0.2594, -0.3219,  0.5661,  0.5566,  0.3528,  0.0859, -0.0767,\n",
        "           0.2061,  0.4714,  0.0083, -0.4456, -0.2961, -0.3941, -0.2900,\n",
        "           0.0819, -0.0337, -0.0102,  0.0859, -0.4251,  0.6070, -0.0316,\n",
        "           0.1063,  0.5070,  0.2150, -0.2391, -0.4070,  0.2172,  0.0495,\n",
        "           0.0835, -0.2452,  0.5351, -0.0327, -0.2884, -0.3648, -0.0094,\n",
        "           0.5583, -0.2700,  0.5532,  0.0142,  0.7182, -0.0661, -0.5174,\n",
        "           0.0961, -0.3635, -0.1779, -0.1880,  0.5331,  0.3277, -0.3084,\n",
        "           0.1672,  0.0055, -0.2268, -0.3967, -0.4033,  0.2496,  0.3316,\n",
        "          -0.1275,  0.0867, -0.0274, -0.0131,  0.3742, -0.1140,  0.4145,\n",
        "           0.5734,  0.4997]])\n",
        "y = torch.tensor([[ 0.2000,  0.0000,  0.3000, -0.4000,  0.6000,  0.3000,  0.0000,\n",
        "           0.0000,  0.5000, -0.4000,  0.2000,  0.0000, -0.4000, -0.2000,\n",
        "           0.0000,  0.0000, -0.2000,  0.5000, -0.4000,  0.5000,  0.5000,\n",
        "           0.5000, -0.2000,  0.0000,  0.3000,  0.0000,  0.5000,  0.3000,\n",
        "           0.3000,  0.0000,  0.6000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
        "           0.0000, -0.4000,  0.6000,  0.0000,  0.2000,  0.2000,  0.5000,\n",
        "           0.6000,  0.0000,  0.0000,  0.0000, -0.4000,  0.3000,  0.0000,\n",
        "          -0.2000,  0.2000, -0.4000,  0.3000,  0.0000,  0.0000,  0.0000,\n",
        "           0.5000, -0.2000,  0.2000,  0.0000,  0.2000, -0.4000,  0.0000,\n",
        "           0.0000, -0.2000,  0.0000,  0.0000,  0.6000, -0.4000,  0.5000,\n",
        "          -0.4000, -0.4000, -0.4000, -0.4000, -0.2000,  0.0000,  0.3000,\n",
        "           0.3000,  0.2000,  0.5000,  0.0000, -0.4000,  0.0000, -0.4000,\n",
        "           0.3000,  0.0000,  0.0000,  0.0000, -0.4000,  0.0000,  0.5000,\n",
        "           0.2000,  0.6000,  0.0000,  0.0000,  0.0000, -0.4000, -0.2000,\n",
        "           0.3000, -0.4000 , 0.0000,  0.6000,  0.0000,  0.0000,  0.2000,  0.0000,  0.6000,\n",
        "           0.6000,  0.3000,  0.3000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
        "           0.5000, -0.4000,  0.6000,  0.6000, -0.4000,  0.0000,  0.0000,\n",
        "           0.2000,  0.0000,  0.5000, -0.2000,  0.3000, -0.4000,  0.3000,\n",
        "           0.6000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
        "           0.0000,  0.0000,  0.0000, -0.2000,  0.6000, -0.4000,  0.6000,\n",
        "           0.0000,  0.3000, -0.4000,  0.6000,  0.0000,  0.0000,  0.3000,\n",
        "           0.0000,  0.2000, -0.4000, -0.4000, -0.4000,  0.3000, -0.4000,\n",
        "          -0.4000,  0.0000,  0.2000,  0.0000,  0.3000,  0.0000, -0.4000,\n",
        "           0.0000,  0.0000,  0.0000,  0.0000,  0.5000,  0.0000,  0.5000,\n",
        "           0.2000,  0.0000, -0.2000,  0.2000, -0.4000, -0.4000, -0.4000,\n",
        "           0.0000,  0.2000,  0.6000,  0.2000, -0.4000,  0.0000, -0.4000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.0000,  0.0000,  0.5000,  0.0000,\n",
        "          -0.2000, -0.4000,  0.0000,  0.0000,  0.0000,  0.2000,  0.6000,\n",
        "           0.0000,  0.5000, 0.2000, -0.4000, -0.4000,  0.0000,  0.0000, -0.4000,  0.0000,\n",
        "          -0.2000,  0.2000,  0.0000,  0.5000,  0.2000,  0.0000, -0.2000,\n",
        "          -0.2000,  0.2000,  0.5000, -0.4000,  0.3000,  0.3000,  0.0000,\n",
        "           0.0000,  0.0000,  0.5000,  0.0000, -0.4000, -0.4000,  0.3000,\n",
        "           0.0000,  0.3000, -0.4000,  0.2000,  0.5000, -0.4000,  0.0000,\n",
        "           0.5000,  0.0000,  0.2000, -0.4000, -0.4000,  0.0000, -0.4000,\n",
        "           0.2000,  0.0000,  0.0000,  0.0000, -0.2000,  0.0000, -0.2000,\n",
        "          -0.4000, -0.4000, -0.4000,  0.6000,  0.6000, -0.2000,  0.0000,\n",
        "           0.0000,  0.0000, -0.2000,  0.2000, -0.2000,  0.6000,  0.3000,\n",
        "           0.0000,  0.5000,  0.6000, -0.4000, -0.4000,  0.2000,  0.0000,\n",
        "          -0.2000,  0.0000,  0.0000,  0.2000,  0.5000,  0.0000,  0.0000,\n",
        "          -0.4000, -0.4000,  0.0000,  0.0000,  0.5000,  0.5000, -0.2000,\n",
        "          -0.4000,  0.3000, -0.2000,  0.0000,  0.0000, -0.2000,  0.0000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.0000,  0.2000,  0.0000,  0.0000,\n",
        "          -0.4000, -0.4000, 0.0000, -0.4000, -0.4000,  0.0000,  0.0000,  0.0000,  0.5000,\n",
        "           0.0000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
        "          -0.4000, -0.4000, -0.2000,  0.6000,  0.6000,  0.3000,  0.0000,\n",
        "           0.0000,  0.5000,  0.2000, -0.2000,  0.5000,  0.0000,  0.5000,\n",
        "           0.0000,  0.0000,  0.2000,  0.5000,  0.3000,  0.5000, -0.4000,\n",
        "           0.5000,  0.2000,  0.5000,  0.6000,  0.2000,  0.5000,  0.5000,\n",
        "           0.5000,  0.5000,  0.0000,  0.0000, -0.4000,  0.0000,  0.0000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.0000,  0.6000,  0.6000,  0.2000,\n",
        "           0.6000, -0.4000,  0.6000,  0.2000,  0.0000,  0.2000, -0.4000,\n",
        "          -0.2000,  0.6000,  0.0000,  0.3000,  0.0000,  0.2000,  0.6000,\n",
        "           0.3000, -0.2000, -0.2000,  0.5000,  0.2000,  0.5000,  0.0000,\n",
        "           0.0000,  0.0000,  0.0000, -0.4000, -0.4000,  0.2000, -0.2000,\n",
        "          -0.4000,  0.3000, -0.4000,  0.3000, -0.2000, -0.4000,  0.5000,\n",
        "           0.3000,  0.5000,  0.6000, -0.2000, -0.2000,  0.0000, -0.2000,\n",
        "           0.5000,  0.0000, -0.2000, -0.4000,  0.0000,  0.5000,  0.0000, -0.4000,  0.2000,\n",
        "          -0.4000,  0.2000,  0.0000, -0.4000,  0.5000,  0.0000,  0.3000,\n",
        "           0.5000,  0.0000, -0.2000,  0.6000,  0.0000,  0.0000,  0.2000,\n",
        "          -0.4000,  0.0000,  0.5000,  0.3000,  0.6000,  0.2000,  0.0000,\n",
        "           0.0000, -0.4000, -0.4000, -0.2000, -0.2000,  0.6000,  0.2000,\n",
        "           0.5000, -0.4000,  0.5000,  0.6000,  0.5000,  0.0000,  0.0000,\n",
        "           0.2000,  0.6000,  0.0000, -0.4000, -0.4000, -0.2000, -0.4000,\n",
        "           0.5000, -0.2000,  0.0000,  0.0000, -0.4000,  0.5000,  0.0000,\n",
        "           0.0000,  0.6000,  0.0000, -0.4000, -0.4000,  0.3000,  0.2000,\n",
        "           0.0000, -0.2000,  0.5000,  0.0000, -0.4000, -0.2000,  0.2000,\n",
        "           0.5000, -0.2000,  0.6000,  0.2000,  0.5000,  0.0000, -0.4000,\n",
        "           0.0000, -0.4000, -0.2000, -0.2000,  0.6000,  0.3000, -0.4000,\n",
        "           0.2000,  0.0000, -0.2000, -0.2000, -0.4000,  0.5000,  0.2000,\n",
        "           0.0000,  0.0000,  0.6000,  0.0000,  0.5000, -0.2000,  0.5000,\n",
        "           0.5000,  0.6000]])\n",
        "psnr(predic, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3z9K58_fKs1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}